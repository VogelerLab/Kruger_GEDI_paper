{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60beb5ec-a2e8-44ca-82eb-96867264a172",
   "metadata": {},
   "source": [
    "# Kruger GEDI analysis\n",
    "Sections of code used for preparing field data and building models for use in the Kruger GEDI paper. Includes the creation of paper figures from prepared data, modeling, and small area estimation. Some parts like making maps from the selected model and performing small area estimation are done in separate scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2760e2-1fa9-42a7-9b51-a8f6073e533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, shapely, joblib, rasterio\n",
    "\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "from glob import glob\n",
    "from rasterstats import zonal_stats, gen_point_query\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import ks_2samp, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb129a6-f05d-4455-97b6-7b036a662c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run list of commands with concurrent threads\n",
    "def cmd_concurrent(cmds, threads=1): \n",
    "    from subprocess import Popen\n",
    "    from itertools import islice\n",
    "    \n",
    "    processes = (Popen(cmd, shell=True) for cmd in cmds)\n",
    "    running_processes = list(islice(processes, threads))  # start new processes\n",
    "    while running_processes:\n",
    "        for i, process in enumerate(running_processes):\n",
    "            if process.poll() is not None:  # the process has finished\n",
    "                running_processes[i] = next(processes, None)  # start new process\n",
    "                if running_processes[i] is None: # no new processes\n",
    "                    del running_processes[i]\n",
    "                    break\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82c0c1-098b-46bf-9c9e-b6d4a97ae4b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Field data\n",
    "Organize collected field data from multiple years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521dacab-ff45-4a21-a4e8-55c46f681524",
   "metadata": {},
   "source": [
    "## Trees\n",
    "Merge tree and plot data from 2023 and 2024 visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9372736-4392-4aa8-82ea-dd79804eeea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GEDI data\n",
    "cols = ['shot_number', 'delta_time', 'cover', 'rh98', 'pai', 'elev_lowestmode', 'lat_lowestmode', 'lon_lowestmode', 'sensitivity', 'geometry']\n",
    "gdf = gpd.read_parquet(r\"J:\\projects\\ECOFOR\\gedi\\gedi_data\\04_gedi_filtered_data_shp\\GEDI_2AB_2019to2023.parquet\", columns=cols)\n",
    "\n",
    "ddirs = [r\"J:\\projects\\ECOFOR\\field\\gedi_jan23\", r\"J:\\projects\\ECOFOR\\field\\gedi_may23\", r\"J:\\projects\\ECOFOR\\field\\gedi_may24\"]\n",
    "dfs = []\n",
    "for ddir in ddirs:\n",
    "    pdf = pd.read_csv(os.path.join(ddir, \"gedi_plot.csv\"), dtype={'gedi':str, 'shot_number':str})\n",
    "    tdf = pd.read_csv(os.path.join(ddir, \"gedi_trees.csv\"))\n",
    "    cols = tdf.columns.difference(pdf.columns.drop(['plot']))\n",
    "    visit = ddir[-5:]\n",
    "    # need to account for multiple groups measuring same plot in merge\n",
    "    if visit=='may24':\n",
    "        cols = cols.tolist()+['group']\n",
    "        mdf = pd.merge(pdf, tdf[cols], on=['group','plot'], how='right')\n",
    "    else:\n",
    "        mdf = pd.merge(pdf, tdf[cols], on='plot', how='right')\n",
    "        mdf['group'] = 0\n",
    "        mdf['shot_number'] = mdf['gedi'].astype(str)\n",
    "    mdf['visit'] = visit\n",
    "    dfs.append(mdf)\n",
    "    \n",
    "df = pd.concat(dfs)\n",
    "\n",
    "df = df.sort_values(['visit', 'plot', 'group'])\n",
    "df['plot_ix'] = pd.factorize(df['visit']+ df['plot'].astype(str) + df['group'].astype(str))[0]\n",
    "\n",
    "# Get quadrant\n",
    "df['quadrant'] = df['Direction'].str.lower()\n",
    "df['quadrant'] = df['quadrant'].fillna(df['num'].replace({1:'ne', 2:'se', 3:'sw', 4:'nw'}))\n",
    "\n",
    "# Save just the trees data\n",
    "cols = ['plot_ix', 'visit', 'plot', 'group', 'quadrant', 'az', 'dist', 'hgt', 'species', 'live', 'pos', 'bole', 'notes',                              # plot and tree identifier, and tree values\n",
    "         'shot_number', 'gps', 'camera', 'photo_num1', 'photo_num2', 'recorder', 'heights', 'photos', 'distances', 'plot_notes']   # other plot variables\n",
    "\n",
    "df[cols].to_csv(r\"J:\\projects\\ECOFOR\\field\\merged\\field_trees_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173b8254-8189-48be-a118-c743c778ba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in GEDI data\n",
    "tdf = pd.merge(df, gdf, how='left', on='shot_number')\n",
    "tdf = gpd.GeoDataFrame(tdf, geometry='geometry', crs=gdf.crs)\n",
    "tdf['date'] = tdf['date'].astype(str)\n",
    "tdf['delta_time'] = tdf['delta_time'].astype(str)\n",
    "tdf = tdf[cols + list(gdf.columns.drop('shot_number'))]\n",
    "\n",
    "# Project tree coordinates but save plot coordinates\n",
    "tdf['geo_utm36n'] = tdf['geometry'].to_crs(epsg = 32636)\n",
    "\n",
    "def get_tree_coords(r):\n",
    "    if np.isnan(r['az']) or np.isnan(r['dist']) or (r['geo_utm36n'] is None):\n",
    "        return None\n",
    "    rad = np.radians(r['az'])\n",
    "    dx = r['dist'] * np.sin(rad)\n",
    "    dy = r['dist']* np.cos(rad)\n",
    "    return shapely.affinity.translate(r['geo_utm36n'], dx, dy)\n",
    "\n",
    "tdf['tree_geo'] = tdf.apply(get_tree_coords, axis=1)\n",
    "tdf['geometry'] = tdf['tree_geo'].set_crs(tdf['geo_utm36n'].crs).to_crs(tdf.crs)\n",
    "tdf = tdf.drop(columns = ['geo_utm36n', 'tree_geo'])\n",
    "\n",
    "tdf.to_file(r\"J:\\projects\\ECOFOR\\field\\merged\\gedi_all_merged.gpkg\", layer=\"trees\", driver=\"GPKG\")\n",
    "\n",
    "tdf.drop(columns='geometry').to_csv(r\"J:\\projects\\ECOFOR\\field\\merged\\gedi_trees_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c062b5-b624-42d0-b625-4283fa54edeb",
   "metadata": {},
   "source": [
    "## Cover  \n",
    "Optical estimates of cover (e.g., tree, soil, litter, etc.) were not used in the paper, but the merged and simplified data is used later for plotting so it is included here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bf6fe7-b429-4b62-990c-397b3a4a4d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddirs = [r\"J:\\projects\\ECOFOR\\field\\gedi_jan23\", r\"J:\\projects\\ECOFOR\\field\\gedi_may23\", r\"J:\\projects\\ECOFOR\\field\\gedi_may24\"]\n",
    "dfs = []\n",
    "for ddir in ddirs:\n",
    "    pdf = pd.read_csv(os.path.join(ddir, \"gedi_plot.csv\"), dtype={'gedi':str, 'shot_number':str})\n",
    "    cdf = pd.read_csv(os.path.join(ddir, \"gedi_cover.csv\"))\n",
    "    cols = cdf.columns.difference(pdf.columns.drop(['plot']))\n",
    "    visit = ddir[-5:]\n",
    "    # need to account for multiple groups measuring same plot in merge\n",
    "    if visit=='may24':\n",
    "        cols = cols.tolist()+['group']\n",
    "        mdf = pd.merge(pdf, cdf[cols], on=['group','plot'], how='right')\n",
    "    else:\n",
    "        mdf = pd.merge(pdf, cdf[cols], on='plot', how='right')\n",
    "        mdf['group'] = 0\n",
    "        mdf['shot_number'] = mdf['gedi'].astype(str)\n",
    "    mdf['visit'] = visit\n",
    "    \n",
    "    \n",
    "    dfs.append(mdf)\n",
    "    \n",
    "df = pd.concat(dfs)\n",
    "\n",
    "df = df.sort_values(['visit', 'plot', 'group'])\n",
    "df['plot_ix'] = pd.factorize(df['visit'] + df['plot'].astype(str) + df['group'].astype(str))[0]\n",
    "\n",
    "# Save just the cover data\n",
    "cols = ['plot_ix', 'visit', 'plot', 'group', 'type', 'species', 'cover', 'notes',                                                                       # plot identifier and plot cover values\n",
    "        'shot_number', 'gps', 'camera', 'photo_num1', 'photo_num2', 'recorder', 'heights', 'photos', 'distances', 'plot_notes']   # other plot variables\n",
    "\n",
    "df[cols].to_csv(r\"J:\\projects\\ECOFOR\\field\\merged\\field_cover_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac27532-a223-4fa2-b98c-3ad3482349c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export overall cover with GEDI data\n",
    "\n",
    "# reshape overall cover only\n",
    "df = df[df['type']=='overall']\n",
    "\n",
    "wdf = df.pivot(index=['plot_ix', 'group'], columns='species', values='cover')\n",
    "wdf['total'] = wdf.sum(axis=1)\n",
    "plot_df = df.drop(columns=['type', 'species', 'cover', 'notes']).drop_duplicates(['plot_ix', 'group']).reset_index()\n",
    "wdf = pd.merge(wdf, plot_df, how='left', on=['plot_ix', 'group'])\n",
    "\n",
    "# merge in GEDI data\n",
    "cdf = pd.merge(wdf, gdf, how='left', on='shot_number')\n",
    "cdf = gpd.GeoDataFrame(cdf, geometry='geometry', crs=gdf.crs)\n",
    "cdf['date'] = cdf['date'].astype(str)\n",
    "cdf['delta_time'] = cdf['delta_time'].astype(str)\n",
    "\n",
    "cols = ['plot_ix', 'visit', 'group', 'plot', 'tree', 'shrub', 'herb', 'soil','litter', 'rock',  'other', 'total',                                                     # plot identifier and plot cover values\n",
    "        'shot_number', 'gps', 'camera', 'photo_num1', 'photo_num2', 'recorder', 'heights', 'photos', 'distances', 'plot_notes']   # other plot variables\n",
    "cdf = cdf[cols + list(gdf.columns.drop('shot_number'))]\n",
    "\n",
    "cdf.to_file(r\"J:\\projects\\ECOFOR\\field\\merged\\gedi_all_merged.gpkg\", layer=\"cover\", driver=\"GPKG\")\n",
    "\n",
    "cdf.drop(columns=['geometry']).to_csv(r\"J:\\projects\\ECOFOR\\field\\merged\\gedi_cover_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05731fa1-fcba-46a6-84c7-f9a7866f9533",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Merged and simplified  \n",
    "Merge tree and cover data and simplify it for use by UBC students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc617270-df42-4424-ae0d-c7108fbdef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = gpd.read_file(r\"J:\\projects\\ECOFOR\\field\\merged\\gedi_all_merged.gpkg\", layer=\"trees\")\n",
    "cdf = gpd.read_file(r\"J:\\projects\\ECOFOR\\field\\merged\\gedi_all_merged.gpkg\", layer=\"cover\")\n",
    "\n",
    "# Extract quadrant with the tallest tree\n",
    "tallest_ix = tdf.groupby('plot_ix')['hgt'].idxmax().dropna()\n",
    "tcols = ['plot_ix', 'quadrant', 'az', 'dist', 'hgt', 'species', 'live', 'pos', 'bole', 'notes']\n",
    "talldf = tdf.loc[tallest_ix, tcols]\n",
    "\n",
    "# Cover columns\n",
    "cols = ['plot_ix', 'visit', 'plot', 'group', 'tree', 'shrub', 'herb', 'soil','litter', 'rock',  'other', 'total',                                                     # plot identifier and plot cover values\n",
    "        'cover', 'rh98', 'pai', 'elev_lowestmode', 'lat_lowestmode', 'lon_lowestmode', 'sensitivity', 'delta_time',                  # GEDI variables\n",
    "        'shot_number', 'gps', 'camera', 'photo_num1', 'photo_num2', 'recorder', 'heights', 'photos', 'distances', 'plot_notes']   # other plot variables\n",
    "\n",
    "simpdf = pd.merge(talldf, cdf[cols], on='plot_ix', how='right')\n",
    "\n",
    "# fill hgt with 0 if null because these are plots with no trees\n",
    "simpdf.loc[simpdf['hgt'].isnull(), 'hgt'] = 0\n",
    "\n",
    "simpdf.to_csv(r\"J:\\projects\\ECOFOR\\field\\merged\\gedi_trees_cover_simp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eea183f-6189-4654-a62d-44e99e1dc7b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model GEDI Canopy Structure  \n",
    "Use Landsat data to construct models of GEDI canopy height and structure metrics that can be used for mapping wall-to-wall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babceaa9-dbc4-4906-ad65-76b951eabb0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and prep data\n",
    "path = r\"J:\\projects\\ECOFOR\\gedi\\extracted\\GEDI_2AB_2019to2023_leafon_sampy500m_all.parquet\"\n",
    "df = gpd.read_parquet(path)\n",
    "\n",
    "outbasedir = r\"J:\\projects\\ECOFOR\\gedi\\models\"\n",
    "ver = \"v08\"\n",
    "outdir = os.path.join(outbasedir, ver)\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "outbasename = os.path.splitext(os.path.basename(path))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab50e1e-8bb9-4543-9283-8b18cc132aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add various radar vegetation indices for PALSAR \n",
    "# # This was tested at request of reviewer, but actually decreased model performance slightly\n",
    "# # Equations taken from table 1 of Hu et al., 2024.\n",
    "# df['palsar_rc'] = df['palsar_HV'] / df['palsar_HH'] # cross-polarization ratio\n",
    "# df['palsar_rvihh'] = 4*df['palsar_HV'] / (df['palsar_HH'] + df['palsar_HV'])\n",
    "# df['palsar_rfdi'] = (df['palsar_HH'] - df['palsar_HV']) / (df['palsar_HH'] + df['palsar_HV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f378ab1c-42df-47bd-9436-91c8ad464dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare modeling dataframe\n",
    "cols = df.columns\n",
    "\n",
    "Xcols = list(cols[cols.str.startswith('lt')]) # LandTrendr\n",
    "bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2'] #'ca', \n",
    "Xcols += [col for col in cols for src in ['hls', 'l30'] for band in bands if col.startswith(src) and (band in col)] # using CCDC\n",
    "Xcols += ['palsar_HV', 'palsar_HH', 'palsar_angle'] #, 'palsar_rc', 'palsar_rvihh', 'palsar_rfdi'] \n",
    "Xcols += list(cols[cols.str.startswith('topo')])\n",
    "Xcols += list(cols[cols.str.startswith('soil')])\n",
    "\n",
    "Ycols = ['cover', 'rh98', 'fhd_normal']\n",
    "meta_cols = ['delta_time', 'year', 'rain_year', 'elev_lowestmode', 'geometry']\n",
    "\n",
    "# Remove NAN's in any cols (same data for each model)\n",
    "mdf = df.dropna(subset=Xcols+Ycols)\n",
    "\n",
    "# Filter unreasonable RH98 (none anyway)\n",
    "mdf = mdf[mdf['rh98']<45]\n",
    "\n",
    "# # Take a random sample for testing\n",
    "# mdf = mdf.sample(100000, random_state=0)\n",
    "\n",
    "# Shuffle the data for use in cross-validation\n",
    "mdf = mdf.sample(frac=1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb8f14-7042-4d84-8149-908606fd4312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor dataset groupings\n",
    "Xcols_dict = {\n",
    "    'lt': [col for col in Xcols if col.startswith('lt')],\n",
    "    'ccdcl30': [col for col in Xcols if col.startswith('l30')],\n",
    "    'ccdchls': [col for col in Xcols if col.startswith('hls')],\n",
    "    'p': ['palsar_HV', 'palsar_HH', 'palsar_angle'],#, 'palsar_rc', 'palsar_rvihh', 'palsar_rfdi'],\n",
    "    's-t': [col for col in Xcols for dstr in [ 'topo', 'soil'] if col.startswith(dstr)],\n",
    "    'lt-p': [col for col in Xcols if col.startswith('lt')] + ['palsar_HV', 'palsar_HH', 'palsar_angle'],\n",
    "    'ccdcl30-p': [col for col in Xcols if col.startswith('l30')] + ['palsar_HV', 'palsar_HH', 'palsar_angle'],\n",
    "    'ccdchls-p': [col for col in Xcols if col.startswith('hls')] + ['palsar_HV', 'palsar_HH', 'palsar_angle'],\n",
    "    'lt-p-s-t': [col for col in Xcols for dstr in ['lt', 'palsar', 'topo', 'soil'] if col.startswith(dstr)],\n",
    "    'ccdcl30-p-s-t': [col for col in Xcols for dstr in ['l30', 'palsar', 'topo', 'soil'] if col.startswith(dstr)],\n",
    "    'ccdchls-p-s-t': [col for col in Xcols for dstr in ['hls', 'palsar', 'topo', 'soil'] if col.startswith(dstr)],\n",
    "    'p-s-t': [col for col in Xcols for dstr in ['palsar', 'topo', 'soil'] if col.startswith(dstr)],\n",
    "}\n",
    "\n",
    "Xsets = Xcols_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41844975-0547-4673-9117-261d00a61f2b",
   "metadata": {},
   "source": [
    "## Build RF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124cd7c5-03f0-41c3-b3cc-c6a6faf9c253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structures to hold predictions, feature importances, and model objects\n",
    "pdf = mdf[meta_cols+Ycols].copy()\n",
    "imp_dict = {}\n",
    "model_dict = {}\n",
    "\n",
    "# Run models for each predictor set\n",
    "for Xset, Xcols in Xcols_dict.items():\n",
    "\n",
    "    X = mdf[Xcols]\n",
    "    for ycol in Ycols:\n",
    "        y = mdf[ycol]\n",
    "        \n",
    "        # Get OOB predictions\n",
    "        rf = RandomForestRegressor(n_estimators=100, max_features='sqrt', oob_score=True, random_state=0, n_jobs=20)\n",
    "        rf.fit(X,y)\n",
    "        pdf['pred_'+Xset+'_'+ycol] = pd.Series(rf.oob_prediction_, index=y.index)\n",
    "\n",
    "        # Get feature importances of every tree\n",
    "        imps = [tree.feature_importances_ for tree in rf.estimators_]\n",
    "        imps = pd.DataFrame(imps, columns=X.columns)\n",
    "        imp_dict[Xset+'_'+ycol] = imps\n",
    "\n",
    "        # keep the model and training data for the model\n",
    "        model_dict[Xset+'_'+ycol] = rf \n",
    "\n",
    "# merge feature importances\n",
    "imp_merged = pd.concat(imp_dict, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8b2e1-286d-4df9-b2e7-5dc007503d61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine and save predictions, importances, and model objects\n",
    "\n",
    "# TODO: Set these data types on load next time\n",
    "pdf['delta_time'] = pdf['delta_time'].astype(str)\n",
    "\n",
    "oob_path = os.path.join(outdir, outbasename+\"_oob_\"+ver+\".parquet\")\n",
    "pdf.to_parquet(oob_path, index=True)\n",
    "\n",
    "imp_path = os.path.join(outdir, outbasename+\"_imps_\"+ver+\".csv\")\n",
    "imp_merged.to_csv(imp_path, index=False)\n",
    "\n",
    "for dset, model in model_dict.items():\n",
    "    model_path = os.path.join(outdir, outbasename + \"_\" + dset + \"_\" + ver + \".joblib\")\n",
    "    joblib.dump(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d45cfd-c3e3-453f-82d7-483344286e3a",
   "metadata": {},
   "source": [
    "## Temporal cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7e9e8-142c-49c3-a0a2-20d8929fc137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for TCV\n",
    "tcv_path = os.path.join(outdir, outbasename+\"_tcv_\"+ver+\".parquet\")\n",
    "\n",
    "years = list(mdf['rain_year'].unique())\n",
    "years.sort()\n",
    "meta_cols = ['delta_time', 'year', 'rain_year']\n",
    "pdf = mdf[meta_cols+Ycols].copy()\n",
    "stats = pd.DataFrame(columns=['Xset', 'metric', 'year', 'n', 'r2', 'rmse', 'bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefcc085-3bb6-4328-9a7b-3ce2fed149dc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform temporal cross-validation on all Xsets\n",
    "for Xset, Xcols in Xcols_dict.items():\n",
    "    for ycol in Ycols:\n",
    "        print(Xset, ycol)\n",
    "        for year in years:\n",
    "            ddf = mdf[Xcols+[ycol, 'rain_year']].dropna()\n",
    "            train, test = ddf[ddf['rain_year']!=year], mdf[mdf['rain_year']==year]\n",
    "            Xtrain, ytrain = train[Xcols], train[ycol]\n",
    "            Xtest, ytest = test[Xcols], test[ycol]\n",
    "            rf = RandomForestRegressor(n_estimators=100, max_features='sqrt', oob_score=False, random_state=0, n_jobs=20)\n",
    "            rf = rf.fit(Xtrain, ytrain)\n",
    "            pred = rf.predict(Xtest)\n",
    "            pdf.loc[ytest.index, Xset+'_'+ycol] = pd.Series(pred, index=ytest.index)\n",
    "\n",
    "            # accuracy stats\n",
    "            # TODO: consider assigning non-forest as 0, like use of irr_area in irrigation_ks.ipynb\n",
    "            obs = ytest.copy()\n",
    "\n",
    "            ix = len(stats)\n",
    "            stats.loc[ix, 'metric'] = ycol \n",
    "            stats.loc[ix, 'Xset'] = Xset\n",
    "            stats.loc[ix, 'year'] = year\n",
    "            stats.loc[ix, 'n'] = obs.size\n",
    "            stats.loc[ix, 'r2'] = r2_score(obs, pred)\n",
    "            stats.loc[ix, 'rmse'] = mean_squared_error(obs, pred)**0.5\n",
    "            stats.loc[ix, 'bias'] = bias = (pred-obs).mean()\n",
    "\n",
    "            # model_dict[dsetyr+'tcv'] = rf\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f4a080-d1e5-4e6b-a45e-1fa148079991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "pdf.to_parquet(tcv_path, index=True)\n",
    "stats.to_csv(os.path.splitext(tcv_path)[0]+\"_stats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3ed70-8210-489e-a452-32c1f1cc8c05",
   "metadata": {},
   "source": [
    "## Bias correction of selected model\n",
    "\n",
    "Use BC1 in Zhang and Lu 2012 to reduce compression to the mean in the selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c57b1e6-fa88-4752-afd2-7d066a676f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup \n",
    "Xset = 'lt-p-s-t' # selected model predictor set\n",
    "Xcols = Xcols_dict[Xset]\n",
    "\n",
    "train, test = train_test_split(mdf, test_size=0.3, random_state=42)\n",
    "pdf = test[meta_cols+Ycols].copy()\n",
    "model_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e36c94-a98d-48c5-9505-5f71535c4ea7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run bias corrected random forest on each GEDI metric\n",
    "for ycol in Ycols:\n",
    "    Xtrain, ytrain = train[Xcols], train[ycol]\n",
    "    Xtest, ytest = test[Xcols], test[ycol]\n",
    "    \n",
    "    # Random forest 1 with OOB predictions\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_features='sqrt', oob_score=True, random_state=0, n_jobs=20)\n",
    "    rf.fit(Xtrain,ytrain)\n",
    "    ypred = pd.Series(rf.oob_prediction_, index=ytrain.index, name='ypred')\n",
    "\n",
    "    # Random forest 2 of residuals\n",
    "    resids = ytrain - ypred\n",
    "    rtrain = pd.concat([Xtrain, ypred], axis=1)\n",
    "    rf_resid = RandomForestRegressor(n_estimators=100, max_features='sqrt', oob_score=True, random_state=0, n_jobs=20)\n",
    "    rf_resid.fit(rtrain, resids)\n",
    "\n",
    "    # Apply both RFs to test data and sum pred + resid\n",
    "    ypred_test = pd.Series(rf.predict(Xtest), index=ytest.index, name='ypred')\n",
    "    rtest = pd.concat([Xtest, ypred_test], axis=1)\n",
    "    resids_test = pd.Series(rf_resid.predict(rtest), index=rtest.index, name='resid')\n",
    "    bc_test = ypred_test + resids_test\n",
    "\n",
    "    # Save test results\n",
    "    pdf[\"pred_\"+ycol] = ypred_test\n",
    "    pdf[\"pred_resid_\"+ycol] = resids_test\n",
    "    pdf[\"pred_bc_\"+ycol] = bc_test\n",
    "    model_dict[ycol] = rf\n",
    "    model_dict[ycol+\"_resid\"] = rf_resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d3dd57-b7f1-4f04-a1b5-b9d75c9fe889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine and save predictions, importances, and model objects\n",
    "\n",
    "# TODO: Set these data types on load next time\n",
    "pdf['delta_time'] = pdf['delta_time'].astype(str)\n",
    "\n",
    "# oob_path = os.path.join(outdir, outbasename+\"_oob_\"+ver+\".gpkg\")\n",
    "# pdf.to_file(oob_path, driver='GPKG', index=True)\n",
    "\n",
    "bc_path = os.path.join(outdir, outbasename+\"_bc_\"+ver+\".parquet\")\n",
    "pdf.to_parquet(bc_path, index=True)\n",
    "\n",
    "for mname, model in model_dict.items():\n",
    "    model_path = os.path.join(outdir, outbasename + \"_\" + mname + \"_bc_\" + ver + \".joblib\")\n",
    "    joblib.dump(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c6c54-c457-430a-b3f8-726c3e429f01",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Maps  \n",
    "\n",
    "This section does additional prep and analysis of the already created rasters of the GEDI metrics.  \n",
    "\n",
    "Applying the random forest models to the predictor sets to make rasters of each GEDI metric for each year is done with map_concurrent_multimodel.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5b830-3c97-468f-bad9-2a4e271acefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating pyramids and stats for all maps\n",
    "def pyr_stats(path, run=True):\n",
    "    \"\"\"Set nodata (str of number or 'nan'). Calculate stats and pyramids for image at path (str).\"\"\"\n",
    "    from subprocess import check_output\n",
    "    cmds = {'stats':[], 'pyr':[]}\n",
    "    \n",
    "    stats_cmd = 'gdalinfo -approx_stats ' + path\n",
    "    if run:\n",
    "        result = check_output(stats_cmd)\n",
    "    \n",
    "    if not os.path.exists(path+\".ovr\"):\n",
    "        pyr_cmd = 'gdaladdo -ro --config COMPRESS_OVERVIEW ZSTD --config ZSTD_LEVEL 1 --config PREDICTOR 2 --config INTERLEAVE_OVERVIEW BAND --config GDAL_NUM_THREADS 6 --config GDAL_CACHEMAX 4096 ' + path\n",
    "        if run:\n",
    "            result = check_output(pyr_cmd)\n",
    "    else:\n",
    "        pyr_cmd = \"ECHO \" + path + \" completed\"\n",
    "    \n",
    "    return stats_cmd, pyr_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb1a8d-12a4-495b-a047-266cdfcbc0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_cmds, pyr_cmds = [], []\n",
    "paths = glob(r\"D:\\ECOFOR\\gedi\\maps\\v04_ltpa2\\*\\*.tif\")\n",
    "for path in paths:\n",
    "    stat_cmd, pyr_cmd = pyr_stats(path, run=False)\n",
    "    stat_cmds.append(stat_cmd)\n",
    "    pyr_cmds.append(pyr_cmd)\n",
    "\n",
    "cmd_concurrent(stat_cmds, threads=16)\n",
    "cmd_concurrent(pyr_cmds, threads=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ed9d05-2dbc-44a0-ab77-69379807f9ff",
   "metadata": {},
   "source": [
    "## Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cdb4c9-2ae7-45ec-8afc-b3c08b3f4719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate difference between two years for each metric\n",
    "basedir = r\"J:\\projects\\ECOFOR\\gedi\\maps\\v08\\lt-p-s-t\"\n",
    "y1 = \"2017\"\n",
    "y2 = \"2021\"\n",
    "pct_chg = False #True\n",
    "\n",
    "outdir = os.path.join(basedir, 'change')\n",
    "metrics = [\"cover\", \"rh98\", \"fhd\"]\n",
    "\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "for metric in metrics:\n",
    "    path1 = os.path.join(basedir, metric, metric+\"_\"+y1+\".tif\")\n",
    "    path2 = os.path.join(basedir, metric, metric+\"_\"+y2+\".tif\")\n",
    "    pct_str = \"_pct\" if pct_chg else \"\"\n",
    "    outpath = os.path.join(outdir, metric+\"_\"+y2+\"minus\"+y1+pct_str+\".tif\")\n",
    "    \n",
    "    with rasterio.open(path1) as src:\n",
    "        arr1 = src.read(1)\n",
    "        profile = src.profile\n",
    "    \n",
    "    with rasterio.open(path2) as src:\n",
    "        arr2 = src.read(1)\n",
    "\n",
    "    chg = np.subtract(arr2, arr1, dtype=np.float32)\n",
    "    chg[np.isnan(arr1) | np.isnan(arr2)] = np.nan\n",
    "    \n",
    "    if pct_chg:\n",
    "        arr1[arr1==0] = 0.01\n",
    "        chg = chg / arr1 * 100\n",
    "\n",
    "    with rasterio.open(outpath, 'w', **profile) as dst:\n",
    "        dst.write(chg, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f2a2a5-1db6-4971-9a44-89bdcce5858e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stat_cmds, pyr_cmds = [], []\n",
    "paths = glob(r\"J:\\projects\\ECOFOR\\gedi\\maps\\v08\\lt-p-s-t\\change\\*.tif\")\n",
    "for path in paths:\n",
    "    stat_cmd, pyr_cmd = pyr_stats(path, run=False)\n",
    "    stat_cmds.append(stat_cmd)\n",
    "    pyr_cmds.append(pyr_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd89d1e-329a-4ede-92d3-5ef5f52fa08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_concurrent(stat_cmds, threads=10)\n",
    "cmd_concurrent(pyr_cmds, threads=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce2909f-4442-49a2-adc5-710113bcd46c",
   "metadata": {},
   "source": [
    "## Prep SAE tables  \n",
    "Prepare data necessary for doing small area estimation in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b376149-62a5-4b21-a3e0-bd46962e2351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get population of pixels for all years for each AOI\n",
    "aois_path = r\"J:\\projects\\ECOFOR\\gedi\\sae\\sae_aois.gpkg\"\n",
    "basedir = r\"D:\\ECOFOR\\gedi\\maps\\v08\\lt-p-s-t\"\n",
    "outpath = r\"J:\\projects\\ECOFOR\\gedi\\sae\\sae_gedi_pop.parquet\"\n",
    "\n",
    "rast_paths = [p for p in glob(os.path.join(basedir, \"*/*.tif\")) if \"change\" not  in p]\n",
    "\n",
    "def get_fdf(fdict):\n",
    "    fprop = fdict['properties']\n",
    "    arr = fprop['mini_raster_array'].ravel()\n",
    "    arr = arr[~arr.mask].data\n",
    "    aoi = fprop['name']\n",
    "    return pd.DataFrame({'val':arr, 'aoi':aoi})\n",
    "\n",
    "def get_aoi_vals(aois_path, rast_path):\n",
    "    zstats = zonal_stats(aois_path, rast_path, stats=\"count\", raster_out=True, geojson_out=True)\n",
    "    fname = os.path.basename(rast_path)[:-4]\n",
    "    metric, year = fname.split('_')\n",
    "    fdfs = [get_fdf(f) for f in zstats]\n",
    "    df = pd.concat(fdfs, axis=0)\n",
    "    df['metric'] = metric\n",
    "    df['year'] = int(year)\n",
    "    return df\n",
    "\n",
    "rast_dfs = joblib.Parallel(n_jobs=10)(joblib.delayed(get_aoi_vals)(aois_path, rast_path) for rast_path in rast_paths)\n",
    "\n",
    "df = pd.concat(rast_dfs, axis=0)\n",
    "\n",
    "# pivoting this way takes 3ish minutes and lots of ram\n",
    "df['ix'] = df.index\n",
    "dfw = df.pivot(index=['aoi', 'year', 'ix'], columns='metric', values='val').reset_index()\n",
    "dfw['domain'] = dfw['aoi']+'_'+dfw['year'].astype(str)\n",
    "\n",
    "dfw.to_parquet(outpath)\n",
    "del dfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5404c9db-fd1a-48d3-8dd9-38e3cc9925f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GEDI pred/obs samples in domains\n",
    "oob_path = r\"J:\\projects\\ECOFOR\\gedi\\models\\v08\\GEDI_2AB_2019to2023_leafon_sampy500m_all_v08.parquet\"\n",
    "aois_path = r\"J:\\projects\\ECOFOR\\gedi\\sae\\sae_aois.gpkg\"\n",
    "outpath = r\"J:\\projects\\ECOFOR\\gedi\\sae\\sae_gedi_samp_v08.gpkg\"\n",
    "\n",
    "df = gpd.read_parquet(oob_path)\n",
    "aois = gpd.read_file(aois_path)\n",
    "\n",
    "sdf = df.sjoin(aois[['name', 'geometry']], how='inner')\n",
    "sdf = sdf.drop(columns=['index_right'])\n",
    "sdf = sdf.rename(columns={'name':'aoi'})\n",
    "sdf['domain'] = sdf['aoi']+'_'+sdf['rain_year'].astype(str)\n",
    "\n",
    "sdf.to_file(outpath, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a9d6ac-89d2-4a1e-adae-3bc0dc6b5fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all GEDI footprints in domains for direct estimators\n",
    "path = r\"J:\\projects\\ECOFOR\\gedi\\gedi_data\\04_gedi_filtered_data_shp\\GEDI_2AB_2019to2023.parquet\"\n",
    "aois_path = r\"J:\\projects\\ECOFOR\\gedi\\sae\\sae_aois.gpkg\"\n",
    "outpath = r\"J:\\projects\\ECOFOR\\gedi\\sae\\sae_gedi_all.gpkg\"\n",
    "\n",
    "df = gpd.read_parquet(path)\n",
    "aois = gpd.read_file(aois_path)\n",
    "\n",
    "# Filter to points that will be used (leaf-on only)\n",
    "df['delta_time'] = pd.to_datetime(df['delta_time'])\n",
    "df = df[df['rh98']<45] # Remove unreasonable points\n",
    "df = df[(df['delta_time'].dt.day_of_year < 121) | (df['delta_time'].dt.day_of_year > 305)] # keep only leaf-on (Nov - Apr) as defined in Li 2023\n",
    "\n",
    "# Rain year is defined as the year beginning with the start of the dry season (121-273) and the following wet season (274-120)\n",
    "# e.g., rain year 2018 is May 1, 2018 - April 30 2019\n",
    "df['year'] = df['delta_time'].dt.year\n",
    "df['rain_year'] = df['year'].copy()\n",
    "df.loc[df['delta_time'].dt.day_of_year < 121, 'rain_year'] += -1 \n",
    "\n",
    "df = df.to_crs(aois.crs)\n",
    "\n",
    "sdf = df.sjoin(aois[['name', 'geometry']], how='inner')\n",
    "sdf = sdf.drop(columns=['index_right'])\n",
    "sdf = sdf.rename(columns={'name':'aoi'})\n",
    "\n",
    "sdf['domain'] = sdf['aoi']+'_'+sdf['rain_year'].astype(str)\n",
    "\n",
    "sdf.to_file(outpath, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e3b588-c3cb-4135-b386-b0d01116b9da",
   "metadata": {},
   "source": [
    "## Climate sensitivity  \n",
    "Evaluate the sensitivity of the predicted metrics to climate, and in particular the drought in 2015/2016. This drought was during the rainy season of 2015/16 so October 2015 to April 2016. There was a recovery in the wet season of 2016. https://doi.org/10.2989/10220119.2020.1718755\n",
    "\n",
    "Compare 2015 (2015 dry + 2015/16) to 2016 for areas with no apparent vegetation change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93cfbb5-61dd-4c73-9997-b59fc8db8a92",
   "metadata": {},
   "source": [
    "**Prep sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27953707-55cb-4dc7-8094-8c1d094c37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample of VCA sites to use for photo-interp of change\n",
    "\n",
    "# Filter VCA sites for intersection with fires between 2009 and 2017\n",
    "fire_paths = [\n",
    "    'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2009fires.shp',\n",
    "    'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2010fires.shp',\n",
    "    'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2011fires.shp',\n",
    "    'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2012fires.shp',\n",
    "    'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2013fires.shp',\n",
    "    'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2014firesUTM.shp',\n",
    "    'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2015firesUTM.shp',\n",
    "    'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2016firesUTM.shp',\n",
    "    'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2017firesUTM.shp'\n",
    "]\n",
    "\n",
    "fire_df = pd.concat([gpd.read_file(p).to_crs(epsg=32736) for p in fire_paths])\n",
    "\n",
    "burned = fire_df.unary_union\n",
    "\n",
    "# Prep VCA points and convert to boxes for photo-interp\n",
    "path = r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\derived\\vca_sites.gpkg\"\n",
    "df = gpd.read_file(path)\n",
    "df = df.to_crs(epsg=fire_df.crs.to_epsg())\n",
    "\n",
    "# Drop points intersecting burns, or with a big difference in coordiantes or their is no geometry\n",
    "df = df[~df.intersects(burned)]\n",
    "df = df[~(df[\"coords_dif\"]>10) & (~df[\"geometry\"].isnull())]\n",
    "\n",
    "# Shuffle to get better spatial distribution when examining first X in list\n",
    "df = df.sample(frac=1, random_state=42) \n",
    "\n",
    "# Snap to nearest pixel\n",
    "rast_path = r\"J:\\projects\\ECOFOR\\gedi\\maps\\v08\\lt-p-s-t\\cover\\cover_2015.tif\"\n",
    "\n",
    "with rasterio.open(rast_path) as src:\n",
    "    src_crs = src.crs\n",
    "df = df.to_crs(epsg=src_crs.to_epsg())\n",
    "\n",
    "def snap_points(gdf_points, raster_filepath):\n",
    "    with rasterio.open(raster_filepath) as src:\n",
    "        transform = src.transform\n",
    "        res = src.res[0]   \n",
    "    coords = np.array([(p.x, p.y) for p in gdf_points.geometry])\n",
    "    rows, cols = rasterio.transform.rowcol(transform, coords[:, 0], coords[:, 1])\n",
    "    x_snapped, y_snapped = rasterio.transform.xy(transform, rows, cols, offset='center')\n",
    "    snapped_geometries = [shapely.geometry.Point(x, y) for x, y in zip(x_snapped, y_snapped)]\n",
    "    return snapped_geometries\n",
    "\n",
    "df[\"geometry\"] = snap_points(df, rast_path)\n",
    "\n",
    "df.to_file(r\"J:\\projects\\ECOFOR\\climate_sensitivity\\vca_unburned09to17_snapped.gpkg\", driver=\"GPKG\")\n",
    "df.to_file(r\"J:\\projects\\ECOFOR\\climate_sensitivity\\vca_unburned09to17_snapped.shp\")\n",
    "\n",
    "# Create boxes for visualization in Google earth\n",
    "df[\"geometry\"] = df.buffer(15, cap_style=\"square\")\n",
    "df.to_file(r\"J:\\projects\\ECOFOR\\climate_sensitivity\\vca_unburned09to17_snapped_box.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa40e715-590d-4d48-8504-1d395d411673",
   "metadata": {},
   "source": [
    "**Extract data**  \n",
    "Extract Landsat data and predictor data for photo-interp sample for creating plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47fa3ff-b051-4c4f-b9f7-26afc6ddcfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the version with PI completed to a shapefile for upload to GEE\n",
    "path = r\"J:\\projects\\ECOFOR\\climate_sensitivity\\vca_unburned09to17_snapped_pi.gpkg\"\n",
    "df = gpd.read_file(path)\n",
    "df.to_file(os.path.splitext(path)[0]+\".shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e1e6b0-5b3d-468e-bc2b-000e9da1a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract cover and LandTrendr for points\n",
    "path = r\"J:\\projects\\ECOFOR\\climate_sensitivity\\vca_unburned09to17_snapped_pi.gpkg\"\n",
    "outpath = r\"J:\\projects\\ECOFOR\\climate_sensitivity\\vca_unburned09to17_snapped_pi_extract.gpkg\"\n",
    "\n",
    "df = gpd.read_file(path)\n",
    "\n",
    "indirs = {\n",
    "    \"ltwet\":r\"J:\\projects\\ECOFOR\\lt\\wet\",\n",
    "    \"ltdry\":r\"J:\\projects\\ECOFOR\\lt\\dry\",\n",
    "    \"cover\":r\"J:\\projects\\ECOFOR\\gedi\\maps\\v08\\lt-p-s-t\\cover\"\n",
    "}\n",
    "\n",
    "layers = {}\n",
    "for key, indir in indirs.items():\n",
    "    ext = \"vrt\" if key.startswith(\"lt\") else \"tif\"\n",
    "    paths = glob(os.path.join(indir, \"*.\"+ext))\n",
    "    for path in paths:\n",
    "        year = path[-8:-4]\n",
    "#         if int(year) < 2007:\n",
    "#             continue\n",
    "        layers[key+\"_\"+year] = path\n",
    "        \n",
    "# Only points with a classification for extraction\n",
    "df = df[~df[\"change\"].isnull()]\n",
    "\n",
    "# Extract layers for each point\n",
    "for name, path in layers.items():\n",
    "    with rasterio.open(path) as src:\n",
    "        bands = src.descriptions\n",
    "        \n",
    "    if len(bands)<2:\n",
    "        bands = [name.split(\"_\")[0]]\n",
    "    \n",
    "    def extract_vals(band, band_name):\n",
    "        vals = list(gen_point_query(df['geometry'], path, band=band+1, interpolate='nearest'))\n",
    "        return pd.Series(vals, index=df.index, name=name+'_'+band_name)\n",
    "\n",
    "    val_series = joblib.Parallel(n_jobs=8)(joblib.delayed(extract_vals)(band, band_name) for band, band_name in enumerate(bands))\n",
    "    valdf = pd.concat(val_series, axis=1)\n",
    "    df = pd.merge(df, valdf, 'left', left_index=True, right_index=True)\n",
    "\n",
    "df.to_file(outpath, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df67a0e-774c-4fc2-a3c5-7c5d414981d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load earth engine functions\n",
    "import sys\n",
    "sys.path.append(r'J:\\users\\stevenf\\code\\utils\\pee')\n",
    "import ee\n",
    "import landsat as lxtools\n",
    "import time_series\n",
    "\n",
    "ee.Initialize()\n",
    "\n",
    "# Extract original landsat wet season NDVI composites for points (before LandTrendr)\n",
    "fc = ee.FeatureCollection(\"projects/earthengine-legacy/assets/users/stevenf/ecofor/vca_unburned09to17_snapped_pi\")\n",
    "fc = fc.filter(ee.Filter.notEquals(\"change\", \"\"))\n",
    "# fc = ee.FeatureCollection([fc.first()])\n",
    "\n",
    "starty = 1984\n",
    "endy = 2022\n",
    "startdoy, enddoy = 274, 120 # Oct 1st, Apr 30 - non-leap year wet season\n",
    "# startdoy, enddoy = 121, 273 # May 1st, Sept 30 - non-leap year dry season\n",
    "orig_bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']\n",
    "ixbands = ['ndvi', 'nbr', 'ndmi','tcb', 'tcg', 'tcw']\n",
    "coll_kwargs = {'bands':orig_bands, 'rescale':True, 'cloud_cover':50, 'tdom':False}\n",
    "\n",
    "comps = time_series.annual_composites(fc, starty, endy, startdoy, enddoy,\n",
    "                                      lxtools.sr_collection, time_series.medoid,\n",
    "                                      coll_kwargs, fill=False)\n",
    "\n",
    "comps = comps.map(lambda i: (i.addBands(lxtools.specixs(i, ixlist=ixbands))))\n",
    "\n",
    "DEFAULT_PROPERTIES = ee.Dictionary(\n",
    "    {band: -32768 for band in orig_bands + ixbands}\n",
    ")\n",
    "\n",
    "\n",
    "def ensure_schema(feature):\n",
    "    feature_props = feature.toDictionary()\n",
    "    merged_props = feature_props.combine(DEFAULT_PROPERTIES, overwrite=False)\n",
    "    return ee.Feature(feature.geometry(), merged_props)\n",
    "\n",
    "\n",
    "def extract_point_data(image):\n",
    "    sampled_fc = image.reduceRegions(\n",
    "        collection=fc,\n",
    "        reducer=ee.Reducer.first(),\n",
    "        scale=30,\n",
    "        tileScale=16\n",
    "    )\n",
    "    \n",
    "    sampled_fc = sampled_fc.map(ensure_schema)\n",
    "    \n",
    "    return sampled_fc.map(lambda feature: feature.set('year', image.get(\"year\")))\n",
    "\n",
    "fc = comps.map(extract_point_data).flatten()\n",
    "\n",
    "# fc.getInfo()\n",
    "\n",
    "task = ee.batch.Export.table.toDrive(\n",
    "    collection=fc,\n",
    "    description='vca_unburned09to17_snapped_pi_landsat_wet2',\n",
    "    folder='gee',\n",
    "    fileFormat='CSV'\n",
    ")\n",
    "task.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e52ea-f69c-42bb-b83e-afa67c4cde9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7840d04c-c1cc-4e2a-989f-724516a61e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper set up\n",
    "figdir = r\"E:\\My Drive\\Work\\ecofor\\manuscript\\figs\"\n",
    "os.makedirs(figdir, exist_ok=True)\n",
    "\n",
    "mpl.rcParams['font.family'] = 'sans-serif'\n",
    "mpl.rcParams['font.sans-serif'] = ['Arial']\n",
    "mpl.rcParams['font.size'] = 8\n",
    "\n",
    "sns.set_style('ticks',\n",
    "               {'font.family':'sans-serif', 'font.sans-serif':['Arial'], 'font.size':8})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2af426-2a55-4d43-b5cb-3de1227a2f13",
   "metadata": {},
   "source": [
    "## Distribution of GEDI  \n",
    "Show distribution of GEDI values for entire study area and by vegetation type.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bfdad8-ad75-4bcc-b39c-0d4a4160f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"J:\\projects\\ECOFOR\\gedi\\models\\v08\\GEDI_2AB_2019to2023_leafon_sampy500m_all_oob_v08.parquet\"\n",
    "df = gpd.read_parquet(path)\n",
    "\n",
    "df['cover'] *= 100 # convert cover to %\n",
    "ycol_dict = {'cover':'Cover (%)', 'rh98':'RH98 (m)', 'fhd_normal':'FHD'} #'pai':'PAI', \n",
    "df = df.rename(columns=ycol_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b2590-9190-4e15-b754-ff83625f0956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load land cover or veg type for hue\n",
    "path = r\"J:\\projects\\ECOFOR\\gedi\\extracted\\GEDI_2AB_2019to2023_leafon_sampy500m_sanlc20.csv\"\n",
    "cdf = pd.read_csv(path).set_index('shot_number')\n",
    "\n",
    "# Create a modification of salcc1 to separate out open woodland and group others\n",
    "rat_path = r\"J:\\projects\\ECOFOR\\lcluc\\SANLC\\2020\\SA_NLC_2020_GEO.tif.vat.dbf\"\n",
    "rat = gpd.read_file(rat_path).drop('geometry', axis=1)\n",
    "rat['SALCC_1'] = rat['SALCC_1'].replace('Forested Land', 'Forested land')\n",
    "mod_dict = rat.set_index('Value')['SALCC_1'].to_dict()\n",
    "mod_dict[4] = 'Open Woodland'\n",
    "cdf['sanlc20_salcc1_mod'] = cdf['sanlc20_val'].map(mod_dict)\n",
    "other_mask = cdf['sanlc20_salcc1_mod'].isin([None, 'Built-up', 'Wetlands', 'Barren Land', 'Waterbodies', 'Mines & Quarries', 'Shrubland'])\n",
    "cdf.loc[other_mask, 'sanlc20_salcc1_mod'] = 'Other'\n",
    "\n",
    "# Merge GEDI with land cover classes\n",
    "df = pd.merge(df, cdf, how='left', left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90e95e1-7bb1-4f8e-a3a3-83ef539a6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plot\n",
    "hue_order = df['sanlc20_salcc1_mod'].value_counts().index\n",
    "palette =  ['#CDAA66', '#728944', '#FFAA00', '#CD6666', '#E9FFBE'] #'deep'#['#1b9e77','#d95f02','#7570b3','#e7298a','#66a61e']\n",
    "# palette.reverse()\n",
    "\n",
    "g = sns.PairGrid(df, vars=ycol_dict.values(), hue='sanlc20_salcc1_mod',\n",
    "                 corner=True, height=1.2, diag_sharey=False,\n",
    "                 hue_order=hue_order)\n",
    "g.map_diag(sns.kdeplot, palette=palette)#, common_norm=False)\n",
    "# g.map_diag(sns.ecdfplot, stat=\"proportion\", alpha=.6, linewidth=1.5)\n",
    "g.map_lower(sns.histplot, hue=None, bins=50, cmap = 'YlGn', vmin=50, vmax=2000)\n",
    "lines = g.fig.axes[-1].get_lines()\n",
    "lines.reverse()\n",
    "g.fig.legend(lines, hue_order, loc=(0.63,0.75))\n",
    "\n",
    "ax = g.fig.add_axes((0.635, 0.64, 0.3, 0.06))\n",
    "norm = mpl.colors.Normalize(vmin=50, vmax=2000)\n",
    "g.fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=mpl.cm.YlGn),\n",
    "             cax=ax, orientation='horizontal')\n",
    "\n",
    "figname = \"gedi_metrics_pairplot\"\n",
    "for ext in [\".pdf\", \".svg\"]:\n",
    "    figpath = os.path.join(figdir, figname + ext)\n",
    "    g.fig.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd22d2a-96e6-4caf-ae93-9059ce327a55",
   "metadata": {},
   "source": [
    "## Model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b760396-a93a-4ae6-9676-a85cbfc387af",
   "metadata": {},
   "source": [
    "### All models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f025186-ef4d-4963-b28e-06ef215af720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and setup data\n",
    "path = r\"J:\\projects\\ECOFOR\\gedi\\models\\v08\\GEDI_2AB_2019to2023_leafon_sampy500m_all_oob_v08.parquet\"\n",
    "df = gpd.read_parquet(path)\n",
    "\n",
    "source_dict = {\n",
    "    'lt-p-s-t': 'LandTrendr + PALSAR + Soils + Topo',\n",
    "    'ccdcl30-p-s-t': '$CCDC^{L30}$ + PALSAR + Soils + Topo',\n",
    "    'ccdchls-p-s-t': '$CCDC^{HLS}$ + PALSAR + Soils + Topo',\n",
    "    'lt-p': 'LandTrendr + PALSAR',\n",
    "    'ccdcl30-p': '$CCDC^{L30}$ + PALSAR',\n",
    "    'ccdchls-p':'$CCDC^{HLS}$ + PALSAR',\n",
    "    'p-s-t': 'PALSAR + Soils + Topo',\n",
    "    'lt': 'LandTrendr',\n",
    "    'ccdcl30': '$CCDC^{L30}$',\n",
    "    'ccdchls': '$CCDC^{HLS}$',\n",
    "    'p': 'PALSAR',\n",
    "    's-t': 'Soils + Topography',\n",
    "}\n",
    "\n",
    "\n",
    "ydict = {'cover': 'Cover',\n",
    "         'rh98': 'RH98',\n",
    "         'fhd_normal': 'FHD'}\n",
    "Xsets = list(source_dict.keys())\n",
    "Ycols = list(ydict.keys())\n",
    "\n",
    "# make cover cols as percent\n",
    "df[df.columns[df.columns.str.contains('cover')]] *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941fa40-f99e-4096-b371-2563c75980db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get stats for all models\n",
    "sdf = pd.DataFrame(columns = pd.MultiIndex.from_product([list(ydict.values()), ['R2', 'RMSE', 'Bias', 'N']], names=(\"Metric\", \"Stat\")))\n",
    "sdf.index.name = 'Xset'\n",
    "\n",
    "for Xset in Xsets:\n",
    "    for ycol in Ycols:\n",
    "        x, y = df['pred_'+Xset+'_'+ycol], df[ycol]\n",
    "        \n",
    "        # Save stats\n",
    "        sdf.loc[Xset, (ydict[ycol], 'R2')] = r2_score(y, x)\n",
    "        sdf.loc[Xset, (ydict[ycol], 'RMSE')] = mean_squared_error(y, x)**0.5\n",
    "        sdf.loc[Xset, (ydict[ycol], 'Bias')] = (x-y).mean()\n",
    "        sdf.loc[Xset, (ydict[ycol], 'N')] = len(y)\n",
    "sdf = sdf.apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9ff1e6-aea3-4e5e-a3e3-2c8a8d9df374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and setup TCV data\n",
    "tcv_path = r\"J:\\projects\\ECOFOR\\gedi\\models\\v08\\GEDI_2AB_2019to2023_leafon_sampy500m_all_tcv_v08_stats.csv\"\n",
    "tdf = pd.read_csv(tcv_path)\n",
    "\n",
    "tdf = tdf[tdf['metric']!='pai'] # Drop PAI\n",
    "\n",
    "tdf['Metric'] = tdf['metric'].replace(ydict)\n",
    "tdf = tdf.rename(columns={'n':'N', 'r2':'R2', 'rmse':'RMSE', 'bias':'Bias'})\n",
    "tdf.loc[tdf['metric']=='cover', ['RMSE', 'Bias']] *= 100\n",
    "\n",
    "tcv = tdf.groupby(['Xset', 'Metric']).mean(numeric_only=True)\n",
    "tcv = tcv.drop(columns=['year'])\n",
    "tcv.columns.name = 'Stat'\n",
    "\n",
    "tldf = pd.melt(tcv, ignore_index=False, value_name='TCV').reset_index()\n",
    "# tldf['Source'] = tldf['Xset'].map(source_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bf409e-8fd3-44cc-8819-5a131b0906d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bar chart of accuracy stats\n",
    "\n",
    "# Make long form and add TCV stats\n",
    "ldf = pd.melt(sdf, ignore_index=False).reset_index()\n",
    "ldf = pd.merge(ldf, tldf, how='left', on=['Xset', 'Metric', 'Stat'])\n",
    "ldf['Source'] = ldf['Xset'].map(source_dict)\n",
    "\n",
    "mask = ldf['Stat'].isin(['R2', 'RMSE', 'Bias'])\n",
    "\n",
    "# palette = ['#a6cee3','#b2df8a','#cab2d6', '#1f78b4','#33a02c']\n",
    "p = sns.color_palette(palette='tab20c')\n",
    "palette = p[0:9:4]+p[1:10:4]+[p[12]]+p[2:11:4]+[p[13], p[16]]\n",
    "\n",
    "g = sns.catplot(data=ldf[mask], x=\"value\", y=\"Source\", row=\"Stat\", col=\"Metric\", kind='bar',\n",
    "                sharex=False, sharey=True, height=3, aspect=1.3, margin_titles=True, palette=palette)\n",
    "\n",
    "# Overlay point plot with TCV stats\n",
    "g.map(sns.pointplot, \"TCV\", \"Source\", marker=\"o\", join=False, color=\"k\")\n",
    "\n",
    "# Clean up plot\n",
    "g.set_titles(col_template=\"{col_name}\", row_template=\"{row_name}\")\n",
    "g.set_ylabels(\"\")\n",
    "\n",
    "figname = \"gedi_acc_all\"\n",
    "for ext in [\".pdf\", \".svg\"]:\n",
    "    figpath = os.path.join(figdir, figname + ext)\n",
    "    g.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dca50e-d603-4238-a0cc-f9f48a35b34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent difference of RMSE for each optical only model from the mean RMSE of those models\n",
    "opt_rmse = sdf.loc[['lt', 'ccdcl30', 'ccdchls'], (slice(None), 'RMSE')]\n",
    "100 * opt_rmse.subtract(opt_rmse.mean()).divide(opt_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b67f5f-562e-4452-ab41-e35b3fde18f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent difference of chosen model RMSE from best model\n",
    "opt_rmse = sdf.loc[['lt-p-s-t', 'ccdcl30-p-s-t', 'ccdchls-p-s-t'], (slice(None), 'RMSE')]\n",
    "100 * opt_rmse.subtract(opt_rmse.mean()).divide(opt_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bce68ae-3f77-4081-a9d0-d744fc3b4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of LandTrendr only to PALSAR only\n",
    "lt_minus_p_r2 = sdf.loc['lt', (slice(None), 'R2')] - sdf.loc['p', (slice(None), 'R2')]\n",
    "display(lt_minus_p_r2)\n",
    "print(\"LandTrendr explained\", (lt_minus_p_r2.mean()*100).round(), \"% more variance on average than PALSAR.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff98bef9-6a93-4056-804d-b311b2bfb0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change in RMSE when adding PALSAR and Soils and Topo\n",
    "opt_sar_rmse = sdf.loc[['lt-p', 'ccdcl30-p', 'ccdchls-p'], (slice(None), 'RMSE')]\n",
    "opt_sar_rmse = opt_sar_rmse.set_index(opt_rmse.index)\n",
    "mean_opt_sar_chg = np.nanmean(100* opt_sar_rmse.subtract(opt_rmse).divide(opt_rmse)).round(1)\n",
    "print(\"RMSE changed\", mean_opt_sar_chg, \"% on average when adding PALSAR predictors to optical predictors\")\n",
    "\n",
    "# Change in RMSE when adding PALSAR and Soils and Topo\n",
    "opt_pst_rmse = sdf.loc[['lt-p-s-t', 'ccdcl30-p-s-t', 'ccdchls-p-s-t'], (slice(None), 'RMSE')]\n",
    "opt_pst_rmse = opt_pst_rmse.set_index(opt_rmse.index)\n",
    "mean_opt_pst_chg = np.nanmean(100* opt_pst_rmse.subtract(opt_rmse).divide(opt_rmse)).round(1)\n",
    "print(\"RMSE changed\", mean_opt_pst_chg, \"% on average when adding PALSAR and soil and topography predictors to optical predictors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c438da-b934-4cdd-a381-ff02a8a26164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get difference between TCV stats and OOB stats\n",
    "\n",
    "# Reshape TCV stats to match OOB stats (sdf)\n",
    "tcv_wide = tcv.unstack(level=1)\n",
    "tcv_wide.columns = tcv_wide.columns.swaplevel()\n",
    "tcv_wide.sort_index(axis=1, level=0, inplace=True)\n",
    "\n",
    "sdif = tcv_wide.subtract(sdf)\n",
    "spct = sdif.divide(sdf) * 100\n",
    "\n",
    "r2_dif_mean = np.nanmean(sdif.loc[:,(slice(None), 'R2')]).round(2)\n",
    "print(\"TCV R2 different from OOB R2 by\", r2_dif_mean, \"on average.\")\n",
    "\n",
    "rmse_pct_dif_mean = np.nanmean(spct.loc[:,(slice(None), 'RMSE')]).round(2)\n",
    "print(\"TCV RMSE different from OOB RMSE by\", rmse_pct_dif_mean, \"% on average.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220aaae2-f6d2-4aa5-bd1e-20b9e9deb204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias as % of mean observed value\n",
    "# Mean observed value\n",
    "mean_obs = df[['cover', 'rh98', 'fhd_normal']].mean()\n",
    "mean_obs.index = ['Cover', 'RH98', 'FHD']\n",
    "\n",
    "# OOB absolute bias\n",
    "oob_bias = sdf.loc[:,(slice(None), 'Bias')].droplevel(1, axis=1).abs()\n",
    "oob_bias_pct = oob_bias / mean_obs * 100\n",
    "\n",
    "# TCV absolute bias\n",
    "tcv_bias = tcv_wide.loc[:,(slice(None), 'Bias')].droplevel(1, axis=1).abs()\n",
    "tcv_bias_pct = tcv_bias / mean_obs * 100\n",
    "\n",
    "def get_df_max(df):\n",
    "    col_max = df.max().idxmax()\n",
    "    row_max = df[col_max].idxmax()\n",
    "    max_val = df.loc[row_max, col_max]\n",
    "    return max_val, (row_max, col_max)\n",
    "\n",
    "val, (model, metric) = get_df_max(oob_bias_pct)\n",
    "print(\"OOB absolute bias maximum as a percent of the mean observed value was\", val.round(1), \"% for\", model, metric)\n",
    "\n",
    "val, (model, metric) = get_df_max(tcv_bias_pct)\n",
    "print(\"TCV absolute bias maximum  as a percent of the mean observed value was\", val.round(1), \"% for\", model, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a320e90-9a03-4ae6-87df-489695d3cb99",
   "metadata": {},
   "source": [
    "### Chosen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004633e-1e3f-4f66-874e-e968af9b3780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Obs vs pred for only the best model\n",
    "# 1x4\n",
    "Xset = 'lt-p-s-t'\n",
    "ycol_dict = {'cover':'Cover (%)','pai':'PAI', 'rh98':'RH98 (m)', 'fhd_normal':'FHD'}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(2.15*3, 1.5))\n",
    "\n",
    "for i, (ycol, ax) in enumerate(zip(Ycols, axes.flat)):\n",
    "    x, y = df['pred_'+Xset+'_'+ycol], df[ycol]\n",
    "    hb = ax.hexbin(x, y, gridsize=20, mincnt=1, cmap='magma_r', linewidths=0, edgecolor='none', vmax=2000)\n",
    "    ax.plot((y.min(), y.max()), (y.min(),y.max()), '--k')\n",
    "\n",
    "    r2 = r2_score(y, x)\n",
    "    bias = (x-y).mean()\n",
    "    rmse = mean_squared_error(y, x)**0.5\n",
    "\n",
    "    # add text\n",
    "    ax.text(0.99, 0.22, \"R$^2$= \" + str(np.round(r2, 2)), transform=ax.transAxes, ha='right')\n",
    "    ax.text(0.99, 0.13, \"Bias= \"+\"{:.2f}\".format(np.round(bias, 2)), transform=ax.transAxes, ha='right')\n",
    "    ax.text(0.99, 0.02,  \"RMSE= \" + str(np.round(rmse, 2)), transform=ax.transAxes, ha='right')\n",
    "\n",
    "    ax.set(title=ycol_dict[ycol])\n",
    "    if i==0:\n",
    "        ax.set(ylabel='Observed')\n",
    "    if i==1:\n",
    "        ax.set(xlabel='Predicted')\n",
    "\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "cb = fig.colorbar(hb, ax=axes, location='right', orientation='vertical', pad=0.02)#, shrink=True, aspect=16, pad=0.02) #cax=cax, aspect=)#\n",
    "cb.set_label('Count')\n",
    "\n",
    "figname = \"gedi_acc_\" + Xset\n",
    "for ext in [\".pdf\", \".svg\"]:\n",
    "    figpath = os.path.join(figdir, figname + ext)\n",
    "    fig.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf6e50-cecb-42ca-a767-c14038db8fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosen model verseus model with lowest RMSE\n",
    "chosen = \"lt-p-s-t\"\n",
    "best_rmse = pd.concat([sdf.loc[:,(slice(None), \"RMSE\")].min(), \n",
    "                       sdf.loc[:,(slice(None), \"RMSE\")].idxmin()], axis=1, keys=['RMSE', 'Xset'])\n",
    "\n",
    "print(\"Model with lowest RMSE\")\n",
    "display(best_rmse)\n",
    "\n",
    "print(\"Difference of chosen model from best model\")\n",
    "display(sdf.loc[chosen, (slice(None), \"RMSE\")] - best_rmse[\"RMSE\"])\n",
    "\n",
    "print(\"Percent difference of chosen model from best model\")\n",
    "display((sdf.loc[chosen, (slice(None), \"RMSE\")] - best_rmse[\"RMSE\"]) / best_rmse[\"RMSE\"] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b79de2-aa99-4982-af77-612c774213ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of CCDC and LandTrendr variables\n",
    "lt_path = r\"J:\\projects\\ECOFOR\\gedi\\extracted\\GEDI_2AB_2019to2023_leafon_sampy500m_lt.csv\"\n",
    "ccdc_path = r\"J:\\projects\\ECOFOR\\gedi\\extracted\\GEDI_2AB_2019to2023_leafon_sampy500m_l30s2_ccdc.csv\"\n",
    "\n",
    "lt = pd.read_csv(lt_path, nrows=0).columns\n",
    "ccdc = pd.read_csv(ccdc_path, nrows=0).columns\n",
    "\n",
    "# Filter to same columns used in modeling\n",
    "lt = lt.drop('shot_number')\n",
    "bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2'] #'ca', \n",
    "ccdc = [col for col in ccdc for band in bands if col.startswith(band)] # using CCDC\n",
    "\n",
    "print(\"LandTrendr used\", len(lt), \"variables.\")\n",
    "print(\"CCDC used\", len(ccdc), \"variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d482cc-1d0f-4435-b035-cd494681f8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of predicted and observed for chosen model\n",
    "pred_cov_lt_1pct = df.loc[df['cover']<1, 'pred_'+chosen+'_cover'].mean()\n",
    "print(\"Observed cover <1% was predicted as\", pred_cov_lt_1pct, \"% on average.\")\n",
    "\n",
    "mask = df['cover']>50\n",
    "pred_cov_err_mask = (df.loc[mask, 'pred_'+chosen+'_cover'] - df.loc[mask, 'cover']).mean()\n",
    "print(\"Observed cover >50% was overestimated by\", pred_cov_err_mask, \"% on average.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e11ecf-552a-477d-b685-29b40e42b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df['rh98']>15\n",
    "(df.loc[mask, 'pred_'+chosen+'_rh98'] - df.loc[mask, 'rh98']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598458b-59cc-4c87-9a55-6a4b9ce95adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TCV stats for chosen model\n",
    "tldf[tldf['Xset']=='lt-p-s-t'].pivot(index=['Xset', 'Metric'], columns='Stat', values='TCV').round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97f123-2776-4983-a741-37942e0761f5",
   "metadata": {},
   "source": [
    "### Bias correction\n",
    "Bias correction results for chosen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6494b12-daaf-48c8-ae96-56cc64d4534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"J:\\projects\\ECOFOR\\gedi\\models\\v08BC\\GEDI_2AB_2019to2023_leafon_sampy500m_all_bc_v08BC.parquet\"\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "ycol_dict = {'cover':'Cover (%)', 'rh98':'RH98 (m)', 'fhd_normal':'FHD'}\n",
    "Ycols=list(ycol_dict.keys())\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(2.15*3, 3.1))\n",
    "\n",
    "for pred_type, axrow in zip(['pred', 'pred_bc'], axes):\n",
    "    for i, (ycol, ax) in enumerate(zip(Ycols, axrow)):\n",
    "        x, y = df[pred_type+'_'+ycol], df[ycol]\n",
    "        hb = ax.hexbin(x, y, gridsize=20, mincnt=1, cmap='magma_r', linewidths=0, edgecolor='none', vmax=1000)\n",
    "        ax.plot((y.min(), y.max()), (y.min(),y.max()), '--k')\n",
    "\n",
    "        r2 = r2_score(y, x)\n",
    "        bias = (x-y).mean()\n",
    "        rmse = mean_squared_error(y, x)**0.5\n",
    "\n",
    "        # add text\n",
    "        ax.text(0.99, 0.22, \"R$^2$= \" + str(np.round(r2, 2)), transform=ax.transAxes, ha='right')\n",
    "        ax.text(0.99, 0.13, \"Bias= \"+\"{:.2f}\".format(np.round(bias, 2)), transform=ax.transAxes, ha='right')\n",
    "        ax.text(0.99, 0.02,  \"RMSE= \" + str(np.round(rmse, 2)), transform=ax.transAxes, ha='right')\n",
    "        ytext = \"Original\" if pred_type==\"pred\" else \"Bias-corrected\"\n",
    "        \n",
    "        if pred_type=='pred':\n",
    "            ax.set(title=ycol_dict[ycol])\n",
    "            ax.set_xticklabels([])\n",
    "        if i==0:\n",
    "            ax.set(ylabel='Observed')\n",
    "            ax.text(-0.5, 0.5, ytext, transform=ax.transAxes, ha='center', va='center', rotation='vertical', fontsize=10, fontweight='bold')\n",
    "        if i==1 and pred_type=='pred_bc':\n",
    "            ax.set(xlabel='Predicted')\n",
    "            \n",
    "\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "cb = fig.colorbar(hb, ax=axes, location='right', orientation='vertical', pad=0.02)#, shrink=True, aspect=16, pad=0.02) #cax=cax, aspect=)#\n",
    "cb.set_label('Count')\n",
    "\n",
    "figname = \"gedi_acc_biascorrection\"\n",
    "for ext in [\".pdf\", \".svg\"]:\n",
    "    figpath = os.path.join(figdir, figname + ext)\n",
    "    fig.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b380d0f-c443-4b21-b70b-3ccb9b031174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions of the predictions and observations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(6.5, 2))\n",
    "\n",
    "for i, (ycol, ax) in enumerate(zip(Ycols, axes)):\n",
    "    pred_col = 'pred_'+ycol\n",
    "    bc_col = 'pred_bc_'+ycol\n",
    "    xydf = pd.melt(df[[pred_col, bc_col, ycol]])\n",
    "    sns.ecdfplot(xydf, x='value', hue='variable', ax=ax, legend=False)\n",
    "\n",
    "    # ks_2samp silently gives wrong values if nan's included, so make sure they're removed\n",
    "    mask = df[[pred_col, ycol]].notna().all(axis=1)\n",
    "    ks, pval = ks_2samp(df.loc[mask, pred_col], df.loc[mask, ycol])\n",
    "    ax.text(0.95, 0.16, f\"Orig & Obs KS= {np.round(ks,2)}\", ha='right', transform=ax.transAxes)\n",
    "    \n",
    "    mask = df[[bc_col, ycol]].notna().all(axis=1)\n",
    "    ks, pval = ks_2samp(df.loc[mask, bc_col], df.loc[mask, ycol])\n",
    "    ax.text(0.95, 0.1, f\"BC & Obs KS= {np.round(ks,2)}\", ha='right', transform=ax.transAxes)\n",
    "    \n",
    "    mask = df[[bc_col, pred_col]].notna().all(axis=1)\n",
    "    ks, pval = ks_2samp(df.loc[mask, bc_col], df.loc[mask, pred_col])\n",
    "    ax.text(0.95, 0.03, f\"BC & Orig KS= {np.round(ks,2)}\", ha='right', transform=ax.transAxes)\n",
    "\n",
    "    ax.set(ylabel=None, title=ycol_dict[ycol])\n",
    "        \n",
    "orange_line = mpl.lines.Line2D([0], [0], color='orange', lw=2)\n",
    "blue_line = mpl.lines.Line2D([0], [0], color='blue', lw=2)\n",
    "green_line = mpl.lines.Line2D([0], [0], color='green', lw=2)\n",
    "fig.legend([orange_line, blue_line, green_line], ['Bias-corrected Prediction', 'Original Prediction', 'Observed'], loc='center', bbox_to_anchor=(0.5,-0.15), ncol=3)\n",
    "\n",
    "figname = \"gedi_bc_ecdf\"\n",
    "for ext in [\".pdf\", \".svg\"]:\n",
    "    figpath = os.path.join(figdir, figname + ext)\n",
    "    fig.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d347fa-f8ab-4f56-8969-9842957ca65e",
   "metadata": {},
   "source": [
    "## Field evaluation\n",
    "Compare the field measurements to GEDI footprints and predicted maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95514bc5-b576-46d0-9668-43d86e795daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_regplot(x, y, ax, lims=None, reg=True, oneone=True, **kwargs):\n",
    "    from scipy.stats import linregress\n",
    "    \n",
    "    sns.regplot(x=x, y=y, ax=ax, **kwargs)\n",
    "\n",
    "    if reg:\n",
    "        # Regression\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "        rmse = mean_squared_error(y, x*slope+intercept)**0.5\n",
    "        eq = \"y = \" + str(np.round(slope,2)) + \"x + \" + str(np.round(intercept, 2))\n",
    "        ax.text(0.97, 0.26, \"Regression:\", transform=ax.transAxes, ha='right')\n",
    "        ax.text(0.98, 0.17, eq, transform=ax.transAxes, ha='right')\n",
    "        ax.text(0.98, 0.09, \"R$^2$= \" + str(np.round(r_value**2, 2)), transform=ax.transAxes, ha='right')\n",
    "        ax.text(0.98, 0.01, \"RMSE= \"+str(np.round(rmse, 2)), transform=ax.transAxes, ha='right')\n",
    "\n",
    "    if oneone:\n",
    "        if lims is None:\n",
    "            lims = (0, np.nanmax(x.append(y))) #np.nanmin(x.append(y))\n",
    "        ax.plot(lims, lims, '--k')\n",
    "        ax.set(ylim=lims, xlim=lims)\n",
    "\n",
    "        # add text for R2 and RMSE\n",
    "        r2 = r2_score(y, x)\n",
    "        rmse = mean_squared_error(y, x)**0.5\n",
    "        bias = (x-y).mean()\n",
    "        ax.text(0.03, 0.93, \"1:1 stats:\", transform=ax.transAxes)\n",
    "        ax.text(0.03, 0.85,\"R$^2$= \"+str(np.round(r2, 2)), transform=ax.transAxes)\n",
    "        ax.text(0.03, 0.77, \"RMSE= \"+str(np.round(rmse, 2)), transform=ax.transAxes)\n",
    "        ax.text(0.03, 0.69, \"Bias= \"+str(np.round(bias, 2)), transform=ax.transAxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b962cd5-e113-4f63-bc86-636f7ca9866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot max tree height compared to GEDI's RH98\n",
    "path = r\"J:\\projects\\ECOFOR\\field\\merged\\gedi_trees_cover_simp.csv\"\n",
    "df = pd.read_csv(path, index_col='plot_ix')\n",
    "\n",
    "# Load plot notes to drop bad plots\n",
    "path = r\"J:\\projects\\ECOFOR\\field\\merged\\plot_notes.csv\"\n",
    "pdf = pd.read_csv(path, index_col='plot_ix')\n",
    "\n",
    "df[['exclude_plot', 'exclude_reason']] = pdf[['exclude_plot', 'exclude_reason']]\n",
    "df = df[~df['exclude_plot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50c75db-7dd6-4a0a-826d-1f76c60f7e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get predicted RH98 for 2022 for the location of the tallest measured tree for comparison\n",
    "trees = gpd.read_file(r\"J:\\projects\\ECOFOR\\field\\merged\\gedi_all_merged.gpkg\", layer=\"trees\")\n",
    "rast_path = r\"J:\\projects\\ECOFOR\\gedi\\maps\\v08\\lt-p-s-t\\rh98\\rh98_2022.tif\"\n",
    "\n",
    "with rasterio.open(rast_path) as src:\n",
    "    trees = trees.to_crs(src.crs)\n",
    "trees = trees.dropna(subset=['hgt', 'geometry'])\n",
    "\n",
    "trees['pred_rh98'] = list(gen_point_query(trees, rast_path, interpolate='nearest'))\n",
    "\n",
    "# Get tallest tree of kept plots\n",
    "trees = trees[trees['plot_ix'].isin(df.index.values)]\n",
    "tallest_ix = trees.groupby('plot_ix')['hgt'].idxmax().dropna()\n",
    "trees = trees.loc[tallest_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a86236-bbef-4f8f-85e3-ac1eb1cff4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make figure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(4,2))\n",
    "present_regplot(df['rh98'], df['hgt'], lims=(0,30), ax=ax1, scatter_kws={'s': 15, 'zorder':2, 'alpha':0.5}, line_kws={'color':'k', 'alpha':0.5, 'zorder':1})\n",
    "present_regplot(trees['pred_rh98'], trees['hgt'], lims=(0,30), ax=ax2, scatter_kws={'s': 15, 'zorder':2, 'alpha':0.5}, line_kws={'color':'k', 'alpha':0.5, 'zorder':1})\n",
    "ax1.set(xlabel='RH98 (m)', ylabel='Max tree height (m)')\n",
    "ax2.set(xlabel='Predicted RH98 (m)', ylabel='Max tree height (m)')\n",
    "fig.tight_layout()\n",
    "\n",
    "figname = \"field_height\"\n",
    "for ext in [\".pdf\", \".svg\"]: \n",
    "    figpath = os.path.join(figdir, figname + ext)\n",
    "    fig.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aab4ff-f054-4c45-a179-4698afb6a001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot without the tall tree observation\n",
    "fig, ax = plt.subplots(figsize=(2,2))\n",
    "mask = trees['plot_ix']!=14\n",
    "present_regplot(trees.loc[mask, 'pred_rh98'], trees.loc[mask, 'hgt'], lims=(0,15), ax=ax, scatter_kws={'s': 15, 'zorder':2, 'alpha':0.5}, line_kws={'color':'k', 'alpha':0.5, 'zorder':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1705b142-7589-4042-837d-d5ce05b127e9",
   "metadata": {},
   "source": [
    "## Climate sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc36df3-0863-4da5-938c-001025604e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LandTrendr and predictions and make long form for plotting\n",
    "path = r\"J:\\projects\\ECOFOR\\climate_sensitivity\\vca_unburned09to17_snapped_pi_extract.gpkg\"\n",
    "\n",
    "df = gpd.read_file(path)\n",
    "\n",
    "df = df[df[\"change\"]==\"none\"] # only analyze no change sites\n",
    "\n",
    "srcs = list(indirs.keys())\n",
    "value_vars = [col for col in df.columns for src in srcs if col.startswith(src)]\n",
    "id_vars = [col for col in df.columns if col not in value_vars]\n",
    "\n",
    "ldf = (\n",
    "    df.melt(\n",
    "        id_vars=id_vars,\n",
    "        value_vars=value_vars,\n",
    "        var_name='original_column_name', # Create a temporary column for old names\n",
    "        value_name='value' # The required 'value' column\n",
    "    )\n",
    "    # Split the temporary column into the required new columns\n",
    "    .assign(\n",
    "        source = lambda x: x['original_column_name'].str.split('_').str[0],\n",
    "        year = lambda x: x['original_column_name'].str.split('_').str[1].astype(int), # Convert to int\n",
    "        band = lambda x: x['original_column_name'].str.split('_').str[2]\n",
    "    )\n",
    "    # Drop the temporary column\n",
    "    .drop(columns=['original_column_name'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3156eeb-5493-49aa-86f3-5215321fe276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot all sites for cover\n",
    "bdf = ldf[(ldf[\"band\"]==\"cover\")]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3,1.5))\n",
    "sns.lineplot(data=bdf, x=\"year\", y=\"value\", hue=\"site\", linewidth=0.5, legend=False, palette=\"tab20\", ax=ax)\n",
    "ax.axvspan(xmin=2010.5, xmax=2014.5, facecolor='gray', alpha=0.3, label='No predictions')\n",
    "ax.axvline(2015, linestyle='--', c='k')\n",
    "ax.text(2014, 0.32, '2015 drought')\n",
    "ax.set(xlabel=\"Cover (%)\", ylabel=\"Year\")\n",
    "\n",
    "figname = \"climate_sensitivity_cover\"\n",
    "for ext in [\".svg\"]:\n",
    "    figpath = os.path.join(figdir, figname + ext)\n",
    "    fig.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3671ba-2316-4cd4-85db-580a83b934e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get important predictors to decide which ones to plot\n",
    "path = r\"J:\\projects\\ECOFOR\\gedi\\models\\v08\\GEDI_2AB_2019to2023_leafon_sampy500m_all_imps_v08.csv\"\n",
    "idf = pd.read_csv(path, header=[0,1])\n",
    "idf = idf['lt-p-s-t_cover']\n",
    "idf.mean().sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecac962-9077-49bf-a46f-a181e88a9be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select LandTrendr fits for demonstration\n",
    "combos = [\n",
    "    (521, \"wet\", \"ndvi\", 2007), \n",
    "    (1704, \"wet\", \"green\", 2007),\n",
    "    (1704, \"wet\", \"green\", 1984)\n",
    "]\n",
    "\n",
    "for (site, season, band, starty) in combos:\n",
    "    # Load base landsat composite values for comparison\n",
    "    path = r\"J:\\projects\\ECOFOR\\climate_sensitivity\\vca_unburned09to17_snapped_pi_landsat_\"+season+\"2.csv\"\n",
    "    odf = pd.read_csv(path)\n",
    "    odf = odf[(odf[\"site\"] == site) & (odf[band]!=-32768) & (odf[\"year\"]>starty)]\n",
    "    \n",
    "    mask = (ldf[\"site\"]==site) & (ldf[\"source\"]==\"lt\"+season) & (ldf[\"band\"]==band) & (ldf[\"year\"]>starty)\n",
    "    bdf = ldf[mask].copy()\n",
    "    bdf[\"value\"] = bdf[\"value\"] / 1000 # rescale to real value\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(3,1.5))\n",
    "    sns.lineplot(data=bdf, x=\"year\", y=\"value\", linewidth=1, legend=False, ax=ax)\n",
    "    sns.scatterplot(data=odf, x=\"year\", y=band, size=1, legend=False, ax=ax)\n",
    "    ax.set(xlabel=\"Year\", ylabel=band)\n",
    "    \n",
    "    figname = \"lt_vs_landsat_\"+str(site)+\"_\"+season+\"_\"+band+\"_\"+str(starty)\n",
    "    for ext in [\".svg\"]:\n",
    "        figpath = os.path.join(figdir, figname + ext)\n",
    "        fig.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ed0a0e-6eb3-4c8e-951e-ba08faa1f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take difference between all pairs of subsequent years\n",
    "bdf_s = bdf.sort_values(by=['site', 'year']).reset_index(drop=True)\n",
    "bdf_s['valdif'] = bdf_s.groupby('site')['value'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd226a4-60c9-4d5a-a34a-0059fc8e3432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average change across all year pairs\n",
    "bdf_s.groupby('year')['valdif'].mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b283a7c-4f3f-4158-8578-803bee68053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average change for 2010 to 2015\n",
    "bdf_s[bdf_s[\"year\"]==2015].set_index(\"site\")[\"valdif\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a74d85e-ef13-465e-8b18-81f9fe01975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average change for 2015 to 2016\n",
    "bdf_s[bdf_s[\"year\"]==2016].set_index(\"site\")[\"valdif\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a4fa93-8344-4ea5-badb-6439fb10596d",
   "metadata": {},
   "source": [
    "## Small Area Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb04f7b-28c2-4062-a316-47794980b823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get model-based estimators exported from R\n",
    "path =  r\"J:\\projects\\ECOFOR\\gedi\\sae\\sae_gedi_estimates_20251103.csv\"\n",
    "package = \"emdi\" #\"sae\" # \n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df = df[df['metric']!='pai'] # drop use of PAI\n",
    "\n",
    "if package==\"sae\":\n",
    "    df = df.rename(columns={\"mean\":\"Mean\", \"domain\":\"Domain\", \"mse\":\"Mean_MSE\"})\n",
    "\n",
    "df['metric_title'] = df['metric'].replace({'cover':'Cover (%)', 'rh98': 'RH98 (m)', 'fhd': 'FHD', 'pai':'PAI'})\n",
    "df['aoi'] = df['Domain'].str[:-5]\n",
    "df['year'] = df['Domain'].str[-4:].astype(int)\n",
    "\n",
    "# Fix cover to be in percent for plotting and get confidence intervals\n",
    "df['mean_rmse'] = df['Mean_MSE']**0.5\n",
    "df.loc[df['metric']=='cover', ['Mean', 'mean_rmse']] *= 100\n",
    "t_val = 1.645 # critical value for 90% CI from t-distribution with inf degrees of freedom\n",
    "df['mean_ci90_half'] = (df['mean_rmse'] * 1.645) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f48b6-619f-4124-b639-4e4e4e0d6dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make one figure of all metrics of pre/post for each AOI\n",
    "aoi_years = {\n",
    "    'thornybush':{'pre':2017, 'post':2021},\n",
    "    'bushbuckridge_a':{'pre':2007, 'post':2021},\n",
    "    'plantation_a':{'pre':2017, 'post':2021},\n",
    "    'skukuza_se':{'pre':2007, 'post':2022},\n",
    "            }\n",
    "for aoi, ydict in aoi_years.items():\n",
    "    print(aoi)\n",
    "    pdf = df[df['aoi']==aoi]\n",
    "    pdf = pdf[pdf['year'].isin([ydict['pre'], ydict['post']])]\n",
    "    display(pdf)\n",
    "\n",
    "    def errplot(x, y, yerr, **kwargs):\n",
    "        ax = plt.gca()\n",
    "        data = kwargs.pop(\"data\")\n",
    "        data.plot(x=x, y=y, yerr=yerr, kind=\"bar\", ax=ax, capsize=3, **kwargs)\n",
    "\n",
    "    g = sns.FacetGrid(pdf, col=\"metric_title\", sharey=False, height=2, aspect=0.5)\n",
    "    g.map_dataframe(errplot, \"year\", \"Mean\", \"mean_ci90_half\")\n",
    "    g.set_xlabels(\"\")\n",
    "    g.set_xticklabels(rotation=45, ha='right', rotation_mode='anchor')\n",
    "    g.set_titles(template=\"{col_name}\")\n",
    "    g.tight_layout()\n",
    "    \n",
    "    figname = \"gedi_sae_\" + aoi + str(ydict['pre'])+\"_\"+str(ydict['post'])\n",
    "    for ext in [\".pdf\", \".svg\"]:\n",
    "        figpath = os.path.join(figdir, figname + ext)\n",
    "        g.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45531aae-0c35-466c-89f6-647cf3b58fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df['aoi']=='bushbuckridge_a') & (df['metric']=='cover')\n",
    "calc_chg = df.loc[mask & (df['year']==2021), 'mean'].iloc[0] - df.loc[mask & (df['year']==2007), 'mean'].iloc[0]\n",
    "print('Cover in bushbuckridge changed', np.round(calc_chg, 1), '%')\n",
    "\n",
    "mask = (df['aoi']=='thornybush') & (df['metric']=='cover')\n",
    "calc_chg = df.loc[mask & (df['year']==2021), 'mean'].iloc[0] - df.loc[mask & (df['year']==2017), 'mean'].iloc[0]\n",
    "print('Cover in thornybush changed', np.round(calc_chg, 1), '%')\n",
    "\n",
    "mask = (df['aoi']=='thornybush') & (df['metric']=='rh98')\n",
    "calc_chg = df.loc[mask & (df['year']==2021), 'mean'].iloc[0] - df.loc[mask & (df['year']==2017), 'mean'].iloc[0]\n",
    "print('RH98 in thornybush changed', np.round(calc_chg, 1), 'm')\n",
    "\n",
    "mask = (df['aoi']=='skukuza_se') & (df['metric']=='cover')\n",
    "calc_chg = df.loc[mask & (df['year']==2022), 'mean'].iloc[0] - df.loc[mask & (df['year']==2007), 'mean'].iloc[0]\n",
    "print('Cover in skukuza changed', np.round(calc_chg, 1), '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ecofor]",
   "language": "python",
   "name": "conda-env-ecofor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
