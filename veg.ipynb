{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60beb5ec-a2e8-44ca-82eb-96867264a172",
   "metadata": {},
   "source": [
    "# Ecofor veg analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2760e2-1fa9-42a7-9b51-a8f6073e533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, shapely, joblib, sys, subprocess, rasterio\n",
    "# os.environ['USE_PYGEOS'] = '0'\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from osgeo import ogr\n",
    "from joblib import Parallel, delayed\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from rasterstats import zonal_stats, gen_point_query\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import r2_score, mean_squared_error, classification_report\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "\n",
    "# gpd.options.io_engine = \"pyogrio\" # need to update to use this for faster i/o\n",
    "# os.environ[\"PYOGRIO_USE_ARROW\"] = 1\n",
    "\n",
    "# sys.path.append(r'J:\\users\\stevenf\\code\\language\\python')\n",
    "# from sfgeo.raster_bounds import aoi_raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb129a6-f05d-4455-97b6-7b036a662c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run list of commands with concurrent threads\n",
    "# TODO: look for better library to do this (joblib?) or make better non-blocking, reusable function with progress bar.\n",
    "def cmd_concurrent(cmds, threads=1): \n",
    "    from subprocess import Popen\n",
    "    from itertools import islice\n",
    "    \n",
    "    processes = (Popen(cmd, shell=True) for cmd in cmds)\n",
    "    running_processes = list(islice(processes, threads))  # start new processes\n",
    "    while running_processes:\n",
    "        for i, process in enumerate(running_processes):\n",
    "            if process.poll() is not None:  # the process has finished\n",
    "                running_processes[i] = next(processes, None)  # start new process\n",
    "                if running_processes[i] is None: # no new processes\n",
    "                    del running_processes[i]\n",
    "                    break\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fdb3fa-5c40-47c6-b846-3f0a2510a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_matrix(y_true, y_pred, normalize=True, outline_diag=True, labels=None):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    if labels is None:\n",
    "        names = np.unique(pd.concat([y_true, y_pred]))\n",
    "    else:\n",
    "        names=labels\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    fmt = \"d\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fmt = \"0.2f\"\n",
    "    cm = pd.DataFrame(cm, index = names, columns= names)\n",
    "    cm.index.name = \"Observed\"\n",
    "    cm.columns.name = \"Predicted\"\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=fmt, cmap=\"viridis\", ax=ax);\n",
    "    \n",
    "    if outline_diag:\n",
    "        for i in range(len(cm)):\n",
    "            ax.plot([i, i, i+1, i+1, i], [i, i+1, i+1, i, i], color=\"Red\", linewidth=2)\n",
    "   \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf613e-00a1-4294-b533-f890f1839ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_pred_hexbin(y, x, folds=None, vmax=100):\n",
    "    \"\"\"y=true, x=pred, \n",
    "       k = Series of fold index in x and y used in K-fold cross-validation. Calculate mean error across folds if given.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(4.5,3.75))   #(3, 2.5)\n",
    "    hb = ax.hexbin(x, y, gridsize=20, mincnt=1, cmap='magma_r', linewidths=0, edgecolor='none', vmax=vmax)\n",
    "    cb = fig.colorbar(hb, ax=ax)\n",
    "    cb.set_label('counts')\n",
    "    ax.plot((y.min(), y.max()), (y.min(),y.max()), '--k')\n",
    "    \n",
    "    if folds is not None:\n",
    "        fdf = pd.DataFrame({'true':y, 'pred':x, 'fold':folds})\n",
    "        folded = fdf.groupby('fold')\n",
    "        r2 = folded.apply(lambda g: r2_score(g['true'], g['pred'])).mean()\n",
    "        bias = folded.apply(lambda g: (g['pred'] - g['true']).mean()).mean()\n",
    "        rmse = folded.apply(lambda g: mean_squared_error(g['true'], g['pred'])**0.5).mean()\n",
    "    else:\n",
    "        r2 = r2_score(y, x)\n",
    "        bias = (x-y).mean()\n",
    "        rmse = mean_squared_error(y, x)**0.5\n",
    "    \n",
    "    # add text\n",
    "    ax.text(0.99, 0.22, \"R$^2$= \" + str(np.round(r2,2)), transform=ax.transAxes, ha='right')\n",
    "    ax.text(0.99, 0.13, \"Bias= \"+str(np.round(bias, 2)), transform=ax.transAxes, ha='right')\n",
    "    ax.text(0.99, 0.02,  \"RMSE= \" + str(np.round(rmse, 2)), transform=ax.transAxes, ha='right')\n",
    "    ax.set(xlabel='Predicted', ylabel='Observed')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9c13c3-b754-4b4c-b79f-605cc271a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyramid and stats helper function\n",
    "# If pyramids are generated for the tiles before creating VRT mosaics then gdalbuildvrt will recognize the \n",
    "# presence of the pyramids and add a line in the XML file to use them. They can then be used for doing \n",
    "# approximate statistics too.\n",
    "def pyr_stats(path, nodata=None, run=True):\n",
    "    \"\"\"Set nodata (str of number or 'nan'). Calculate stats and pyramids for image at path (str).\"\"\"\n",
    "    cmds = {'stats':[], 'pyr':[]}\n",
    "    if nodata:\n",
    "        cmd = 'rio edit-info --nodata ' + str(nodata) + ' ' + path\n",
    "        result = subprocess.check_output(cmd)\n",
    "    \n",
    "    stats_cmd = 'gdalinfo -approx_stats --config GDAL_PAM_ENABLED TRUE ' + path\n",
    "    if run:\n",
    "        result = subprocess.check_output(stats_cmd)\n",
    "    \n",
    "    if not os.path.exists(path[:-4]+\".ovr\"):\n",
    "        pyr_cmd = 'gdaladdo -ro --config COMPRESS_OVERVIEW ZSTD --config ZSTD_LEVEL 1 --config PREDICTOR 2 --config INTERLEAVE_OVERVIEW BAND --config GDAL_CACHEMAX 4096 ' + path\n",
    "        if run:\n",
    "            result = subprocess.check_output(pyr_cmd)\n",
    "    \n",
    "    return stats_cmd, pyr_cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7958596b-8630-475a-950a-44a6b9b91aa1",
   "metadata": {},
   "source": [
    "# Temporary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f3c594-d1fa-4bb3-9b5c-5d2c04330468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiona\n",
    "fiona.supported_drivers['KML'] = 'rw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4caded-7e81-4981-b94c-a0dea52e9363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick creation of shapefile of examples\n",
    "locationDict = {\n",
    "  'Thornybush': {'lon': 31.180085534751232, 'lat': -24.449807514937508, 'zoom': 12, 'metric':'cover', 'year1':2016, 'year2':2020},\n",
    "  'Tree plantations': {'lon': 30.748241, 'lat': -25.249696, 'zoom': 13, 'metric':'rh98', 'year1':2015, 'year2':2022},\n",
    "  'Communal lands': {'lon': 30.9675782982199, 'lat': -24.49883259221857, 'zoom': 14, 'metric':'pai', 'year1':2017, 'year2':2021},\n",
    "  'Citrus orchards': {'lon': 31.657928, 'lat': -25.441006, 'zoom': 14, 'metric':'pai', 'year1':2007, 'year2':2022},\n",
    "  'Urbanization': {'lon': 31.691362, 'lat': -25.452824, 'zoom': 13, 'metric':'cover', 'year1':2007, 'year2':2015},\n",
    "  'Woody encroachment': {'lon': 31.719479, 'lat': -25.051213, 'zoom': 14, 'metric':'fhd', 'year1':2009, 'year2':2019},\n",
    "  'Prescribed fire': {'lon': 31.830821, 'lat': -24.46095, 'zoom': 13, 'metric':'cover', 'year1':2018, 'year2':2022},\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(locationDict).T\n",
    "df['name'] = df.index\n",
    "gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['lon'], df['lat']), crs=\"EPSG:4326\")\n",
    "gdf = gdf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f493370-8a08-4377-97dc-e5682e68697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_file(r\"H:\\ECOFOR\\gedi\\maps\\v03\\gedi_change_examps_v3.kml\", driver=\"KML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a9da0-fc12-495a-bb7c-d2c36da9ba73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18cc231-2820-4315-848c-3838c4da0eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "\n",
    "def convert_partial_year(number):\n",
    "\n",
    "    year = int(number)\n",
    "    d = timedelta(days=(number - year)*365)\n",
    "    day_one = datetime(year,1,1)\n",
    "    date = d + day_one\n",
    "    return date\n",
    "\n",
    "{d:convert_partial_year(d) for d in [2010.0, 2015.8, 2020.5, 2022.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9853d0-0acc-493a-aa1c-92cb1cba9655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2ebce6-3fc6-4f92-ad50-dc9a01448072",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"J:\\data\\lidar\\_readMe\\DataSources.xlsx\"\n",
    "df = pd.read_excel(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c225604c-fb28-4256-83c0-1d1fad387424",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Current drive'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2e5bd4-75e0-44f2-9930-e4d2f47e1351",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['Current drive']!='None').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d12f04-f9b0-4cb0-b98c-4adf543c906f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78c1a20-2573-4736-a6f9-f048e7f089db",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"J:\\projects\\ECOFOR\\gedi\\gedi_data\\04_gedi_filtered_data_shp\\GEDI_2AB_2019to2023_leafon_sampy500m.parquet\"\n",
    "df = gpd.read_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34628e91-b623-42e8-9163-47ba8fd85dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94044467-7fe0-414a-be44-1cff989a314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['shot_number', 'beam', 'algorithmrun_flag', 'degrade_flag', 'l2b_quality_flag', 'stale_return_flag', 'surface_flag',\n",
    "        'solar_elevation', 'delta_time', 'sensitivity', 'lat_lowestmode', 'lon_lowestmode', 'elev_highestreturn', 'elev_lowestmode',\n",
    "        'local_beam_elevation', 'fhd_normal', 'pgap_theta', 'pai', 'rhov', 'rhog', 'omega', 'cover', 'cover_z_0_5m', 'cover_z_5_10m',\n",
    "        'cover_z_10_15m', 'cover_z_15_20m', 'cover_z_20_25m', 'pai_z0_5m', 'pai_z5_10m', 'pai_z10_15m', 'pai_z15_20m', 'pai_z20_25m',\n",
    "        'pavd_z0_5m', 'pavd_z5_10m', 'pavd_z10_15m', 'pavd_z15_20m', 'pavd_z20_25m', 'rh90', 'rh95', 'rh96', 'rh97', 'rh98', 'rh99',\n",
    "        'rh100', 'geometry', 'millis', 'year', 'rain_year', 'x', 'y', 'x_grid', 'y_grid']\n",
    "sub = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1921969a-58a6-4921-93f8-4d2b688567e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_file(r\"D:\\GEDI_2AB_2019to2023_leafon_sampy500m_subcols.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d08ef95-9fc3-46c0-bd3a-a8ee1a10a4db",
   "metadata": {},
   "source": [
    "## Compare GEDI to 2018 CHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da21a5-a3b5-479c-b48e-5cd948d26563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CHM 1m GEDI extract and compare\n",
    "path = r\"C:\\scratch\\ecofor\\gedi\\GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31_chm1m.parquet\"\n",
    "df = gpd.read_parquet(path)\n",
    "df = df.set_index('shot_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a23694c-6aa7-4b90-a141-08e61395eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={i:'chm1m_'+i for i in ['mean', 'median', 'max']}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d7a296-edd7-4502-88bf-eb237e5b8b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = [('chm1m_mean','rh50'), ('chm1m_median','rh50'), ('chm1m_max','rh98')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69366752-6fe9-43f1-acc4-a4f42349ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(8, 1.75))\n",
    "\n",
    "for i, (cols, ax) in enumerate(zip(col_list, axes)):\n",
    "    subdf = df[list(cols)].dropna(axis=0)\n",
    "    obscol, predcol = cols[0], cols[1]\n",
    "    \n",
    "    # remove gedi outliers for now\n",
    "    subdf = subdf[subdf[predcol]<subdf[obscol].max()]\n",
    "    \n",
    "    # zero out negative CHM values for now\n",
    "    subdf.loc[subdf[obscol]<0, obscol] = 0\n",
    "    \n",
    "    x, y = subdf[predcol], subdf[obscol]\n",
    "    hb = ax.hexbin(x, y, gridsize=20, mincnt=1, cmap='magma_r', linewidths=0, edgecolor='none', vmax=2000)\n",
    "    ax.plot((y.min(), y.max()), (y.min(),y.max()), '--k')\n",
    "    \n",
    "    r2 = r2_score(y, x)\n",
    "    bias = (x-y).mean()\n",
    "    rmse = mean_squared_error(y, x)**0.5\n",
    "    \n",
    "    # add text\n",
    "    ax.text(0.99, 0.22, \"R$^2$= \" + str(r2.round(2)), transform=ax.transAxes, ha='right')\n",
    "    ax.text(0.99, 0.13, \"Bias= \"+str(bias.round(2)), transform=ax.transAxes, ha='right')\n",
    "    ax.text(0.99, 0.02,  \"RMSE= \" + str(rmse.round(2)), transform=ax.transAxes, ha='right')\n",
    "    \n",
    "    ax.set(xlabel=predcol, ylabel=obscol)\n",
    "    # ax.set(title=ycol)\n",
    "    # if i==0:\n",
    "    #     ax.set(ylabel='Observed')\n",
    "    # fig.supxlabel('Predicted', x=0.47, y=-0.16, ha='center', fontsize=10)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.4, hspace=0.5)\n",
    "cb = fig.colorbar(hb, ax=axes, shrink=True, aspect=16, pad=0.02) #cax=cax, aspect=)#\n",
    "cb.set_label('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7a1975-4077-4db2-b57c-337ff52a37cb",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Thornybush data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2603a0b3-6413-4a92-a8b2-646b33175e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"J:\\projects\\ECOFOR\\ancillary_data\\Thornybush - NASA Tree Data with dates.xlsx\"\n",
    "df = pd.read_excel(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767cdce3-8d84-4744-adbc-723a2b66e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Longitude'], df['Latitude']), crs=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e39b37-7ce4-4bf6-8939-a9565c1d91d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_file(r\"J:\\projects\\ECOFOR\\ancillary_data\\Thornybush - NASA Tree Data with dates.gpkg\", driver=\"GPKG\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba6d9e3-ff02-4ff7-abe2-a7696c331079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5603720-d792-4ad9-9cca-d18221f622ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df82c0c1-098b-46bf-9c9e-b6d4a97ae4b7",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Field data\n",
    "Prepare samples for field visits and wrangle collected data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c4100a-be32-495d-82c2-b9b81ed991bb",
   "metadata": {},
   "source": [
    "## GEDI field sample\n",
    "Filter for points next to the roads that will be visited in each field season. Use stratified random sample to select points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31cf6d3-1b6c-42f9-8210-048e3a788027",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# need to read a gpd first to initiallize fiona before turning on driver\n",
    "gpd.io.file.fiona.drvsupport.supported_drivers['LIBKML'] = 'rw'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0b7148-ec85-43ee-994b-7614f8e6991e",
   "metadata": {},
   "source": [
    "### January 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bfb2f2-465f-4020-b3e5-c23dc3f821c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path = r\"C:\\scratch\\ecofor\\Kruger route.kml\"\n",
    "# route_names = ['Main route', 'Skukuza drives']\n",
    "\n",
    "path = r\"G:\\My Drive\\Work\\ecofor\\field_collection\\Bushriver roads north.kml\"\n",
    "route_names = [\"Bushriver roads north\"]\n",
    "\n",
    "# path = r\"G:\\My Drive\\Work\\ecofor\\field_collection\\bushriver roads south.kml\"\n",
    "# route_names = [\"bushriver roads south\"]\n",
    "\n",
    "dfs = []\n",
    "for name in route_names:\n",
    "    df_route = gpd.read_file(path, layer=name)\n",
    "    df_route = df_route[df_route['Name']==name]\n",
    "    dfs.append(df_route)\n",
    "rts = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf1acb-4153-432c-ab14-26b60177fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgs84bufrect = rts.unary_union.envelope.buffer(0.01)\n",
    "rts = rts.to_crs(epsg=32636)\n",
    "rt = rts.unary_union\n",
    "rtbuf = rt.buffer(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5c89cd-d855-4083-ae84-f952d4d8c0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gedi data and rough filter\n",
    "cols = ['shot_number', 'beam', 'sensitivity', 'pai', 'cover', 'rh98', 'geometry']\n",
    "df = gpd.read_parquet(r\"C:\\scratch\\ecofor\\GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31.parquet\", columns=cols)\n",
    "\n",
    "sub = df[df.intersects(wgs84bufrect)]\n",
    "sub = sub.to_crs(epsg=32636)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c418a19d-f128-4f2d-a579-658b20e67309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Route intersection and save\n",
    "sdf = sub[sub.intersects(rtbuf)]\n",
    "sdf = sdf.to_crs(epsg=4326)\n",
    "sdf['strat'] = pd.cut(sdf['rh98'],[0,3,5,10,15,60], include_lowest=True)\n",
    "sdf['strat']=sdf['strat'].astype(str)\n",
    "\n",
    "sdf.to_file(r\"C:\\scratch\\ecofor\\GEDI_bushrivernorth.shp\")#, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbea2306-3e5f-4a46-924f-4cee4fa8f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['shot_number', 'comment', 'geometry']\n",
    "sdf['covstr'] = (sdf['cover']*100).round().astype(int).astype(str)\n",
    "sdf['rh98str'] = sdf['rh98'].round(2).astype(str)\n",
    "sdf['comment'] = (\"strat=\"+sdf['strat']+'   '+\n",
    "                   \"cov=\"+sdf['covstr']+'   '+\n",
    "                   \"rh98=\"+sdf['rh98str']+'   '+\n",
    "                   \"shot=\"+sdf['shot_number'])\n",
    "sdf[cols].to_file(r\"C:\\scratch\\ecofor\\GEDI_bushriversouth_togpx.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a37d02-aab2-4418-9dfd-0c7a95c58f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified random samp by height\n",
    "df = gpd.read_file(r\"C:\\scratch\\ecofor\\GEDI_rt220107.gpkg\")\n",
    "df['strat'] = pd.cut(df['rh98'],[0,3,5,10,15,60], include_lowest=True)\n",
    "samp = df.groupby('strat', group_keys=False).apply(lambda x: x.sample(n=min(len(x), 10), random_state=0))\n",
    "samp['strat']=samp['strat'].astype(str)\n",
    "samp.to_file(r\"C:\\scratch\\ecofor\\GEDI_rt220107_samp.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb7aec4-3def-4e82-a531-de0ce6e57333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to shapefile with subset of columns for gps\n",
    "samp = gpd.read_file(r\"C:\\scratch\\ecofor\\GEDI_rt220107_samp.gpkg\")\n",
    "samp['covstr'] = (samp['cover']*100).round().astype(int).astype(str)\n",
    "samp['rh98str'] = samp['rh98'].round(2).astype(str)\n",
    "samp['comment'] = (\"strat=\"+samp['strat']+'   '+\n",
    "                   \"cov=\"+samp['covstr']+'   '+\n",
    "                   \"rh98=\"+samp['rh98str']+'   '+\n",
    "                   \"shot=\"+samp['shot_number'])\n",
    "samp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a9b70-acdb-4765-9132-e0991358ce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['shot_number', 'comment', 'geometry']\n",
    "samp[cols].to_file(r\"C:\\scratch\\ecofor\\GEDI_rt220107_samp.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e93104-1ee2-46bc-826c-2a4b5db73d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23614889-8444-42ce-9aa6-2656b09e96b7",
   "metadata": {},
   "source": [
    "### May 2023 v1  \n",
    "First creation of points to be used in May 2023 campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767c56f8-cab6-4a54-8824-8be43e89b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to read a gpd first to initiallize fiona before turning on driver\n",
    "# gpd.io.file.fiona.drvsupport.supported_drivers['LIBKML'] = 'rw'\n",
    "\n",
    "# path = r\"E:\\ecofor\\ancillary_data\\SAPAD_OR_2021_Q3\\SAPAD_OR_2021_Q3.shp\"\n",
    "# path = r\"C:\\scratch\\ecofor\\GEDI_skukuza230118.gpkg\"\n",
    "# path = r\"C:\\scratch\\ecofor\\gedi_pundaroute.gpkg\"\n",
    "path = r\"J:\\projects\\ECOFOR\\maps\\ecofor\\ecofor.gdb\"\n",
    "# polys = gpd.read_file(path, layer='pundamaria_poly')\n",
    "polys = gpd.read_file(path, layer='hotosm_roads_kruger_buf500m_nodissolve')\n",
    "# polys = polys[polys[\"WDPAID\"] == 555570306] # Olifants west\n",
    "# polys = polys[polys[\"CUR_NME\"] == \"Thornybush Nature Reserve\"]\n",
    "polys = polys.to_crs(epsg=32636)\n",
    "polys.sindex;\n",
    "# poly = polys.unary_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc6701-4c09-4636-8404-73e4af13f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gedi data and rough filter\n",
    "cols = ['shot_number', 'beam', 'sensitivity', 'pai', 'cover', 'rh98', 'geometry']\n",
    "df = gpd.read_parquet(r\"C:\\scratch\\ecofor\\GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31.parquet\", columns=cols)\n",
    "\n",
    "df = df.to_crs(epsg=32636)\n",
    "df.sindex;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec551f8-35f2-4808-b9b1-1275588fc8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join with all individual polys faster than intersects or sjoin with unary_union poly\n",
    "sdf = df.sjoin(polys, how='inner')\n",
    "sdf = sdf.drop_duplicates('shot_number')\n",
    "\n",
    "## simple but slow intersection\n",
    "# sdf = df[df.intersects(poly)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f6b8a-9c90-4852-9019-0ec5d3abd01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf = sdf.to_crs(epsg=4326)\n",
    "sdf['strat'] = pd.cut(sdf['rh98'],[0,3,5,10,15,60], include_lowest=True)\n",
    "sdf['strat'] = sdf['strat'].astype(str)\n",
    "\n",
    "sdf.to_file(r\"C:\\scratch\\ecofor\\GEDI_hotosm_roadbuf500m_points.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49032da-08f5-4a87-91c8-7df80571ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output as GPX\n",
    "outpath = r\"J:\\projects\\ECOFOR\\field\\gedi_may23\\samples\\GEDI_osmroads_buf500m.gpx\"\n",
    "\n",
    "# For GPX combine data into comment field\n",
    "cols = ['shot_number', 'comment', 'geometry']\n",
    "sdf['covstr'] = (sdf['cover']*100).round().astype(int).astype(str)\n",
    "sdf['rh98str'] = sdf['rh98'].round(2).astype(str)\n",
    "sdf['comment'] = (\"strat=\"+sdf['strat']+'   '+\n",
    "                   \"cov=\"+sdf['covstr']+'   '+\n",
    "                   \"rh98=\"+sdf['rh98str']+'   '+\n",
    "                   \"shot=\"+sdf['shot_number'])\n",
    "\n",
    "\n",
    "\n",
    "sdf['name'] = sdf['shot_number'].astype(str)\n",
    "sdf['ele'] = 0.\n",
    "sdf['magvar'] = 0.\n",
    "sdf['time'] = pd.to_datetime('2019-08-02T14:17:50Z')\n",
    "sdf['geoidheight'] = 0.\n",
    "sdf['cmt'] = sdf['comment']\n",
    "sub_cols = ['geometry', 'ele', 'time', 'magvar', 'geoidheight', 'name', 'cmt']\n",
    "sdf[sub_cols].to_file(outpath, driver='GPX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ed33ec-cd41-4a78-b5b2-1734f62adc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save as CSV for making trimble wpt file\n",
    "# outpath = r\"C:\\scratch\\ecofor\\GEDI_pundamaria.csv\"\n",
    "# sdf['Latitude'], sdf['Longitude'] = sdf.geometry.y, sdf.geometry.x\n",
    "# sdf[['shot_number', 'Latitude', 'Longitude', ]].to_csv(outpath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88410cd2-33f2-4304-8532-6d9184a2e031",
   "metadata": {},
   "source": [
    "### May 2023\n",
    "Second version? Clip out a few areas David is visiting in May 2023 with UBC students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f3fd1d-a3df-4e76-b95a-a871019a04f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to roads in the greater Kruger area\n",
    "roads_path = r\"J:\\projects\\ECOFOR\\ancillary_data\\roads\\hotosm_zaf_roads_gpkg\\hotosm_zaf_roads.gpkg\"\n",
    "roads = gpd.read_file(roads_path)\n",
    "roads.sindex;\n",
    "\n",
    "aoi = gpd.read_file(r\"J:\\projects\\ECOFOR\\boundaries\\GKSDP_Area_Prj\\GKSDP_Area_Prj.shp\")\n",
    "aoi = aoi.to_crs(roads.crs)\n",
    "aoi.sindex;\n",
    "\n",
    "roads = roads[roads.intersects(aoi.loc[0, 'geometry'])]\n",
    "\n",
    "roads = roads.to_crs(epsg=32636)\n",
    "\n",
    "roads_buf = gpd.GeoDataFrame(geometry=roads.buffer(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d2bb30-e5dc-429d-a388-ef1dec2f325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "roads_buf.to_file(r\"G:\\temp\\hotosm_zaf_roads_buf500m.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec7fbd-e08a-4a50-b4ea-412d27c32878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gedi data\n",
    "cols = ['shot_number', 'beam', 'sensitivity', 'pai', 'cover', 'rh98', 'geometry']\n",
    "df = gpd.read_parquet(r\"C:\\scratch\\ecofor\\GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31.parquet\", columns=cols)\n",
    "\n",
    "# Get points that intersect the buffered roads\n",
    "df = df.to_crs(epsg=32636)\n",
    "df.sindex;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4068d1cd-6542-4bef-a17c-a55b8805ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "roads_buf_x = gpd.GeoDataFrame(geometry=[roads_buf.unary_union], crs=roads_buf.crs).explode().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4f82a7-bd32-4538-9476-b51105a7ba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip gedi points to buffered greater kruger roads \n",
    "# (takes 14 hours) probably still faster to do spatial join and drop duplicates\n",
    "rdf = df.overlay(roads_buf_x, how='intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e41d9f-36b6-453f-8c68-60c41f0356cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf['strat'] = pd.cut(rdf['rh98'],[0,3,5,10,15,60], include_lowest=True)\n",
    "rdf['strat'] = rdf['strat'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce627dc-28bb-4c62-901c-ed6fc960180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf.to_crs(epsg=4326).to_file(r\"J:\\projects\\ECOFOR\\field\\gedi_may23\\samples\\GEDI_gkdp_osmroads_buf500m.gpkg\", driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653afc72-460e-48ca-8f46-e9b1b4e40f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf.sindex;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a496929-dc8f-4f44-a001-8ffd533a4c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip out AOIs\n",
    "polys_path = r\"J:\\projects\\ECOFOR\\field\\gedi_may23\\samples\\ubc_may23_aois.gpkg\"\n",
    "polys = gpd.read_file(polys_path)\n",
    "polys = polys.to_crs(rdf.crs)\n",
    "polys.sindex;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551e1a69-98c8-412d-b2df-167493712d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = rdf.sjoin(polys, how='inner')\n",
    "pdf = pdf.drop_duplicates('shot_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0edd50c-2663-4182-af66-795639a1d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = pdf.drop(columns='index_right')\n",
    "pdf.to_crs(epsg=4326).to_file(r\"J:\\projects\\ECOFOR\\field\\gedi_may23\\samples\\GEDI_osmroads_ubcmay23aois_buf500m.gpkg\", driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21245a2f-61d9-4caf-a5f0-ea4e87f7dc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby and sample for each aoi\n",
    "grouped = pdf.groupby('name', as_index=False)\n",
    "sampdf = grouped.apply(lambda x: x.sample(min(len(x), 10))).reset_index(drop=True)\n",
    "sampdf = sampdf.to_crs(epsg=4326)\n",
    "sampdf.to_file(r\"J:\\projects\\ECOFOR\\field\\gedi_may23\\samples\\GEDI_osmroads_ubcmay23aois_buf500m_samp10.gpkg\", driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe90d878-409e-42de-98f0-c02ccaca971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output as GPX\n",
    "outpath = r\"J:\\projects\\ECOFOR\\field\\gedi_may23\\samples\\GEDI_osmroads_ubcmay23aois_buf500m_samp10.gpx\"\n",
    "\n",
    "# For GPX combine data into comment field\n",
    "cols = ['shot_number', 'comment', 'geometry']\n",
    "sampdf['covstr'] = (sampdf['cover']*100).round().astype(int).astype(str)\n",
    "sampdf['rh98str'] = sampdf['rh98'].round(2).astype(str)\n",
    "sampdf['comment'] = (\"strat=\"+sampdf['strat']+'   '+\n",
    "                   \"cov=\"+sampdf['covstr']+'   '+\n",
    "                   \"rh98=\"+sampdf['rh98str']+'   '+\n",
    "                   \"shot=\"+sampdf['shot_number'])\n",
    "\n",
    "\n",
    "\n",
    "sampdf['name'] = sampdf['shot_number'].astype(str)\n",
    "sampdf['ele'] = 0.\n",
    "sampdf['magvar'] = 0.\n",
    "sampdf['time'] = pd.to_datetime('2019-08-02T14:17:50Z')\n",
    "sampdf['geoidheight'] = 0.\n",
    "sampdf['cmt'] = sampdf['comment']\n",
    "sub_cols = ['geometry', 'ele', 'time', 'magvar', 'geoidheight', 'name', 'cmt']\n",
    "sampdf[sub_cols].to_file(outpath, driver='GPX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db12932-368c-4c22-b555-3c8c490da3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# load gedi data\n",
    "cols = ['shot_number', 'beam', 'sensitivity', 'pai', 'cover', 'rh98', 'geometry']\n",
    "gdf = gpd.read_parquet(r\"C:\\scratch\\ecofor\\GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31.parquet\")#, columns=cols)\n",
    "gdf = gdf.rename(columns={'shot_number':'gedi'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61eb169-afb6-4544-bec9-f2090b3a7ce7",
   "metadata": {},
   "source": [
    "### May 2024\n",
    "Get sample of GEDI shots for UBC students to visit in May 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca556805-a7c4-402f-8594-dc087a1172eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gedi data\n",
    "cols = ['shot_number', 'beam', 'sensitivity', 'pai', 'cover', 'rh98', 'geometry']\n",
    "df = gpd.read_parquet(r\"H:\\ECOFOR\\gedi\\extracted\\GEDI_2AB_2019to2023_all.parquet\", columns=cols)\n",
    "df = df.reset_index()\n",
    "df['shot_number'] = df['shot_number'].astype(str)\n",
    "df = df.to_crs(epsg=32636)\n",
    "df.sindex;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85543ad-6c5f-43ad-b58c-d55fc279b306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip out AOIs\n",
    "polys_path = r\"J:\\projects\\ECOFOR\\field\\gedi_may24\\samples\\ubc_may24_north_aois.gpkg\" #r\"J:\\projects\\ECOFOR\\field\\gedi_may24\\samples\\ubc_may24_aois.gpkg\"\n",
    "outpath = r\"J:\\projects\\ECOFOR\\field\\gedi_may24\\samples\\GEDI_ubcmay24northaois.gdb\" #r\"J:\\projects\\ECOFOR\\field\\gedi_may24\\samples\\GEDI_ubcmay24aois.gdb\"\n",
    "\n",
    "polys = gpd.read_file(polys_path)\n",
    "polys = polys.to_crs(df.crs)\n",
    "polys.sindex;\n",
    "\n",
    "pdf = df.sjoin(polys, how='inner')\n",
    "pdf = pdf.loc[pdf.index.drop_duplicates()]\n",
    "\n",
    "pdf = pdf.drop(columns='index_right')\n",
    "\n",
    "pdf['strat'] = pd.cut(pdf['rh98'],[0,3,5,10,15,60], include_lowest=True)\n",
    "pdf['strat'] = pdf['strat'].astype(str)\n",
    "\n",
    "# Save\n",
    "pdf.to_crs(epsg=4326).to_file(outpath, driver='OpenFileGDB') # saving to GDB because GPKG uses 64-bit objectid which is not compatible with \"enable sync\" in AGOL web layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8040cb59-1916-47cb-8e88-0f3904d801fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby and sample for each aoi\n",
    "grouped = pdf.groupby('name', as_index=False)\n",
    "sampdf = grouped.apply(lambda x: x.sample(min(len(x), 30))).reset_index(drop=True)\n",
    "sampdf = sampdf.to_crs(epsg=4326)\n",
    "sampdf.to_file(outpath[:-4]+\"_samp30.gdb\", driver='OpenFileGDB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d8e7c-5d21-4801-84cd-9ad1949b818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output as GPX\n",
    "\n",
    "# For GPX combine data into comment field\n",
    "cols = ['shot_number', 'comment', 'geometry']\n",
    "sampdf['covstr'] = (sampdf['cover']*100).round().astype(int).astype(str)\n",
    "sampdf['rh98str'] = sampdf['rh98'].round(2).astype(str)\n",
    "sampdf['comment'] = (\"strat=\"+sampdf['strat']+'   '+\n",
    "                   \"cov=\"+sampdf['covstr']+'   '+\n",
    "                   \"rh98=\"+sampdf['rh98str']+'   '+\n",
    "                   \"shot=\"+sampdf['shot_number'])\n",
    "\n",
    "sampdf['name'] = sampdf['shot_number']\n",
    "sampdf['ele'] = 0.\n",
    "sampdf['magvar'] = 0.\n",
    "sampdf['time'] = pd.to_datetime('2019-08-02T14:17:50Z')\n",
    "sampdf['geoidheight'] = 0.\n",
    "sampdf['cmt'] = sampdf['comment']\n",
    "sub_cols = ['geometry', 'ele', 'time', 'magvar', 'geoidheight', 'name', 'cmt']\n",
    "sampdf[sub_cols].to_file(outpath[:-5]+\"_samp30.gpx\", driver='GPX')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936b61f-2899-4294-8cc3-f8e203a4ed70",
   "metadata": {},
   "source": [
    "### May 2025\n",
    "Get sample of GEDI shots for UBC students to visit in May 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbbdafc-be03-4ee7-aee2-701817ec100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gedi data\n",
    "cols = ['shot_number', 'beam', 'delta_time', 'l2b_quality_flag', 'sensitivity', 'pai', 'cover', 'rh98', 'geometry']\n",
    "df = gpd.read_parquet(r\"F:\\ECOFOR\\gedi\\gedi_data\\04_gedi_filtered_data_shp\\GEDI_2AB_2019to2023.parquet\", columns=cols)\n",
    "\n",
    "df['shot_number'] = df['shot_number'].astype(str)\n",
    "\n",
    "# Filter to quality shots, leaf-on, and reasonable height\n",
    "df = df[df['l2b_quality_flag']==1] # quality filter\n",
    "df['delta_time'] = pd.to_datetime(df['delta_time'])\n",
    "df = df[df['rh98']<45] # Remove unreasonable points\n",
    "# df['leaf_on'] = (df['delta_time'].dt.day_of_year < 121) | (df['delta_time'].dt.day_of_year > 305) # keep only leaf-on (Nov - Apr) as defined in Li 2023 (removes too many points)\n",
    "df = df[(df['delta_time'].dt.day_of_year < 121) | (df['delta_time'].dt.day_of_year > 305)] # keep only leaf-on (Nov - Apr) as defined in Li 2023 (removes too many points)\n",
    "\n",
    "df = df.to_crs(epsg=32636)\n",
    "df.sindex;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eead5f-98fd-4d46-b26c-16d37f979762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip out AOIs\n",
    "import fiona\n",
    "fiona.supported_drivers['LIBKML'] = 'rw'\n",
    "\n",
    "polys_path = r\"J:\\projects\\ECOFOR\\field\\gedi_may25\\GEDI point locations 2025 (1).kml\"\n",
    "outpath = r\"J:\\projects\\ECOFOR\\field\\gedi_may25\\samples\\GEDI_ubcmay25_points.gdb\"\n",
    "\n",
    "polys = gpd.read_file(polys_path)\n",
    "\n",
    "polys = polys[polys['geometry'].geom_type=='Polygon']\n",
    "polys = polys.to_crs(df.crs)\n",
    "polys.sindex;\n",
    "\n",
    "pdf = df.sjoin(polys, how='inner')\n",
    "pdf = pdf.loc[pdf.index.drop_duplicates()]\n",
    "\n",
    "pdf = pdf.drop(columns='index_right')\n",
    "\n",
    "pdf['strat'] = pd.cut(pdf['rh98'],[0,3,5,10,15,60], include_lowest=True)\n",
    "pdf['strat'] = pdf['strat'].astype(str)\n",
    "\n",
    "# Save\n",
    "pdf.to_crs(epsg=4326).to_file(outpath, driver='OpenFileGDB') # saving to GDB because GPKG uses 64-bit objectid which is not compatible with \"enable sync\" in AGOL web layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873ba322-4208-41a0-8fe6-922d18471fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby and sample for each aoi\n",
    "grouped = pdf.groupby('Name', as_index=False)\n",
    "sampdf = grouped.apply(lambda x: x.sample(min(len(x), 30))).reset_index(drop=True)\n",
    "sampdf = sampdf.to_crs(epsg=4326)\n",
    "sampdf.to_file(outpath[:-4]+\"_samp30.gdb\", driver='OpenFileGDB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40deabfd-b208-4e0b-ad7b-3ce9d3c92115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output as GPX\n",
    "\n",
    "# For GPX combine data into comment field\n",
    "cols = ['shot_number', 'comment', 'geometry']\n",
    "sampdf['covstr'] = (sampdf['cover']*100).round().astype(int).astype(str)\n",
    "sampdf['rh98str'] = sampdf['rh98'].round(2).astype(str)\n",
    "sampdf['comment'] = (\"strat=\"+sampdf['strat']+'   '+\n",
    "                   \"cov=\"+sampdf['covstr']+'   '+\n",
    "                   \"rh98=\"+sampdf['rh98str']+'   '+\n",
    "                   \"shot=\"+sampdf['shot_number'])\n",
    "\n",
    "sampdf['name'] = sampdf['shot_number']\n",
    "sampdf['ele'] = 0.\n",
    "sampdf['magvar'] = 0.\n",
    "sampdf['time'] = pd.to_datetime('2019-08-02T14:17:50Z')\n",
    "sampdf['geoidheight'] = 0.\n",
    "sampdf['cmt'] = sampdf['comment']\n",
    "sub_cols = ['geometry', 'ele', 'time', 'magvar', 'geoidheight', 'name', 'cmt']\n",
    "sampdf[sub_cols].to_file(outpath[:-5]+\"_samp30.gpx\", driver='GPX')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52af89d-288f-4e44-ba24-2f542715f03a",
   "metadata": {},
   "source": [
    "## Organize collected data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d0a025-3727-41ed-90c7-f0ee584074c5",
   "metadata": {},
   "source": [
    "### 2023 data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dda89c-51a5-48dd-aece-b758ddafd25e",
   "metadata": {},
   "source": [
    "#### GEDI trees\n",
    "Merge tree and plot data from May and Jan visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb59d154-1b93-4ae1-8709-d29e51197b55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddirs = [r\"J:\\projects\\ECOFOR\\field\\gedi_jan23\", r\"J:\\projects\\ECOFOR\\field\\gedi_may23\"]\n",
    "dfs = []\n",
    "for ddir in ddirs:\n",
    "    pdf = pd.read_csv(os.path.join(ddir, \"gedi_plot.csv\"), dtype={'gedi':str})\n",
    "    tdf = pd.read_csv(os.path.join(ddir, \"gedi_trees.csv\"))\n",
    "    cols = tdf.columns.difference(pdf.columns.drop(['plot']))\n",
    "    \n",
    "    mdf = pd.merge(pdf, tdf[cols], on='plot', how='right')\n",
    "    mdf['visit'] = ddir[-5:]\n",
    "    dfs.append(mdf)\n",
    "    \n",
    "df = pd.concat(dfs)\n",
    "\n",
    "df = df.sort_values(['visit', 'plot'])\n",
    "df['plot_ix'] = pd.factorize(df['visit']+df['plot'].astype(str))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d6aaf5-44b7-41e9-b022-7fd18ee45349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get quadrant\n",
    "\n",
    "# def get_az_quadrant(az):\n",
    "#     if np.isnan(az):\n",
    "#         return None\n",
    "#     else:\n",
    "#         labels = {0:'ne', 1:'se', 2:'sw', 3:'nw'}\n",
    "#         return labels[int(az//90)]\n",
    "\n",
    "# df['quadrant'] = df['az'].apply(get_az_quadrant)\n",
    "\n",
    "df['quadrant'] = df['Direction'].str.lower()\n",
    "df['quadrant'] = df['quadrant'].fillna(df['num'].replace({1:'ne', 2:'se', 3:'sw', 4:'nw'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402bf45b-6ce4-4d53-893d-2ec790a4ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save just the trees data\n",
    "cols = ['plot_ix', 'quadrant', 'az', 'dist', 'hgt', 'species', 'live', 'pos', 'bole', 'notes',                              # plot and tree identifier, and tree values\n",
    "        'visit', 'plot', 'gedi', 'gps', 'camera', 'photo_num1', 'photo_num2', 'recorder', 'heights', 'photos', 'distances', 'plot_notes']   # other plot variables\n",
    "\n",
    "df[cols].to_csv(r\"J:\\projects\\ECOFOR\\field\\field_trees_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e935e1-e308-4bc1-a494-bb2c7afbd36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in GEDI data\n",
    "tdf = pd.merge(df, gdf, how='left', on='gedi')\n",
    "tdf = gpd.GeoDataFrame(tdf, geometry='geometry', crs=gdf.crs)\n",
    "tdf['date'] = tdf['date'].astype(str)\n",
    "tdf['delta_time'] = tdf['delta_time'].astype(str)\n",
    "tdf = tdf[cols + list(gdf.columns.drop('gedi'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf1508d-74d1-4e4b-aa24-7335fbf519f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project tree coordinates but save plot coordinates\n",
    "tdf['geo_utm36n'] = tdf['geometry'].to_crs(epsg = 32636)\n",
    "\n",
    "def get_tree_coords(r):\n",
    "    if np.isnan(r['az']) or np.isnan(r['dist']) or (r['geo_utm36n'] is None):\n",
    "        return None\n",
    "    rad = np.radians(r['az'])\n",
    "    dx = r['dist'] * np.sin(rad)\n",
    "    dy = r['dist']* np.cos(rad)\n",
    "    return shapely.affinity.translate(r['geo_utm36n'], dx, dy)\n",
    "\n",
    "tdf['tree_geo'] = tdf.apply(get_tree_coords, axis=1)\n",
    "tdf['geometry'] = tdf['tree_geo'].set_crs(tdf['geo_utm36n'].crs).to_crs(tdf.crs)\n",
    "tdf = tdf.drop(columns = ['geo_utm36n', 'tree_geo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c9f124-5610-4459-b4ca-bf11b9cb8e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.to_file(r\"J:\\projects\\ECOFOR\\field\\gedi_all_merged.gpkg\", layer=\"trees\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941afa1-8d7f-4cb8-bd1f-ab7ed61362a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.drop(columns='geometry').to_csv(r\"J:\\projects\\ECOFOR\\field\\gedi_trees_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab69e2df-cc5a-4dce-a6da-40ce82ae568c",
   "metadata": {},
   "source": [
    "#### GEDI cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930953f-5c72-4ebc-a205-2dd79811ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddirs = [r\"J:\\projects\\ECOFOR\\field\\gedi_jan23\", r\"J:\\projects\\ECOFOR\\field\\gedi_may23\"]\n",
    "dfs = []\n",
    "for ddir in ddirs:\n",
    "    pdf = pd.read_csv(os.path.join(ddir, \"gedi_plot.csv\"), dtype={'gedi':str})\n",
    "    cdf = pd.read_csv(os.path.join(ddir, \"gedi_cover.csv\"))\n",
    "    cols = cdf.columns.difference(pdf.columns.drop(['plot']))\n",
    "    \n",
    "    mdf = pd.merge(pdf, cdf[cols], on='plot', how='right')\n",
    "    mdf['visit'] = ddir[-5:]\n",
    "    \n",
    "    dfs.append(mdf)\n",
    "    \n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507cca6a-382c-47c1-b443-69a5a3c70b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(['visit', 'plot'])\n",
    "df['plot_ix'] = pd.factorize(df['visit']+df['plot'].astype(str))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce3a897-8c3a-4b4d-8ec0-23ad79002323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save just the cover data\n",
    "cols = ['plot_ix', 'type', 'species', 'cover', 'notes',                                                                       # plot identifier and plot cover values\n",
    "        'visit', 'plot', 'gedi', 'gps', 'camera', 'photo_num1', 'photo_num2', 'recorder', 'heights', 'photos', 'distances', 'plot_notes']   # other plot variables\n",
    "\n",
    "df[cols].to_csv(r\"J:\\projects\\ECOFOR\\field\\field_cover_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304f24e3-af94-48e3-9899-bb45bc95d08b",
   "metadata": {},
   "source": [
    "**Export overall cover with GEDI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4e121f-7c17-48cc-a4b1-a2c1fde55930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape overall cover only\n",
    "df = df[df['type']=='overall']\n",
    "wdf = df.pivot(index='plot_ix', columns='species', values='cover')\n",
    "wdf['total'] = wdf.sum(axis=1)\n",
    "plot_df = df.drop(columns=['type', 'species', 'cover', 'notes']).drop_duplicates('plot_ix')\n",
    "wdf = pd.merge(wdf, plot_df, how='left', left_index=True, right_on='plot_ix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3881e2-eda9-4288-b770-255cbe7ff3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in GEDI data\n",
    "cdf = pd.merge(wdf, gdf, how='left', on='gedi')\n",
    "cdf = gpd.GeoDataFrame(cdf, geometry='geometry', crs=gdf.crs)\n",
    "cdf['date'] = cdf['date'].astype(str)\n",
    "cdf['delta_time'] = cdf['delta_time'].astype(str)\n",
    "\n",
    "cols = ['plot_ix', 'tree', 'shrub', 'herb', 'soil','litter', 'rock',  'other', 'total',                                                     # plot identifier and plot cover values\n",
    "        'visit', 'plot', 'gedi', 'gps', 'camera', 'photo_num1', 'photo_num2', 'recorder', 'heights', 'photos', 'distances', 'plot_notes']   # other plot variables\n",
    "cdf = cdf[cols + list(gdf.columns.drop('gedi'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca8228e-cbef-41a1-9edc-b7daae154664",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.to_file(r\"J:\\projects\\ECOFOR\\field\\gedi_all_merged.gpkg\", layer=\"cover\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e5dcce-a134-430e-9fee-366939f74847",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.drop(columns=['geometry']).to_csv(r\"J:\\projects\\ECOFOR\\field\\gedi_cover_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a27e9-560b-4855-b7ab-dc64a060b6cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Merged and simplified  \n",
    "Merge tree and cover data and simplify it for use by UBC students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93281ef5-685b-432c-a24f-cd1d4ea55779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract quadrant with the tallest tree\n",
    "tallest_ix = tdf.groupby('plot_ix')['hgt'].idxmax().dropna()\n",
    "tcols = ['plot_ix', 'quadrant', 'az', 'dist', 'hgt', 'species', 'live', 'pos', 'bole', 'notes']\n",
    "talldf = tdf.loc[tallest_ix, tcols]\n",
    "\n",
    "# Cover columns\n",
    "cols = ['plot_ix', 'tree', 'shrub', 'herb', 'soil','litter', 'rock',  'other', 'total',                                                     # plot identifier and plot cover values\n",
    "        'cover', 'rh98', 'pai', 'elev_lowestmode', 'lat_lowestmode', 'lon_lowestmode', 'l2b_quality_flag', 'sensitivity',                   # GEDI variables\n",
    "        'visit', 'plot', 'gedi', 'gps', 'camera', 'photo_num1', 'photo_num2', 'recorder', 'heights', 'photos', 'distances', 'plot_notes']   # other plot variables\n",
    "\n",
    "simpdf = pd.merge(talldf, cdf[cols], on='plot_ix', how='right')\n",
    "\n",
    "simpdf.to_csv(r\"J:\\projects\\ECOFOR\\field\\gedi_trees_cover_simp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86dea59-c850-4fa4-800a-328b5aa69d70",
   "metadata": {},
   "source": [
    "### 2024 data prep\n",
    "Get metrics for GEDI field plots for David and Logan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663d13ca-4dcf-469c-9a60-4d95b3fed3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick merge\n",
    "path = r\"C:\\scratch\\ECOFOR\\gedi\\GEDI_2AB_2019to2023.parquet\"\n",
    "df = gpd.read_parquet(path)\n",
    "df['GEDI #'] = df['shot_number'].astype(np.uint64)\n",
    "\n",
    "field_path = r\"J:\\projects\\ECOFOR\\field\\gedi_may24\\GEDI_CONS454samples_final.xlsx\"\n",
    "fdf = pd.read_excel(field_path)\n",
    "\n",
    "cols = ['GEDI #', 'delta_time', 'rh98', 'lat_lowestmode', 'lon_lowestmode', 'elev_lowestmode', 'fhd_normal', 'pai', 'cover', 'rh98']\n",
    "mdf = pd.merge(fdf, df[cols], on = 'GEDI #')\n",
    "mdf.to_excel(field_path[:-5]+'_wGEDI.xlsx', sheet_name='PrimaryVeg', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4218c2-013c-4405-85de-8813c289c26a",
   "metadata": {},
   "source": [
    "### Combine 2023 and 2024 field data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521dacab-ff45-4a21-a4e8-55c46f681524",
   "metadata": {},
   "source": [
    "#### GEDI trees\n",
    "Merge tree and plot data from 2023 and 2024 visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cebd882-9c97-4fc8-8312-900127f9588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GEDI data\n",
    "cols = ['shot_number', 'delta_time', 'cover', 'rh98', 'pai', 'elev_lowestmode', 'lat_lowestmode', 'lon_lowestmode', 'sensitivity', 'geometry']\n",
    "gdf = gpd.read_parquet(r\"J:\\projects\\ECOFOR\\gedi\\gedi_data\\04_gedi_filtered_data_shp\\GEDI_2AB_2019to2023.parquet\", columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beefa769-342b-458d-9158-11bd46df842d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddirs = [r\"J:\\projects\\ECOFOR\\field\\gedi_jan23\", r\"J:\\projects\\ECOFOR\\field\\gedi_may23\", r\"J:\\projects\\ECOFOR\\field\\gedi_may24\"]\n",
    "dfs = []\n",
    "for ddir in ddirs:\n",
    "    pdf = pd.read_csv(os.path.join(ddir, \"gedi_plot.csv\"), dtype={'gedi':str, 'shot_number':str})\n",
    "    tdf = pd.read_csv(os.path.join(ddir, \"gedi_trees.csv\"))\n",
    "    cols = tdf.columns.difference(pdf.columns.drop(['plot']))\n",
    "    visit = ddir[-5:]\n",
    "    # need to account for multiple groups measuring same plot in merge\n",
    "    if visit=='may24':\n",
    "        cols = cols.tolist()+['group']\n",
    "        mdf = pd.merge(pdf, tdf[cols], on=['group','plot'], how='right')\n",
    "    else:\n",
    "        mdf = pd.merge(pdf, tdf[cols], on='plot', how='right')\n",
    "        mdf['group'] = 0\n",
    "        mdf['shot_number'] = mdf['gedi'].astype(str)\n",
    "    mdf['visit'] = visit\n",
    "    dfs.append(mdf)\n",
    "    \n",
    "df = pd.concat(dfs)\n",
    "\n",
    "df = df.sort_values(['visit', 'plot', 'group'])\n",
    "df['plot_ix'] = pd.factorize(df['visit']+ df['plot'].astype(str) + df['group'].astype(str))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2952f836-ef31-499c-92cc-d70e1b90c69f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get quadrant\n",
    "\n",
    "# def get_az_quadrant(az):\n",
    "#     if np.isnan(az):\n",
    "#         return None\n",
    "#     else:\n",
    "#         labels = {0:'ne', 1:'se', 2:'sw', 3:'nw'}\n",
    "#         return labels[int(az//90)]\n",
    "\n",
    "# df['quadrant'] = df['az'].apply(get_az_quadrant)\n",
    "\n",
    "df['quadrant'] = df['Direction'].str.lower()\n",
    "df['quadrant'] = df['quadrant'].fillna(df['num'].replace({1:'ne', 2:'se', 3:'sw', 4:'nw'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad3cabd-0a54-4254-8b25-072b60a09d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save just the trees data\n",
    "cols = ['plot_ix', 'visit', 'plot', 'group', 'quadrant', 'az', 'dist', 'hgt', 'species', 'live', 'pos', 'bole', 'notes',                              # plot and tree identifier, and tree values\n",
    "         'shot_number', 'gps', 'camera', 'photo_num1', 'photo_num2', 'recorder', 'heights', 'photos', 'distances', 'plot_notes']   # other plot variables\n",
    "\n",
    "df[cols].to_csv(r\"J:\\projects\\ECOFOR\\field\\merged\\field_trees_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4185a7e3-5956-408e-85f2-f25779f7f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in GEDI data\n",
    "tdf = pd.merge(df, gdf, how='left', on='shot_number')\n",
    "tdf = gpd.GeoDataFrame(tdf, geometry='geometry', crs=gdf.crs)\n",
    "tdf['date'] = tdf['date'].astype(str)\n",
    "tdf['delta_time'] = tdf['delta_time'].astype(str)\n",
    "tdf = tdf[cols + list(gdf.columns.drop('shot_number'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43cb19-bee9-496d-832b-35286477828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project tree coordinates but save plot coordinates\n",
    "tdf['geo_utm36n'] = tdf['geometry'].to_crs(epsg = 32636)\n",
    "\n",
    "def get_tree_coords(r):\n",
    "    if np.isnan(r['az']) or np.isnan(r['dist']) or (r['geo_utm36n'] is None):\n",
    "        return None\n",
    "    rad = np.radians(r['az'])\n",
    "    dx = r['dist'] * np.sin(rad)\n",
    "    dy = r['dist']* np.cos(rad)\n",
    "    return shapely.affinity.translate(r['geo_utm36n'], dx, dy)\n",
    "\n",
    "tdf['tree_geo'] = tdf.apply(get_tree_coords, axis=1)\n",
    "tdf['geometry'] = tdf['tree_geo'].set_crs(tdf['geo_utm36n'].crs).to_crs(tdf.crs)\n",
    "tdf = tdf.drop(columns = ['geo_utm36n', 'tree_geo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac97287d-50f1-4c61-a93c-7f57f96ba0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.to_file(r\"J:\\projects\\ECOFOR\\field\\merged\\gedi_all_merged.gpkg\", layer=\"trees\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c786825-604b-4775-a05f-82ad8d95caee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.drop(columns='geometry').to_csv(r\"J:\\projects\\ECOFOR\\field\\merged\\gedi_trees_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c062b5-b624-42d0-b625-4283fa54edeb",
   "metadata": {},
   "source": [
    "#### GEDI cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0bfa66-934c-4f8e-b5fc-8fe95dad0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddirs = [r\"J:\\projects\\ECOFOR\\field\\gedi_jan23\", r\"J:\\projects\\ECOFOR\\field\\gedi_may23\", r\"J:\\projects\\ECOFOR\\field\\gedi_may24\"]\n",
    "dfs = []\n",
    "for ddir in ddirs:\n",
    "    pdf = pd.read_csv(os.path.join(ddir, \"gedi_plot.csv\"), dtype={'gedi':str, 'shot_number':str})\n",
    "    cdf = pd.read_csv(os.path.join(ddir, \"gedi_cover.csv\"))\n",
    "    cols = cdf.columns.difference(pdf.columns.drop(['plot']))\n",
    "    visit = ddir[-5:]\n",
    "    # need to account for multiple groups measuring same plot in merge\n",
    "    if visit=='may24':\n",
    "        cols = cols.tolist()+['group']\n",
    "        mdf = pd.merge(pdf, cdf[cols], on=['group','plot'], how='right')\n",
    "    else:\n",
    "        mdf = pd.merge(pdf, cdf[cols], on='plot', how='right')\n",
    "        mdf['group'] = 0\n",
    "        mdf['shot_number'] = mdf['gedi'].astype(str)\n",
    "    mdf['visit'] = visit\n",
    "    \n",
    "    \n",
    "    dfs.append(mdf)\n",
    "    \n",
    "df = pd.concat(dfs)\n",
    "\n",
    "df = df.sort_values(['visit', 'plot', 'group'])\n",
    "df['plot_ix'] = pd.factorize(df['visit'] + df['plot'].astype(str) + df['group'].astype(str))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c844b4-32ce-4bb3-8284-df960d3edd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save just the cover data\n",
    "cols = ['plot_ix', 'visit', 'plot', 'group', 'type', 'species', 'cover', 'notes',                                                                       # plot identifier and plot cover values\n",
    "        'shot_number', 'gps', 'camera', 'photo_num1', 'photo_num2', 'recorder', 'heights', 'photos', 'distances', 'plot_notes']   # other plot variables\n",
    "\n",
    "df[cols].to_csv(r\"J:\\projects\\ECOFOR\\field\\merged\\field_cover_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdbda74-8809-4ce3-8920-2a261c519463",
   "metadata": {},
   "source": [
    "#### Export overall cover with GEDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5833192-452e-4841-9468-58c3c7821865",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reshape overall cover only\n",
    "df = df[df['type']=='overall']\n",
    "\n",
    "wdf = df.pivot(index=['plot_ix', 'group'], columns='species', values='cover')\n",
    "wdf['total'] = wdf.sum(axis=1)\n",
    "plot_df = df.drop(columns=['type', 'species', 'cover', 'notes']).drop_duplicates(['plot_ix', 'group']).reset_index()\n",
    "wdf = pd.merge(wdf, plot_df, how='left', on=['plot_ix', 'group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36336fba-73cd-4e01-bfcb-237c8fb63a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in GEDI data\n",
    "cdf = pd.merge(wdf, gdf, how='left', on='shot_number')\n",
    "cdf = gpd.GeoDataFrame(cdf, geometry='geometry', crs=gdf.crs)\n",
    "cdf['date'] = cdf['date'].astype(str)\n",
    "cdf['delta_time'] = cdf['delta_time'].astype(str)\n",
    "\n",
    "cols = ['plot_ix', 'visit', 'group', 'plot', 'tree', 'shrub', 'herb', 'soil','litter', 'rock',  'other', 'total',                                                     # plot identifier and plot cover values\n",
    "        'shot_number', 'gps', 'camera', 'photo_num1', 'photo_num2', 'recorder', 'heights', 'photos', 'distances', 'plot_notes']   # other plot variables\n",
    "cdf = cdf[cols + list(gdf.columns.drop('shot_number'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ae6c74-9378-42b5-b5f8-30a0c6b6f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.to_file(r\"J:\\projects\\ECOFOR\\field\\merged\\gedi_all_merged.gpkg\", layer=\"cover\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d53c7c-8e52-4c7d-8429-c40fcf7de8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.drop(columns=['geometry']).to_csv(r\"J:\\projects\\ECOFOR\\field\\merged\\gedi_cover_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05731fa1-fcba-46a6-84c7-f9a7866f9533",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Merged and simplified  \n",
    "Merge tree and cover data and simplify it for use by UBC students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc617270-df42-4424-ae0d-c7108fbdef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = gpd.read_file(r\"J:\\projects\\ECOFOR\\field\\merged\\gedi_all_merged.gpkg\", layer=\"trees\")\n",
    "cdf = gpd.read_file(r\"J:\\projects\\ECOFOR\\field\\merged\\gedi_all_merged.gpkg\", layer=\"cover\")\n",
    "\n",
    "# Extract quadrant with the tallest tree\n",
    "tallest_ix = tdf.groupby('plot_ix')['hgt'].idxmax().dropna()\n",
    "tcols = ['plot_ix', 'quadrant', 'az', 'dist', 'hgt', 'species', 'live', 'pos', 'bole', 'notes']\n",
    "talldf = tdf.loc[tallest_ix, tcols]\n",
    "\n",
    "# Cover columns\n",
    "cols = ['plot_ix', 'visit', 'plot', 'group', 'tree', 'shrub', 'herb', 'soil','litter', 'rock',  'other', 'total',                                                     # plot identifier and plot cover values\n",
    "        'cover', 'rh98', 'pai', 'elev_lowestmode', 'lat_lowestmode', 'lon_lowestmode', 'sensitivity', 'delta_time',                  # GEDI variables\n",
    "        'shot_number', 'gps', 'camera', 'photo_num1', 'photo_num2', 'recorder', 'heights', 'photos', 'distances', 'plot_notes']   # other plot variables\n",
    "\n",
    "simpdf = pd.merge(talldf, cdf[cols], on='plot_ix', how='right')\n",
    "\n",
    "# fill hgt with 0 if null because these are plots with no trees\n",
    "simpdf.loc[simpdf['hgt'].isnull(), 'hgt'] = 0\n",
    "\n",
    "simpdf.to_csv(r\"J:\\projects\\ECOFOR\\field\\merged\\gedi_trees_cover_simp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c435f0f0-032e-4e94-a6b1-2af54c356e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcols = ['plot_ix', 'visit', 'plot', 'group', 'shot_number', 'delta_time', 'gps', 'camera', 'plot_notes']\n",
    "pdf = simpdf[pcols]\n",
    "pdf['shot_number'] = pdf['shot_number'].astype(str)\n",
    "# pdf.to_csv(r\"J:\\projects\\ECOFOR\\field\\merged\\plot_notes.csv\", index=False)\n",
    "\n",
    "# Need to IMPORT this into excel and tell it to NOT automatically detect data types in order to add the \n",
    "# exclude_plot columns without messing up the existing data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b717c0-c057-4e9a-ade8-f7fa201d62cb",
   "metadata": {},
   "source": [
    "## Compare field and RS data  \n",
    "Compare the field measurements to GEDI footprints and predicted maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71c882-c00a-452e-b7e6-756c79c2e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_regplot(x, y, ax, lims=None, reg=True, oneone=True, **kwargs):\n",
    "    from scipy.stats import linregress\n",
    "    \n",
    "    sns.regplot(x=x, y=y, ax=ax, **kwargs)\n",
    "\n",
    "    if reg:\n",
    "        # Regression\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "        rmse = mean_squared_error(y, x*slope+intercept)**0.5\n",
    "        eq = \"y = \" + str(np.round(slope,2)) + \"x + \" + str(np.round(intercept, 2))\n",
    "        ax.text(0.05, 0.95, \"Regression:\", transform=ax.transAxes)\n",
    "        ax.text(0.05, 0.89, eq, transform=ax.transAxes)\n",
    "        ax.text(0.05, 0.83, \"R$^2$= \" + str(np.round(r_value**2, 2)), transform=ax.transAxes)\n",
    "        ax.text(0.05, 0.77, \"RMSE= \"+str(np.round(rmse, 2)), transform=ax.transAxes)\n",
    "\n",
    "#         # check that line is the same as from sns.reglot\n",
    "#         samp = np.arange(x.min(), x.max(), 1)\n",
    "#         ax.plot(samp, intercept + slope * samp, 'r')\n",
    "\n",
    "    if oneone:\n",
    "        if lims is None:\n",
    "            lims = (0, np.nanmax(x.append(y))) #np.nanmin(x.append(y))\n",
    "        ax.plot(lims, lims, '--k')\n",
    "        ax.set(ylim=lims, xlim=lims)\n",
    "\n",
    "        # add text for R2 and RMSE\n",
    "        r2 = r2_score(y, x)\n",
    "        rmse = mean_squared_error(y, x)**0.5\n",
    "        bias = (x-y).mean()\n",
    "        ax.text(0.97, 0.20, \"1:1 stats:\", transform=ax.transAxes, ha='right')\n",
    "        ax.text(0.98, 0.13,\"R$^2$= \"+str(np.round(r2, 2)), transform=ax.transAxes, ha='right')\n",
    "        ax.text(0.98, 0.07, \"RMSE= \"+str(np.round(rmse, 2)), transform=ax.transAxes, ha='right')\n",
    "        ax.text(0.98, 0.01, \"Bias= \"+str(np.round(bias, 2)), transform=ax.transAxes, ha='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c821ccf8-e47d-430a-a290-b113a037f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot max tree height compared to GEDI's RH98\n",
    "path = r\"J:\\projects\\ECOFOR\\field\\merged\\gedi_trees_cover_simp.csv\"\n",
    "df = pd.read_csv(path, index_col='plot_ix')\n",
    "\n",
    "# Load plot notes to drop bad plots\n",
    "path = r\"J:\\projects\\ECOFOR\\field\\merged\\plot_notes.csv\"\n",
    "pdf = pd.read_csv(path, index_col='plot_ix')\n",
    "\n",
    "df[['exclude_plot', 'exclude_reason']] = pdf[['exclude_plot', 'exclude_reason']]\n",
    "df = df[~df['exclude_plot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb4b6e5-6365-487b-8163-0e17501a7187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy by field visit\n",
    "pcols=df['visit'].unique()\n",
    "# title_dict={'juoc':'Western Juniper', 'pimo': 'Single-leaf Pinyon Pine', 'juos':'Utah Juniper'}\n",
    "fig, axes = plt.subplots(1, 3, figsize=(8.7, 3))\n",
    "for i, (sp, ax) in enumerate(zip(pcols, axes.flat)):\n",
    "    mask = df['visit']==sp\n",
    "    x = df.loc[mask, 'rh98']\n",
    "    y = df.loc[mask, 'hgt']\n",
    "    present_regplot(x, y, ax, scatter_kws={'alpha':0.4})\n",
    "    ax.set(xlabel='RH98',\n",
    "           ylabel='Field max height',\n",
    "           title=sp,\n",
    "           xlim = (-1, 30),\n",
    "           ylim= (-1, 30))\n",
    "#     ax.text(0.97, 0.21, \"1:1 stats:\", transform=ax.transAxes, ha='right')\n",
    "    if i!=0:\n",
    "        ax.set_ylabel('')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470d34b-3994-4ea4-a0b2-83763df5fd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "present_regplot(df['rh98'], df['hgt'], lims=(0,30), ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc8e26f-8001-407e-8133-87bbb6714a2a",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Airborne lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb6dc5-d8d0-4923-8c83-f6de64e02b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089d5a11-68b4-4c36-b844-bfdfcdc02738",
   "metadata": {},
   "source": [
    "## 2014 cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007292f1-52e7-4247-b712-f653211a2eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract lidar cover band from layer stack\n",
    "path = r\"K:\\ECOFOR\\lidar\\2014\\orig\\S1_A_VH_VV_16_17_lidar.dat\"\n",
    "with rasterio.open(path) as src:\n",
    "    arr = src.read(src.indexes[-1])\n",
    "    crs = src.crs\n",
    "    transform = src.transform\n",
    "\n",
    "arr[(arr<0) | (arr>100)] = np.nan\n",
    "\n",
    "new_prof = {'driver':'GTiff', 'dtype': 'float32', 'nodata': np.nan, 'width': 681, 'height': 3293,\n",
    "            'count': 1, 'crs': crs, 'transform': transform,  'blockxsize': 256, 'blockysize': 256,\n",
    "            'tiled': True, 'interleave': 'pixel', 'compress': 'lzw'}\n",
    "\n",
    "with rasterio.open(r\"K:\\ECOFOR\\lidar\\2014\\lidar_cover_2014.tif\", 'w', **new_prof) as dst:\n",
    "    dst.write(arr, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9444b714-89dd-4626-ab0e-8bea0b3807f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bounds of 2012 airborne lidar data for clipping GEDI points in exactextractr script\n",
    "path = r\"K:\\ECOFOR\\lidar\\2014\\lidar_cover_2014.tif\"\n",
    "with rasterio.open(path) as src:\n",
    "    arr = src.read(1)\n",
    "\n",
    "msk = (arr >= 0) & (arr<=100)\n",
    "\n",
    "shapes = rasterio.features.shapes(msk.astype(np.uint8), mask=msk, connectivity=4, transform = src.transform)\n",
    "\n",
    "rows = []\n",
    "for shp in shapes:\n",
    "    row = {'value':shp[1], 'geometry':shapely.geometry.shape(shp[0])}\n",
    "    rows.append(row)\n",
    "\n",
    "df = gpd.GeoDataFrame(rows, crs=src.crs)\n",
    "\n",
    "ldf = gpd.GeoDataFrame(geometry=[df.unary_union], crs=src.crs)\n",
    "ldf.to_file(r\"K:\\ECOFOR\\lidar\\2014\\lidar_cover_2014_bounds.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708e2044-01c6-4e36-9d08-c85cf9b89483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data after run through exactextractR\n",
    "path = \"K:\\ECOFOR\\gedi\\extracted_spectral_data\\GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31_lidar14.gpkg\"\n",
    "df = gpd.read_file(path)\n",
    "df['delta_time'] = pd.to_datetime(df['delta_time'])\n",
    "\n",
    "df['month'] = df['delta_time'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498e3772-15e9-44e9-aea9-44e5c30c9f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try only wet season\n",
    "mask = df['month'].isin([11, 12, 1, 2, 3])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "sns.scatterplot(df.loc[mask, 'cover']*100, df.loc[mask, 'mean'], ax=ax)\n",
    "ax.set(ylabel='2014 airborne cover %', xlabel='GEDI cover %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f57b7-063d-454c-abdc-f124622d3c98",
   "metadata": {},
   "source": [
    "## 2012 lidar points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cf9b54-6e74-45c4-81fd-bef07f1abe52",
   "metadata": {},
   "source": [
    "### Prep ALS data\n",
    "Notes\n",
    "1. The vertical epsg may not be 100% correct but is the closest datum I could find with a code  \n",
    "2. The Orion M200 collects 4 returns per pulse, but the data has no return number.  \n",
    "3. There are a couple tiles missing in the southern river, but they're missing from both ground and veg points so it can be considered an excluded area.  \n",
    "\n",
    "*The processing pipeline below was longer and more complicated than I remember from previous lidar work. Need to implement this in lidr or pdal to avoid error introduced by non-commercial lastools.*  \n",
    "*Note that the data does contain noise and I didn't run lasnoise.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ec82e8-7438-4734-b1d1-c8b0bd2cfee2",
   "metadata": {},
   "source": [
    "**Merge points using existing ground classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442fb28f-b328-41e2-8930-f1e8b1224ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['USE_PYGEOS'] = '1'\n",
    "import importlib\n",
    "importlib.reload(gpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89163999-fbe3-4330-bf05-51ea62961e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge ground points and label as ground\n",
    "\n",
    "indir = r\"K:\\ECOFOR\\lidar\\2012\\orig\\Points\\Points\\Ground_Points\"\n",
    "outpath = r\"K:\\ECOFOR\\lidar\\2012\\lidar_2012_ground.laz\"\n",
    "lastools = r\"C:\\Program Files\\LAStools\\bin\"\n",
    "cmd = lastools + r\"\\lasmerge -v -i \" + indir + r\"\\*.txt -iparse xyzi -set_classification 2 -epsg 32736 -vertical_epsg 9279 -cpu64 -o \" + outpath\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff5421-7799-4a33-910c-aa76f11ef169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge vegetation points\n",
    "paths = glob(r\"K:\\ECOFOR\\lidar\\2012\\orig\\Points\\Points\\Veg*\\*.txt\")\n",
    "with open(r\"K:\\ECOFOR\\lidar\\2012\\veg_lof.txt\", 'w') as f:\n",
    "    f.write(\"\\n\".join(paths))\n",
    "\n",
    "lof_path = r\"K:\\ECOFOR\\lidar\\2012\\veg_lof.txt\"\n",
    "outpath = r\"K:\\ECOFOR\\lidar\\2012\\lidar_2012_veg.laz\"\n",
    "cmd = lastools + r\"\\lasmerge -v -lof \" + lof_path + \" -iparse xyzi -set_classification 1 -epsg 32736 -vertical_epsg 9279 -cpu64 -o \" + outpath\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace2f67b-f5ce-4e63-8c4e-635b83e5231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge ground and veg points\n",
    "veg_path = r\"K:\\ECOFOR\\lidar\\2012\\lidar_2012_veg.laz\"\n",
    "ground_path = r\"K:\\ECOFOR\\lidar\\2012\\lidar_2012_ground.laz\"\n",
    "outpath = r\"K:\\ECOFOR\\lidar\\2012\\lidar_2012_all.laz\"\n",
    "cmd = lastools + r\"\\lasmerge -v -i \" + veg_path + \" \" + ground_path + r\" -cpu64 -o \" + outpath\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650ef347-c000-408e-9a80-865c97b28383",
   "metadata": {},
   "source": [
    "**Normalize points to height above ground**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396479a8-2707-4630-9a69-4b31c82780f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tile for lasheight\n",
    "path = r\"K:\\ECOFOR\\lidar\\2012\\lidar_2012_all.laz\"\n",
    "odir = r\"K:\\ECOFOR\\lidar\\2012\\tiles\"\n",
    "cmd = lastools + \"\\lastile -i \" + path + \" -tile_size 5000 -olaz -odir \" + odir\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5f33c5-0a3c-40ee-98f0-925ce06d506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasheight\n",
    "iglob = r\"K:\\ECOFOR\\lidar\\2012\\tiles\\*.laz\"\n",
    "odir = r\"K:\\ECOFOR\\lidar\\2012\\norm_tiles\"\n",
    "cmd = lastools + \"\\lasheight -i \" + iglob + \" -buffered 25 -replace_z -cores 6 -olaz -odir \" + odir\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f0661-789b-4220-8c82-cdeecb54bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge normalized points\n",
    "iglob = r\"K:\\ECOFOR\\lidar\\2012\\tiles_norm\\*.laz\"\n",
    "outpath = r\"K:\\ECOFOR\\lidar\\2012\\lidar_2012_all_norm.laz\"\n",
    "cmd = lastools + \"\\lasmerge -i \" + iglob + \" -cpu64 -o \" + outpath\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8070ebe7-2ad3-44fa-8845-98c089abfbbc",
   "metadata": {},
   "source": [
    "**height in extra_bytes**  \n",
    "gediSimulator readme recommends reclassifying all points within 60 cm of the ground as ground points for improved cover calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2118c1-7620-493e-b5fe-411862ceaef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasheight\n",
    "iglob = r\"K:\\ECOFOR\\lidar\\2012\\tiles\\*.laz\"\n",
    "odir = r\"K:\\ECOFOR\\lidar\\2012\\tiles_height\"\n",
    "cmd = lastools + \"\\lasheight -i \" + iglob + \" -buffered 25 -store_as_extra_bytes -cores 6 -olaz -odir \" + odir\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259679fb-2680-498b-a210-7d62fa07e19f",
   "metadata": {},
   "source": [
    "### Compare discrete ALS metrics with GEDI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06d7921-d608-469b-accc-e94690952fa5",
   "metadata": {},
   "source": [
    "**Get canopy metrics for GEDI footprints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a345615-7e5c-4a68-b6e6-93cc3221fa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output boundary of points for clipping GEDI data\n",
    "path = r\"K:\\ECOFOR\\lidar\\2012\\lidar_2012_all.laz\"\n",
    "outpath = r\"K:\\ECOFOR\\lidar\\2012\\lidar_2012_all_boundary.shp\"\n",
    "cmd = lastools + \"\\lasboundary -v -i \"+ path + \" -concavity 20 -disjoint_hull -o \" + outpath\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c008d5f9-d969-4b80-a532-02abf0ba6587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip GEDI to the lidar data and buffer to create plots\n",
    "boundary_path = r\"K:\\ECOFOR\\lidar\\2012\\lidar_2012_all_boundary.shp\"\n",
    "gedi_path = r\"K:\\ECOFOR\\gedi\\gedi_data\\04_gedi_filtered_data_shp\\GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31.parquet\"\n",
    "temp_path = r\"K:\\ECOFOR\\gedi\\extracted_spectral_data\\GEDI_2012_lidar_temp.shp\"\n",
    "\n",
    "bdf = gpd.read_file(boundary_path)\n",
    "gdf = gpd.read_parquet(gedi_path)\n",
    "\n",
    "# remove small chunks\n",
    "bdf['area'] = bdf.area/(1000*1000)\n",
    "bdf = bdf[bdf['area']>1]\n",
    "\n",
    "# Get points within lidar extent\n",
    "gdf = gdf.to_crs(bdf.crs)\n",
    "gdf = gdf[gdf.intersects(bdf.unary_union)]\n",
    "\n",
    "# Buffer and get GEDI shots completely within lidar extent\n",
    "gdf['geometry'] = gdf.buffer(12.5)\n",
    "gdf = gdf[gdf.within(bdf.unary_union)]\n",
    "\n",
    "gdf['delta_time'] = gdf['delta_time'].to_string()\n",
    "gdf.to_file(temp_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef909e19-e7d8-4930-b783-c688cb0d53f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get canopy metrics (this isn't merging into one output but writing csvs in the input folder, but it's much faster than using a single input)\n",
    "path = r\"K:\\ECOFOR\\lidar\\2012\\tiles_norm\\*.laz\"\n",
    "temp_path = r\"K:\\ECOFOR\\gedi\\extracted_spectral_data\\GEDI_2012_lidar_temp.shp\"\n",
    "outpath = r\"K:\\ECOFOR\\lidar\\2012\\lidar12_GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31_lascanopy.csv\"\n",
    "cmd = lastools + \"\\lascanopy -i \" + path + \" -lop \" + temp_path + \" shot_number -cover_cutoff 0.5 -height_cutoff 0 -p 50 75 95 98 -avg -max -dns -d 0.5 1 2 3 5 100 -cores 6 -o \" + outpath\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf5eaf-07c6-44e5-a504-197a12a8a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the results into one\n",
    "paths = glob(r\"K:\\ECOFOR\\lidar\\2012\\tiles_norm\\*.csv\")\n",
    "outpath = r\"K:\\ECOFOR\\lidar\\2012\\lidar12_GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31_lascanopy.csv\"\n",
    "df = pd.concat([pd.read_csv(p) for p in paths])\n",
    "df.to_csv(outpath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c35d5a-a4eb-47f7-a748-c8d56fe431ef",
   "metadata": {},
   "source": [
    "**ALS discrete metrics comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6956c2-248e-4767-9839-70c33de02118",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpath = r\"K:\\ECOFOR\\lidar\\2012\\lidar12_GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31.csv\"\n",
    "gpath = r\"K:\\ECOFOR\\gedi\\gedi_data\\04_gedi_filtered_data_shp\\GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31.parquet\"\n",
    "\n",
    "ldf = pd.read_csv(lpath)\n",
    "ldf = ldf.rename(columns={'name':'shot_number'})\n",
    "ldf = ldf[['shot_number', 'max', 'avg', 'p50', 'p75', 'p95', 'p98', 'd00', 'd01', 'd02', 'd03', 'd04', 'dns']]\n",
    "ldf['d01_04'] = ldf['d01'] + ldf['d02'] + ldf['d02'] + ldf['d02'] # makes height cutoff 1 m\n",
    "ldf['d01_\n",
    "\n",
    "gdf = gpd.read_parquet(gpath)\n",
    "gdf['shot_number'] = gdf['shot_number'].astype(np.uint64)\n",
    "gdf['cover'] *= 100\n",
    "\n",
    "df = gpd.GeoDataFrame(pd.merge(ldf, gdf, 'left',  on='shot_number'), crs=gdf.crs)\n",
    "\n",
    "df['delta_time'] = pd.to_datetime(df['delta_time'])\n",
    "df['month'] = df['delta_time'].dt.month\n",
    "\n",
    "# mask = df['month'].isin([11, 12, 1, 2, 3]) # These are wet seasons months\n",
    "mask = df['month'].isin([5,6,7,8,9])       # These are dry season months. Lidar data collected May 30 2012 so this is more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64f2730-371f-488a-a530-df40372db1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top of canopy height\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "sns.scatterplot('p98', 'rh98', data=df[mask], ax=ax)\n",
    "ax.set(xlim=(0,40), ylim=(0,40))\n",
    "ax.plot((0, 40), (0, 40), 'k--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0ad0c5-bbac-4db0-98a8-c85ff338daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cover with ALS height cutoff 50 cm\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "sns.scatterplot('cover', 'dns', data=df[mask], ax=ax)\n",
    "ax.set(xlim=(0,100), ylim=(0,100))\n",
    "ax.plot((0, 100), (0, 100), 'k--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871d56b6-3db7-4c48-bbf8-07929b98c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cover with als height cutoff 2 m\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "sns.scatterplot('cover', 'd01_02', data=df[mask], ax=ax)\n",
    "ax.set(xlim=(0,100), ylim=(0,100))\n",
    "ax.plot((0, 100), (0, 100), 'k--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ac4926-93a7-4600-bfdc-0bf73b49c696",
   "metadata": {},
   "source": [
    "### Compare GEDI simulated waveforms with GEDI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff86dd-07d0-4aa6-90ab-56f9e8af66f7",
   "metadata": {},
   "source": [
    "**GEDI simulator waveforms**  \n",
    "Prep ALS for the waveform simulator and run it to compare metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe468b-aaca-456d-b0f6-cbef272d1417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge height and set <60cm high to class 2 for use in GEDI waveform simulator\n",
    "iglob = r\"K:\\ECOFOR\\lidar\\2012\\tiles_height\\*.laz\"\n",
    "outpath = r\"K:\\ECOFOR\\lidar\\2012\\lidar_2012_all_lt60grd.las\"\n",
    "cmd = lastools + \"\\lasmerge -i \" + iglob + \" -classify_attribute_below_as 0 0.6 2 -cpu64 -o \" + outpath\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb400c4-3643-4dd8-8025-ecc9e2fe714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of shots and files collected around the dry season when the 2012 ALS was collected (May 30)\n",
    "gpath = r\"K:\\ECOFOR\\gedi\\gedi_data\\04_gedi_filtered_data_shp\\GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31.parquet\"\n",
    "gdf = gpd.read_parquet(gpath)\n",
    "gdf['shot_number'] = gdf['shot_number'].astype(np.uint64)\n",
    "gdf['delta_time'] = pd.to_datetime(gdf['delta_time'])\n",
    "gdf['month'] = gdf['delta_time'].dt.month\n",
    "gdf['doy'] = gdf['delta_time'].dt.day_of_year\n",
    "gdf['year'] = gdf['delta_time'].dt.year\n",
    "gdf['ydoy'] = gdf['year'].astype(str) + gdf['doy'].astype(str).str.zfill(3)\n",
    "\n",
    "# mask = df['month'].isin([11, 12, 1, 2, 3]) # These are wet seasons months\n",
    "mask = gdf['month'].isin([5,6,7,8,9])       # These are dry season months. Lidar data collected May 30 2012 so this is more appropriate.\n",
    "gdf = gdf[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f26a4cf-7bdc-4c27-bed2-9d272946bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get shots intersecting 2012 lidar\n",
    "bdf = gpd.read_file(r\"K:\\ECOFOR\\lidar\\2012\\lidar_2012_all_boundary.shp\")\n",
    "gdf = gdf.to_crs(bdf.crs)\n",
    "gdf = gdf[gdf.intersects(bdf.unary_union)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3469555-9fdb-4490-bf71-f1d0cd5101d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of l1b granules to download\n",
    "date_strings = gdf.loc[mask, 'ydoy'].unique()\n",
    "paths = glob(r\"K:\\ECOFOR\\gedi\\gedi_data\\02_download_files\\01_GEDI_downloads_2B\\*.h5\")\n",
    "l2b_granules = [p for p in paths for d in date_strings if d in p]\n",
    "l2b_granules  # Manually filtered and downloaded corresponding L1B granules on EarthData Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba77dcf-8cd9-4ae1-b413-44036b1452fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output list of coordinates for running gediRAT and gediMetric without collocation\n",
    "coord_strs = (gdf.geometry.x.astype(str) + \", \" + gdf.geometry.y.astype(str) + \", \" + gdf['shot_number'].astype(str)).tolist()\n",
    "# coord_strs = coord_strs[:3]\n",
    "with open(r\"K:\\ECOFOR\\lidar\\2012\\lidar12_GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31_coordslist.txt\", \"w\") as f:\n",
    "    f.write('\\n'.join(coord_strs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef479444-e66f-4554-82ef-d9e9cdf53e0c",
   "metadata": {},
   "source": [
    "**Simulator Metrics without collocate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d551526-d7fc-467e-8195-95ba16c705be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate waveforms for list of coordinates\n",
    "lpath = \"/mnt/k/ECOFOR/lidar/2012/lidar_2012_all_lt60grd.las\"\n",
    "coords_path = \"/mnt/k/ECOFOR/lidar/2012/lidar12_GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31_coordslist.txt\"\n",
    "outpath = \"/mnt/k/ECOFOR/lidar/2012/lidar12_GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31_sim.h5\"\n",
    "gedi_cmd = \"gediRat -input \" + lpath + \" -listCoord \" + coords_path + \" -output \" + outpath + \" -hdf -ground\"\n",
    "sing_cmd = \"singularity exec --bind /mnt/k gediSingularity \"\n",
    "cmd = sing_cmd + gedi_cmd \n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5b9fe1-058f-44b8-923b-b49e1415b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for simulated waveforms\n",
    "simpath = \"/mnt/k/ECOFOR/lidar/2012/lidar12_GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31_sim.h5\"\n",
    "outroot = \"/mnt/k/ECOFOR/lidar/2012/lidar12_GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31_sim_canopy\"\n",
    "\n",
    "cmd = \"gediMetric -input \" + simpath + \" -readHDFgedi -ground -outRoot \" + outroot\n",
    "sing_cmd = \"singularity exec --bind /mnt/k gediSingularity \"\n",
    "print(sing_cmd + cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5f5681-6d20-42a1-b93f-c745abd6db60",
   "metadata": {},
   "source": [
    "**Collocate Waves Metrics**  \n",
    "Run collocate waves with one granule at a time so the affine transform is done for each granule not all granules together. This assumes that GEDI shots near in time have similar geolocation error. If that's not true then collocate waves would need to be done per footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c7fe3c-8c97-486f-90a5-242a0ca6b9d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate expected offset between ALS and GEDI vertical datums\n",
    "\n",
    "# Read simulated metrics from original shot location (no collocation)\n",
    "sim_path = r\"K:\\ECOFOR\\lidar\\2012\\lidar12_GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31_sim_canopy.metric.txt\"\n",
    "odf = pd.read_csv(sim_path, skiprows=1, header=None, delimiter=\" \")\n",
    "\n",
    "# Set column headers\n",
    "with open(sim_path) as f:\n",
    "    header = f.readline()\n",
    "header = header[1:] # drop #\n",
    "hlist = header.split(',')\n",
    "hlist = ['_'.join(c.split(' ')[2:]) for c in hlist] # drop number and space\n",
    "hlist = hlist[:-1] # drop end of line\n",
    "\n",
    "odf.columns = hlist\n",
    "odf = odf.rename(columns={'wave_ID':'shot_number'})\n",
    "\n",
    "# Load prepared GEDI canopy table\n",
    "gpath = r\"K:\\ECOFOR\\gedi\\gedi_data\\04_gedi_filtered_data_shp\\GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31.parquet\"\n",
    "gdf = gpd.read_parquet(gpath)\n",
    "gdf['shot_number'] = gdf['shot_number'].astype(np.uint64)\n",
    "\n",
    "mdf = pd.merge(gdf, odf, how='inner', on='shot_number', suffixes=('', '_simN'))\n",
    "\n",
    "dif = mdf['true_ground'] - mdf['elev_lowestmode'] \n",
    "print('offset=', np.median(dif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a91b5d-f716-442a-ac0c-01b57fa3098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run collocatewaves on all granules (do this in python session started in WSL)\n",
    "# I think this will find one tranformation for each gedi file rather than each footprint\n",
    "# Using -listGedi I think finds one transformation for all GEDI files together. Need to check with with Hancock.\n",
    "import os, subprocess, shutil\n",
    "from glob import glob\n",
    "paths = glob(\"/mnt/k/ECOFOR/gedi/gedi_data/02_download_files/GEDI_downloads_1B/*.h5\")\n",
    "# paths = paths[1:2]\n",
    "for gpath in paths:\n",
    "    lpath = \"/mnt/k/ECOFOR/lidar/2012/lidar_2012_all_lt60grd.las\"\n",
    "    outname = \"lidar2012sim_\" + os.path.basename(gpath)\n",
    "    outpath = os.path.join(\"/mnt/k/ECOFOR/lidar/2012/gedi_collocatewaves\", outname)\n",
    "    if os.path.exists(outpath):\n",
    "        print(outpath, 'already exists. Skipping.\\n')\n",
    "        continue\n",
    "\n",
    "    bounds = \"349954 7213558 402632 7370194\"\n",
    "    suggested_settings = \"-solveCofG -geoError 30 5 -fixFsig -minDense 3 -minSense 0.9\"\n",
    "    gedi_cmd = \"collocateWaves -als \" + lpath + \" -gedi \" + gpath + \" -writeWaves \" + outpath + \" -readHDFgedi -aEPSG 32736 -offset -13.84 -bounds \" + bounds + \" -checkCover -skipBeams 1234 \" + suggested_settings\n",
    "    sing_cmd = \"singularity exec --bind /mnt/k gediSingularity \" \n",
    "    cmd = sing_cmd + gedi_cmd \n",
    "    print(cmd)\n",
    "    completed = subprocess.run(cmd.split(' '))\n",
    "    shutil.move('teast.correl', outpath[:-3]+\".correl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5632e223-9cff-4510-881e-1aa5d306bcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the transformation and correlation of each granule\n",
    "paths = glob(r\"K:\\ECOFOR\\lidar\\2012\\gedi_collocatewaves\\*.correl\")\n",
    "df = pd.concat([pd.read_csv(p, skiprows=1, header=None, delimiter=\" \").assign(fname=os.path.basename(p)) for p in paths])\n",
    "\n",
    "# Set column headers\n",
    "with open(paths[0]) as f:\n",
    "    header = f.readline().replace('\\n', '')\n",
    "hlist = header[1:].split(',') # drop # and split\n",
    "hlist = ['_'.join(c.split(' ')[2:]) for c in hlist] # drop number and space\n",
    "df.columns = hlist + ['fname']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3fdeb2-5ee1-4af8-a44e-174a63a0dbbe",
   "metadata": {},
   "source": [
    "Correlation should be >0.7 and delta_CofG should be close to zero. A big difference here could indicate a poor fit or something wrong with georegisration like using different vertical datums.  \n",
    "Some files failed to produce correlation files.   \n",
    "\n",
    "Drop lidar2012sim_GEDI01_B_2020125185739_O07897_01_T03482_02_005_01_V002 during analysis because of weird fit error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80bffb9-e5d9-41bc-9569-340753d9ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run canopy metrics on all granules\n",
    "# Note gediMetric -inList not working, only runs first file\n",
    "paths = glob(\"/mnt/k/ECOFOR/lidar/2012/gedi_collocatewaves/*.h5\")\n",
    "for path in paths:\n",
    "    outroot = os.path.splitext(path)[0] + \"_canopy\"\n",
    "    gedi_cmd = \"gediMetric -input \" + path + \" -readHDFgedi -ground  -outRoot \" + outroot\n",
    "    sing_cmd = \"singularity exec --bind /mnt/k gediSingularity \"\n",
    "    cmd = sing_cmd + gedi_cmd \n",
    "    print(cmd)\n",
    "    completed = subprocess.run(cmd.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3b5c1-c537-409c-8d6c-26ffd67e071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all collocated simulated metrics data and merge\n",
    "sim_paths = glob(r\"K:\\ECOFOR\\lidar\\2012\\gedi_collocatewaves\\*metric.txt\")\n",
    "outpath = r\"K:\\ECOFOR\\lidar\\2012\\lidar12_GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31_collocate_canopy.metric.csv\"\n",
    "\n",
    "sdf = pd.concat([pd.read_csv(p, skiprows=1, header=None, delimiter=\" \") for p in sim_paths])\n",
    "\n",
    "# Set column headers\n",
    "with open(sim_paths[0]) as f:\n",
    "    header = f.readline()\n",
    "header = header[1:] # drop #\n",
    "hlist = header.split(',')\n",
    "hlist = ['_'.join(c.split(' ')[2:]) for c in hlist] # drop number and space\n",
    "hlist = hlist[:-1] # drop end of line\n",
    "\n",
    "sdf.columns = hlist\n",
    "\n",
    "# split wave_id into constituent parts\n",
    "sdf['beam'] = sdf['wave_ID'].str.split('.').str[1]\n",
    "sdf['shot_number'] = sdf['wave_ID'].str.split('.').str[2].astype(np.uint64)\n",
    "sdf['x'] = sdf['wave_ID'].str.split('.').str[3]\n",
    "sdf['y'] = sdf['wave_ID'].str.split('.').str[4]\n",
    "\n",
    "# modify metrics\n",
    "sdf['cover'] *= 100\n",
    "sdf['ALS_cover'] *= 100\n",
    "\n",
    "sdf.to_csv(outpath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d1605d-378c-4523-9af5-35099620fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and plot simulated waveform data\n",
    "sys.path.append(r\"J:\\users\\stevenf\\code\\fork\\gedisimulator\")\n",
    "from gediHandler import gediData\n",
    "\n",
    "gdat = gediData(r\"K:\\ECOFOR\\lidar\\2012\\gedi_collocatewaves\\lidar2012sim_GEDI01_B_2019174002232_O02984_01_T02212_02_005_01_V002.h5\")\n",
    "\n",
    "def plotSimWave(gdat, i):\n",
    "    '''Plot waveforms from a simulated GEDI file'''\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    # make z profile\n",
    "    gdat.res=(gdat.Z0[i]-gdat.ZN[i])/(gdat.nBins-1)\n",
    "    gdat.z=np.linspace(gdat.Z0[i],gdat.ZN[i],num=gdat.nBins)\n",
    "\n",
    "    # determine noise for scaling ground return\n",
    "    reflScale,meanN,stdev=gdat.meanNoise(i,statsLen=10)\n",
    "    # find bounds\n",
    "    minX,maxX=gdat.findBounds(meanN,stdev,i)\n",
    "    # plot it\n",
    "    #plt.plot(gdat.wave[i],gdat.z,label='Waveform')\n",
    "    #plt.plot(gdat.gWave[i]*reflScale+meanN,z,label='Ground')\n",
    "    ax.fill_betweenx(gdat.z,gdat.wave[i],meanN)\n",
    "    #plt.legend()\n",
    "    #plt.xlim(left=0)\n",
    "    ax.set(ylim=(minX,maxX), ylabel='Elevation (m)')\n",
    "    #plt.xlabel('DN')\n",
    "#     ax.ylabel('Elevation (m)')\n",
    "    return ax\n",
    "\n",
    "ax = plotSimWave(gdat, 18)\n",
    "ax.set(ylim=(210, 220))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bdee05-4d38-4399-bfd6-6216d5b31b4e",
   "metadata": {},
   "source": [
    "**Plot comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca44553-56a0-41da-b9cc-588a26b6f06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prepared GEDI canopy table\n",
    "gpath = r\"K:\\ECOFOR\\gedi\\gedi_data\\04_gedi_filtered_data_shp\\GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31.parquet\"\n",
    "\n",
    "gdf = gpd.read_parquet(gpath)\n",
    "gdf['shot_number'] = gdf['shot_number'].astype(np.uint64)\n",
    "gdf['cover'] *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce7c48-4491-4d41-b355-f4ffedcfc520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read simulated metrics from collocate waves\n",
    "cdf = pd.read_csv(r\"K:\\ECOFOR\\lidar\\2012\\lidar12_GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31_collocate_canopy.metric.csv\")\n",
    "cdf['FHD_bfix'] = cdf['FHD']-2\n",
    "\n",
    "# Read simulated metrics from original shot location (no collocation)\n",
    "sim_path = r\"K:\\ECOFOR\\lidar\\2012\\lidar12_GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31_sim_canopy.metric.txt\"\n",
    "ndf = pd.read_csv(sim_path, skiprows=1, header=None, delimiter=\" \")\n",
    "\n",
    "# Set column headers\n",
    "with open(sim_path) as f:\n",
    "    header = f.readline()\n",
    "header = header[1:] # drop #\n",
    "hlist = header.split(',')\n",
    "hlist = ['_'.join(c.split(' ')[2:]) for c in hlist] # drop number and space\n",
    "hlist = hlist[:-1] # drop end of line\n",
    "\n",
    "ndf.columns = hlist\n",
    "ndf = ndf.rename(columns={'wave_ID':'shot_number'})\n",
    "\n",
    "# modify metrics\n",
    "ndf['cover'] *= 100\n",
    "ndf['ALS_cover'] *= 100\n",
    "ndf['FHD_bfix'] = ndf['FHD']-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf6198c-5c1b-4ab5-9ae6-759aa1cca467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrete lidar metrics (but not at the colocated positions)\n",
    "lpath = r\"K:\\ECOFOR\\lidar\\2012\\lidar12_GEDI_2A_2B_merged_filtered_2019-01-01_2022-12-31_lascanopy.csv\"\n",
    "\n",
    "ldf = pd.read_csv(lpath)\n",
    "ldf = ldf.rename(columns={'name':'shot_number'})\n",
    "ldf = ldf[['shot_number', 'max', 'avg', 'p50', 'p75', 'p95', 'p98', 'd00', 'd01', 'd02', 'd03', 'd04', 'dns']]\n",
    "# Density with different height cutoffs\n",
    "ldf['d05m'] = ldf[['d00', 'd01', 'd02', 'd03', 'd04']].sum(axis=1)  # makes height cutoff 0.5 m\n",
    "ldf['d1m'] = ldf[['d01', 'd02', 'd03', 'd04']].sum(axis=1)  # makes height cutoff 1 m\n",
    "ldf['d2m'] = ldf[['d02', 'd03', 'd04']].sum(axis=1)  # makes height cutoff 2 m\n",
    "ldf['d3m'] = ldf[['d03', 'd04']].sum(axis=1)  # makes height cutoff 3 m\n",
    "ldf['d5m'] = ldf['d04']  # makes height cutoff 5 m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ca6e4-4316-42f6-921a-1492337c839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sources\n",
    "mdf = pd.merge(gdf.add_prefix('gedi_'), ndf.add_prefix('nsim_'), how='inner', left_on='gedi_shot_number', right_on='nsim_shot_number')\n",
    "mdf = pd.merge(mdf, cdf.add_prefix('csim_'), how='left', left_on='gedi_shot_number', right_on='csim_shot_number')\n",
    "mdf = pd.merge(mdf, ldf.add_prefix('als_'), how='left', left_on='gedi_shot_number', right_on='als_shot_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fef20d-e0cf-4beb-bbbe-9a08187bc854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def op_plot(xcol, ycol, hexplot=True, vmax=10):\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    sub = mdf[[xcol, ycol]].dropna()\n",
    "    x, y = sub[xcol], sub[ycol]\n",
    "    if hexplot:\n",
    "        hb = ax.hexbin(x, y, gridsize=20, mincnt=1, cmap='magma_r', linewidths=0, edgecolor='none', vmax=vmax)\n",
    "    else:\n",
    "        sns.scatterplot(x=x, y=y, ax=ax)\n",
    "    ax.plot((y.min(), y.max()), (y.min(),y.max()), '--k')\n",
    "    axmin, axmax = sub.min().min(), sub.max().max()\n",
    "    ax.set(xlabel=xcol, ylabel=ycol,\n",
    "           xlim=(axmin, axmax), ylim=(axmin, axmax))\n",
    "    \n",
    "    r2 = r2_score(y, x)\n",
    "    bias = (x-y).mean()\n",
    "    rmse = mean_squared_error(y, x)**0.5\n",
    "    \n",
    "    # add text\n",
    "    ax.text(0.99, 0.22, \"R$^2$= \" + str(r2.round(2)), transform=ax.transAxes, ha='right')\n",
    "    ax.text(0.99, 0.13, \"Bias= \"+str(bias.round(2)), transform=ax.transAxes, ha='right')\n",
    "    ax.text(0.99, 0.02,  \"RMSE= \" + str(rmse.round(2)), transform=ax.transAxes, ha='right')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8049759-aac5-4d38-8086-03f7431a742b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# comparisons\n",
    "# xcol, ycol = 'gedi_cover', 'simn_cover'\n",
    "# xcol, ycol = 'gedi_cover', 'simn_ALS_cover'\n",
    "# xcol, ycol = 'als_dns', 'nsim_cover'\n",
    "\n",
    "# xcol, ycol = 'gedi_rh95', 'simn_rhGauss_95'\n",
    "\n",
    "xcol, ycol = 'gedi_fhd_normal', 'nsim_FHD_bfix'\n",
    "\n",
    "fig = op_plot(xcol, ycol, hexplot=False, vmax=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eea183f-6189-4654-a62d-44e99e1dc7b8",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Model GEDI Canopy Structure  \n",
    "Use Landsat data to construct models of GEDI canopy height and structure metrics that can be used for mapping wall-to-wall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babceaa9-dbc4-4906-ad65-76b951eabb0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and prep data\n",
    "path = r\"J:\\projects\\ECOFOR\\gedi\\extracted\\GEDI_2AB_2019to2023_leafon_sampy500m_all.parquet\"\n",
    "df = gpd.read_parquet(path)\n",
    "\n",
    "outbasedir = r\"J:\\projects\\ECOFOR\\gedi\\models\" #r\"D:\\ECOFOR\\gedi\\models\" #r\"C:\\scratch\\ECOFOR\\gedi\\models\" #r\"J:\\projects\\ECOFOR\\corli_elephantimpacts\" #\n",
    "ver = \"v10\"\n",
    "outdir = os.path.join(outbasedir, ver)\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "outbasename = os.path.splitext(os.path.basename(path))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c501b8e9-8279-47e9-9d9b-cb09462f133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spatial filter for points intersecting the new study area for v6\n",
    "# aoi_path = r\"J:\\projects\\ECOFOR\\boundaries\\gknp_utm36n_v2.gpkg\"\n",
    "# aoi = gpd.read_file(aoi_path)[['geometry']]\n",
    "# # df = df.to_crs(aoi.crs) # should already be the same\n",
    "# df.sindex\n",
    "# df = gpd.tools.sjoin(df, aoi, how='left', predicate='intersects') # spatial join much faster than .intersects which takes 6 minutes.\n",
    "# df.dropna(subset=['index_right'], inplace=True)\n",
    "# df.drop('index_right', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e523e28-e84a-4997-8849-f1f21802e076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spatial filter the points from the buffer aoi to those intersecting only the study area\n",
    "# # TODO: Takes 37 seconds. Do this in GEDI prep next time.\n",
    "# aoi_path = r\"J:\\projects\\ECOFOR\\boundaries\\greaterkruger_utm36n.gpkg\"\n",
    "# aoi = gpd.read_file(aoi_path)[['geometry']]\n",
    "# # df = df.to_crs(aoi.crs) # should already be the same\n",
    "# df.sindex\n",
    "# df = gpd.tools.sjoin(df, aoi, how='left', predicate='intersects') # spatial join much faster than .intersects which takes 6 minutes.\n",
    "# df.dropna(subset=['index_right'], inplace=True)\n",
    "# df.drop('index_right', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab50e1e-8bb9-4543-9283-8b18cc132aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add various radar vegetation indices for PALSAR \n",
    "# # Equations taken from table 1 of Hu et al., 2024.\n",
    "# df['palsar_rc'] = df['palsar_HV'] / df['palsar_HH'] # cross-polarization ratio\n",
    "# df['palsar_rvihh'] = 4*df['palsar_HV'] / (df['palsar_HH'] + df['palsar_HV'])\n",
    "# df['palsar_rfdi'] = (df['palsar_HH'] - df['palsar_HV']) / (df['palsar_HH'] + df['palsar_HV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f378ab1c-42df-47bd-9436-91c8ad464dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns\n",
    "\n",
    "Xcols = list(cols[cols.str.startswith('lt')]) # LandTrendr\n",
    "bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2'] #'ca', \n",
    "Xcols += [col for col in cols for src in ['hls', 'l30'] for band in bands if col.startswith(src) and (band in col)] # using CCDC\n",
    "Xcols += ['palsar_HV', 'palsar_HH', 'palsar_angle'] #, 'palsar_rc', 'palsar_rvihh', 'palsar_rfdi'] \n",
    "Xcols += list(cols[cols.str.startswith('topo')])\n",
    "Xcols += list(cols[cols.str.startswith('soil')])\n",
    "# Xcols += list(cols[cols.str.startswith('ps')]) # PlanetScope\n",
    "# Xcols += list(cols[cols.str.startswith('climate')])\n",
    "# Xcols = list(cols[cols.str.startswith('dry') | cols.str.startswith('wet')])\n",
    "# Xcols = list(cols[cols.str.startswith('dry')]) + ['HH', 'HV', 'angle']\n",
    "\n",
    "\n",
    "# df['cover_gt5m'] = df['cover_z_5_10m']\n",
    "# df['cover_lt5m'] = df['cover'] - df['cover_gt5m']\n",
    "\n",
    "Ycols = ['cover', 'rh98', 'fhd_normal', 'pai']\n",
    "# Ycols = ['cover', 'cover_gt5m', 'cover_lt5m']\n",
    "meta_cols = ['delta_time', 'year', 'rain_year', 'elev_lowestmode', 'geometry']\n",
    "\n",
    "# Remove NAN's in any cols (same data for each model)\n",
    "mdf = df.dropna(subset=Xcols+Ycols)\n",
    "\n",
    "# Filter unreasonable RH98 (none anyway)\n",
    "mdf = mdf[mdf['rh98']<45]\n",
    "\n",
    "# # Take a random sample for efficiency\n",
    "# ## TODO: Run learning curves to see if there's a good number to keep and then use the rest of the data for testing\n",
    "# mdf = mdf.sample(100000, random_state=0)\n",
    "\n",
    "# Shuffle the data for use in cross-validation\n",
    "mdf = mdf.sample(frac=1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb8f14-7042-4d84-8149-908606fd4312",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcols_dict = {\n",
    "    # 'lt': [col for col in Xcols if col.startswith('lt')],\n",
    "    # 'ccdcl30': [col for col in Xcols if col.startswith('l30')],\n",
    "    # 'ccdchls': [col for col in Xcols if col.startswith('hls')],\n",
    "     'p': ['palsar_HV', 'palsar_HH', 'palsar_angle'],#, 'palsar_rc', 'palsar_rvihh', 'palsar_rfdi'],\n",
    "# #     'ps': [col for col in Xcols if col.startswith('ps')],\n",
    "    # 's-t': [col for col in Xcols for dstr in [ 'topo', 'soil'] if col.startswith(dstr)],\n",
    "    # 'lt-p': [col for col in Xcols if col.startswith('lt')] + ['palsar_HV', 'palsar_HH', 'palsar_angle'],\n",
    "    # 'ccdcl30-p': [col for col in Xcols if col.startswith('l30')] + ['palsar_HV', 'palsar_HH', 'palsar_angle'],\n",
    "    # 'ccdchls-p': [col for col in Xcols if col.startswith('hls')] + ['palsar_HV', 'palsar_HH', 'palsar_angle'],\n",
    "# #     'p-ps': [col for col in Xcols if col.startswith('ps')] + ['palsar_HV', 'palsar_HH', 'palsar_angle'],\n",
    "    'lt-p-s-t': [col for col in Xcols for dstr in ['lt', 'palsar', 'topo', 'soil'] if col.startswith(dstr)],\n",
    "    # 'ccdcl30-p-s-t': [col for col in Xcols for dstr in ['l30', 'palsar', 'topo', 'soil'] if col.startswith(dstr)],\n",
    "    # 'ccdchls-p-s-t': [col for col in Xcols for dstr in ['hls', 'palsar', 'topo', 'soil'] if col.startswith(dstr)],\n",
    "    # 'p-s-t': [col for col in Xcols for dstr in ['palsar', 'topo', 'soil'] if col.startswith(dstr)],\n",
    "}\n",
    "\n",
    "Xsets = Xcols_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d16f8-85c7-476c-a3f8-e4ba368c0d0c",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed390978-9426-4558-9267-96d55b88651a",
   "metadata": {},
   "source": [
    "### Learning Curves  \n",
    "Construct learning curves for a few metrics to get a sense of how many data points are needed to construct accurate models and the training/prediction time tradeoff.  \n",
    "\n",
    "TODO: try to get random forest learning curves to work with out-of-bag accuracy and skip using CV because it's slower and not necessary and plain training accuracy is useless for RF. See the GLRI-TCC learning curve code for starting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57e903d-249b-49f5-b733-ccddd0de6abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a4791b4-5e1c-4441-bb25-5c878192f4b7",
   "metadata": {},
   "source": [
    "### Model with different predictor sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124cd7c5-03f0-41c3-b3cc-c6a6faf9c253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structures to hold predictions, feature importances, and model objects\n",
    "pdf = mdf[meta_cols+Ycols].copy()\n",
    "imp_dict = {}\n",
    "model_dict = {}\n",
    "\n",
    "for Xset, Xcols in Xcols_dict.items():\n",
    "    # Run models\n",
    "    X = mdf[Xcols]\n",
    "    for ycol in Ycols:\n",
    "        y = mdf[ycol]\n",
    "        \n",
    "        \n",
    "        # # Get CV predictions\n",
    "        # rf = RandomForestRegressor(n_estimators=200, max_features='sqrt', oob_score=False, random_state=0, n_jobs=20)\n",
    "        # cv_preds = cross_val_predict(rf, X, y, cv=10, n_jobs=1)\n",
    "        # pdf['pred_'+Xset+'_'+ycol] = pd.Series(cv_preds, index=y.index)\n",
    "        \n",
    "        # Get OOB predictions\n",
    "        rf = RandomForestRegressor(n_estimators=100, max_features='sqrt', oob_score=True, random_state=0, n_jobs=20)\n",
    "        rf.fit(X,y)\n",
    "        pdf['pred_'+Xset+'_'+ycol] = pd.Series(rf.oob_prediction_, index=y.index)\n",
    "\n",
    "        # Get feature importances of every tree\n",
    "        imps = [tree.feature_importances_ for tree in rf.estimators_]\n",
    "        imps = pd.DataFrame(imps, columns=X.columns)\n",
    "        imp_dict[Xset+'_'+ycol] = imps\n",
    "\n",
    "        # keep the model and training data for the model\n",
    "        model_dict[Xset+'_'+ycol] = rf \n",
    "\n",
    "# merge feature importances\n",
    "imp_merged = pd.concat(imp_dict, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8e5de5-fa79-47e4-9636-5824f5e305b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test use of Nearest Neighbors instead of RF for v07\n",
    "# from sklearn import neighbors\n",
    "# from sklearn.model_selection import cross_val_predict #train_test_split\n",
    "\n",
    "# # Use top 10 RF model vars for the NN model\n",
    "# imp_path = r\"J:\\projects\\ECOFOR\\gedi\\models\\v04\\GEDI_2AB_2019to2023_leafon_sampy500m_all_imps_v04.csv\"\n",
    "# imps = pd.read_csv(imp_path, header=[0,1])\n",
    "# imps = imps.mean(axis=0)\n",
    "\n",
    "# # Structures to hold predictions, feature importances, and model objects\n",
    "# pdf = mdf[meta_cols+Ycols].copy()\n",
    "# imp_dict = {}\n",
    "# model_dict = {}\n",
    "\n",
    "# for Xset, Xcols in Xcols_dict.items():    \n",
    "#     for ycol in Ycols:\n",
    "#         # Get subset of variables to use\n",
    "#         Xselected = imps[(Xset+\"_\"+ycol, slice(None))]\n",
    "#         Xselected = Xselected.sort_values(ascending=False)[:10].index\n",
    "        \n",
    "#         X = mdf[Xselected]\n",
    "#         y = mdf[ycol]\n",
    "#         nn = neighbors.KNeighborsRegressor(n_neighbors=2, algorithm=\"auto\", leaf_size=30, p=2, metric='minkowski', n_jobs=1)\n",
    "        \n",
    "#         # Get CV predictions\n",
    "#         cv_preds = cross_val_predict(nn, X, y, cv=10, n_jobs=-1)\n",
    "#         pdf['pred_'+Xset+'_'+ycol] = pd.Series(cv_preds, index=y.index)\n",
    "\n",
    "#         # redo model with all data\n",
    "#         nn.fit(X, y)\n",
    "#         model_dict[Xset+'_'+ycol] = nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0410b70d-a02d-48f3-a20f-bcd75a7937fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot rh98 and cover for v04_ltpa for jody\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(5.7, 2))\n",
    "\n",
    "# Ycols_sub = {'rh98':'RH98', 'cover':'Cover'}\n",
    "# for j, (ycol, ax) in enumerate(zip(Ycols_sub.keys(), axes)):\n",
    "#     x, y = pdf['pred_'+Xset+'_'+ycol], pdf[ycol]\n",
    "#     hb = ax.hexbin(x, y, gridsize=20, mincnt=1, cmap='magma_r', linewidths=0, edgecolor='none', vmax=2000)\n",
    "#     ax.plot((y.min(), y.max()), (y.min(),y.max()), '--k')\n",
    "\n",
    "#     r2 = r2_score(y, x)\n",
    "#     bias = (x-y).mean()\n",
    "#     rmse = mean_squared_error(y, x)**0.5\n",
    "\n",
    "#     # add text\n",
    "#     ax.text(0.99, 0.22, \"R$^2$= \" + str(r2.round(2)), transform=ax.transAxes, ha='right')\n",
    "#     ax.text(0.99, 0.13, \"Bias= \"+str(bias.round(2)), transform=ax.transAxes, ha='right')\n",
    "#     ax.text(0.99, 0.02,  \"RMSE= \" + str(rmse.round(2)), transform=ax.transAxes, ha='right')\n",
    "\n",
    "#     ax.set(title=Ycols_sub[ycol])\n",
    "#     if j==0:\n",
    "#         ax.set(ylabel='Observed')\n",
    "#     fig.supxlabel('Predicted', x=0.47, y=-0.1, ha='center', fontsize=10)\n",
    "\n",
    "# fig.subplots_adjust(wspace=0.3, hspace=0.5)\n",
    "# cb = fig.colorbar(hb, ax=axes, shrink=True, aspect=30, pad=0.02) #cax=cax, aspect=)#\n",
    "# cb.set_label('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33679deb-e606-4778-b87b-07e48e3b4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model results\n",
    "nrows = len(Xcols_dict.keys())\n",
    "ncols = len(Ycols)\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*3, 2.4*nrows))\n",
    "\n",
    "if nrows==1:\n",
    "    axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "for i, (Xset, axrow) in enumerate(zip(Xsets, axes)):\n",
    "    for j, (ycol, ax) in enumerate(zip(Ycols, axrow)):\n",
    "        x, y = pdf['pred_'+Xset+'_'+ycol], pdf[ycol]\n",
    "        hb = ax.hexbin(x, y, gridsize=20, mincnt=1, cmap='magma_r', linewidths=0, edgecolor='none', vmax=2000)\n",
    "        ax.plot((y.min(), y.max()), (y.min(),y.max()), '--k')\n",
    "\n",
    "        r2 = r2_score(y, x)\n",
    "        bias = (x-y).mean()\n",
    "        rmse = mean_squared_error(y, x)**0.5\n",
    "\n",
    "        # add text\n",
    "        ax.text(0.99, 0.22, \"R$^2$= \" + str(np.round(r2, 2)), transform=ax.transAxes, ha='right')\n",
    "        ax.text(0.99, 0.13, \"Bias= \"+str(bias.round(2)), transform=ax.transAxes, ha='right')\n",
    "        ax.text(0.99, 0.02,  \"RMSE= \" + str(np.round(rmse, 2)), transform=ax.transAxes, ha='right')\n",
    "\n",
    "        ax.set(title=Xset+'_'+ycol)\n",
    "        if j==0:\n",
    "            ax.set(ylabel='Observed')\n",
    "        fig.supxlabel('Predicted', x=0.47, y=0, ha='center', fontsize=10)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.3, hspace=0.5)\n",
    "cb = fig.colorbar(hb, ax=axes, shrink=True, aspect=30, pad=0.02) #cax=cax, aspect=)#\n",
    "cb.set_label('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699103f4-c237-40ea-821c-dfe4772fba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions of the predictions and observations\n",
    "from scipy.stats import ks_2samp, t\n",
    "\n",
    "nrows = len(Xcols_dict.keys())\n",
    "fig, axes = plt.subplots(nrows, 4, figsize=(10, 2.4*nrows))\n",
    "\n",
    "if nrows==1:\n",
    "    axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "for i, (Xset, axrow) in enumerate(zip(Xsets, axes)):\n",
    "    for j, (ycol, ax) in enumerate(zip(Ycols, axrow)):\n",
    "        xcol = 'pred_'+Xset+'_'+ycol\n",
    "        xydf = pd.melt(pdf[[xcol, ycol]])\n",
    "        sns.ecdfplot(xydf, x='value', hue='variable', ax=ax, legend=False)\n",
    "\n",
    "        # ks_2samp silently gives wrong values if nan's included, so make sure they're removed\n",
    "        mask = pdf[[xcol, ycol]].notna().all(axis=1)\n",
    "        ks, pval = ks_2samp(pdf.loc[mask, xcol], pdf.loc[mask, ycol])\n",
    "        ax.text(0.95, 0.1, f\"KS = {np.round(ks,2)}\", ha='right', transform=ax.transAxes)\n",
    "\n",
    "        ax.set(ylabel=None, title=Xset+'_'+ycol)\n",
    "        \n",
    "orange_line = mpl.lines.Line2D([0], [0], color='orange', lw=2)\n",
    "blue_line = mpl.lines.Line2D([0], [0], color='blue', lw=2)\n",
    "fig.legend([orange_line, blue_line], ['obs', 'pred'], loc='upper center', ncol=2)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8b2e1-286d-4df9-b2e7-5dc007503d61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine and save predictions, importances, and model objects\n",
    "\n",
    "# TODO: Set these data types on load next time\n",
    "pdf['delta_time'] = pdf['delta_time'].astype(str)\n",
    "\n",
    "# oob_path = os.path.join(outdir, outbasename+\"_oob_\"+ver+\".gpkg\")\n",
    "# pdf.to_file(oob_path, driver='GPKG', index=True)\n",
    "\n",
    "oob_path = os.path.join(outdir, outbasename+\"_oob_\"+ver+\".parquet\")\n",
    "pdf.to_parquet(oob_path, index=True)\n",
    "\n",
    "imp_path = os.path.join(outdir, outbasename+\"_imps_\"+ver+\".csv\")\n",
    "imp_merged.to_csv(imp_path, index=False)\n",
    "\n",
    "for dset, model in model_dict.items():\n",
    "    model_path = os.path.join(outdir, outbasename + \"_\" + dset + \"_\" + ver + \".joblib\")\n",
    "    joblib.dump(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d45cfd-c3e3-453f-82d7-483344286e3a",
   "metadata": {},
   "source": [
    "### Temporal cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7e9e8-142c-49c3-a0a2-20d8929fc137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for TCV\n",
    "tcv_path = os.path.join(outdir, outbasename+\"_tcv_\"+ver+\".parquet\")\n",
    "\n",
    "years = list(mdf['rain_year'].unique())\n",
    "years.sort()\n",
    "meta_cols = ['delta_time', 'year', 'rain_year']\n",
    "pdf = mdf[meta_cols+Ycols].copy()\n",
    "stats = pd.DataFrame(columns=['Xset', 'metric', 'year', 'n', 'r2', 'rmse', 'bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefcc085-3bb6-4328-9a7b-3ce2fed149dc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform temporal cross-validation on all Xsets\n",
    "for Xset, Xcols in Xcols_dict.items():\n",
    "    for ycol in Ycols:\n",
    "        print(Xset, ycol)\n",
    "        for year in years:\n",
    "            ddf = mdf[Xcols+[ycol, 'rain_year']].dropna()\n",
    "            train, test = ddf[ddf['rain_year']!=year], mdf[mdf['rain_year']==year]\n",
    "            Xtrain, ytrain = train[Xcols], train[ycol]\n",
    "            Xtest, ytest = test[Xcols], test[ycol]\n",
    "            rf = RandomForestRegressor(n_estimators=100, max_features='sqrt', oob_score=False, random_state=0, n_jobs=20)\n",
    "            rf = rf.fit(Xtrain, ytrain)\n",
    "            pred = rf.predict(Xtest)\n",
    "            pdf.loc[ytest.index, Xset+'_'+ycol] = pd.Series(pred, index=ytest.index)\n",
    "\n",
    "            # accuracy stats\n",
    "            # TODO: consider assigning non-forest as 0, like use of irr_area in irrigation_ks.ipynb\n",
    "            obs = ytest.copy()\n",
    "\n",
    "            ix = len(stats)\n",
    "            stats.loc[ix, 'metric'] = ycol \n",
    "            stats.loc[ix, 'Xset'] = Xset\n",
    "            stats.loc[ix, 'year'] = year\n",
    "            stats.loc[ix, 'n'] = obs.size\n",
    "            stats.loc[ix, 'r2'] = r2_score(obs, pred)\n",
    "            stats.loc[ix, 'rmse'] = mean_squared_error(obs, pred)**0.5\n",
    "            stats.loc[ix, 'bias'] = bias = (pred-obs).mean()\n",
    "\n",
    "            # model_dict[dsetyr+'tcv'] = rf\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f4a080-d1e5-4e6b-a45e-1fa148079991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "pdf.to_parquet(tcv_path, index=True)\n",
    "stats.to_csv(os.path.splitext(tcv_path)[0]+\"_stats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a953a74c-b49f-4d1e-ba5b-a7ba09d22fcc",
   "metadata": {},
   "source": [
    "### Bias correction of selected model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb42b89c-0528-4b27-a555-aa551d309d79",
   "metadata": {},
   "source": [
    "#### BC1\n",
    "Use BC1 in Zhang and Lu 2012 to reduce compression to the mean in the selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c57b1e6-fa88-4752-afd2-7d066a676f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup \n",
    "Xset = 'lt-p-s-t' # selected model predictor set\n",
    "Xcols = Xcols_dict[Xset]\n",
    "\n",
    "train, test = train_test_split(mdf, test_size=0.3, random_state=42)\n",
    "pdf = test[meta_cols+Ycols].copy()\n",
    "model_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e36c94-a98d-48c5-9505-5f71535c4ea7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run bias corrected random forest on each GEDI metric\n",
    "for ycol in Ycols:\n",
    "    Xtrain, ytrain = train[Xcols], train[ycol]\n",
    "    Xtest, ytest = test[Xcols], test[ycol]\n",
    "    \n",
    "    # Random forest 1 with OOB predictions\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_features='sqrt', oob_score=True, random_state=0, n_jobs=20)\n",
    "    rf.fit(Xtrain,ytrain)\n",
    "    ypred = pd.Series(rf.oob_prediction_, index=ytrain.index, name='ypred')\n",
    "\n",
    "    # Random forest 2 of residuals\n",
    "    resids = ytrain - ypred\n",
    "    rtrain = pd.concat([Xtrain, ypred], axis=1)\n",
    "    rf_resid = RandomForestRegressor(n_estimators=100, max_features='sqrt', oob_score=True, random_state=0, n_jobs=20)\n",
    "    rf_resid.fit(rtrain, resids)\n",
    "\n",
    "    # Apply both RFs to test data and sum pred + resid\n",
    "    ypred_test = pd.Series(rf.predict(Xtest), index=ytest.index, name='ypred')\n",
    "    rtest = pd.concat([Xtest, ypred_test], axis=1)\n",
    "    resids_test = pd.Series(rf_resid.predict(rtest), index=rtest.index, name='resid')\n",
    "    bc_test = ypred_test + resids_test\n",
    "\n",
    "    # Save test results\n",
    "    pdf[\"pred_\"+ycol] = ypred_test\n",
    "    pdf[\"pred_resid_\"+ycol] = resids_test\n",
    "    pdf[\"pred_bc_\"+ycol] = bc_test\n",
    "    model_dict[ycol] = rf\n",
    "    model_dict[ycol+\"_resid\"] = rf_resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d3dd57-b7f1-4f04-a1b5-b9d75c9fe889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine and save predictions, importances, and model objects\n",
    "\n",
    "# TODO: Set these data types on load next time\n",
    "pdf['delta_time'] = pdf['delta_time'].astype(str)\n",
    "\n",
    "# oob_path = os.path.join(outdir, outbasename+\"_oob_\"+ver+\".gpkg\")\n",
    "# pdf.to_file(oob_path, driver='GPKG', index=True)\n",
    "\n",
    "bc_path = os.path.join(outdir, outbasename+\"_bc_\"+ver+\".parquet\")\n",
    "pdf.to_parquet(bc_path, index=True)\n",
    "\n",
    "for mname, model in model_dict.items():\n",
    "    model_path = os.path.join(outdir, outbasename + \"_\" + mname + \"_bc_\" + ver + \".joblib\")\n",
    "    joblib.dump(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8fd1ed-b757-4695-98f8-29f2eab555f2",
   "metadata": {},
   "source": [
    "#### EDM\n",
    "Use empirical distribution modeling from ___."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba720c7-86ae-40c5-ab0c-246777be524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c96b49e-1572-4f0b-a9ba-cc805490663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core EDM Implementation ---\n",
    "def empirical_distribution_matching(X_train, Y_train, X_test, random_state=42):\n",
    "    \"\"\"\n",
    "    Implements the Empirical Distribution Matching (EDM) method for post-processing\n",
    "    Random Forest predictions based on OOB scores.\n",
    "\n",
    "    The method finds a mapping function f such that f(Y_OOB) matches the empirical\n",
    "    distribution of the true observations (Y_train). This function is then applied\n",
    "    to the raw test set predictions (Y_test_pred_raw).\n",
    "\n",
    "    f(Y_test_pred_raw) = F_Y_train^-1( F_Y_OOB(Y_test_pred_raw) )\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Training features.\n",
    "        Y_train (np.ndarray): True training observations.\n",
    "        X_test (np.ndarray): Test features.\n",
    "        random_state (int): Random seed for the Random Forest model.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Y_test_pred_corrected, Y_test_pred_raw, rf_model)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Train Random Forest and obtain OOB predictions\n",
    "    # oob_score=True is essential for the EDM method\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        oob_score=True,\n",
    "        random_state=random_state,\n",
    "        max_features='sqrt',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train, Y_train)\n",
    "\n",
    "    # Check for OOB predictions availability\n",
    "    if not hasattr(rf, 'oob_prediction_') or rf.oob_prediction_ is None:\n",
    "        raise ValueError(\n",
    "            \"RandomForestRegressor failed to produce OOB predictions. \"\n",
    "            \"Ensure 'oob_score=True' is set and n_estimators is large enough.\"\n",
    "        )\n",
    "\n",
    "    Y_OOB = rf.oob_prediction_\n",
    "    Y_obs = Y_train  # True observations (Y)\n",
    "\n",
    "    # Filter out NaNs if any (can happen with small datasets/estimators, though unlikely here)\n",
    "    valid_indices = ~np.isnan(Y_OOB)\n",
    "    Y_OOB_valid = Y_OOB[valid_indices]\n",
    "    Y_obs_valid = Y_obs[valid_indices]\n",
    "\n",
    "    # 2. Compute the ECDF of OOB predictions (F_Y_OOB)\n",
    "    # We sort the OOB predictions and define their corresponding cumulative probabilities.\n",
    "    Y_OOB_sorted = np.sort(Y_OOB_valid)\n",
    "    n_oob = len(Y_OOB_sorted)\n",
    "    # ECDF probabilities are defined at [1/N, 2/N, ..., 1]\n",
    "    p_oob = np.linspace(1 / n_oob, 1.0, n_oob)\n",
    "\n",
    "    # 3. Compute the Inverse ECDF (Quantile Function) of Observations (F_Y_obs^-1)\n",
    "    # We sort the observations and define their corresponding cumulative probabilities.\n",
    "    Y_obs_sorted = np.sort(Y_obs_valid)\n",
    "    n_obs = len(Y_obs_sorted)\n",
    "    # Probabilities for interpolation (P = F_Y_obs(Y_obs_sorted))\n",
    "    p_obs = np.linspace(1 / n_obs, 1.0, n_obs)\n",
    "\n",
    "    # 4. Apply the EDM transformation\n",
    "    Y_test_pred_raw = rf.predict(X_test)\n",
    "\n",
    "    # Step A: Find the ECDF value (probability 'p') for each raw test prediction\n",
    "    # This is p = F_Y_OOB(Y_test_pred_raw).\n",
    "    # We interpolate from (Y_OOB_sorted, p_oob) to Y_test_pred_raw.\n",
    "    prob_values = np.interp(\n",
    "        Y_test_pred_raw,  # Values to evaluate (raw test predictions)\n",
    "        Y_OOB_sorted,     # Known x-coordinates (sorted OOB predictions)\n",
    "        p_oob,            # Known y-coordinates (ECDF probabilities)\n",
    "        left=0.0,         # Clip probability to 0.0 below min\n",
    "        right=1.0         # Clip probability to 1.0 above max\n",
    "    )\n",
    "\n",
    "    # Step B: Apply the Inverse ECDF of observations (Quantile function)\n",
    "    # This is Y_test_pred_corrected = F_Y_obs^-1(prob_values).\n",
    "    # We interpolate from (p_obs, Y_obs_sorted) to the probability values 'prob_values'.\n",
    "    Y_test_pred_corrected = np.interp(\n",
    "        prob_values,      # Values to evaluate (the probability 'p' from Step A)\n",
    "        p_obs,            # Known x-coordinates (ECDF probabilities of observations)\n",
    "        Y_obs_sorted,     # Known y-coordinates (sorted observations/quantiles)\n",
    "        left=Y_obs_sorted[0],\n",
    "        right=Y_obs_sorted[-1]\n",
    "    )\n",
    "\n",
    "    return Y_test_pred_corrected, Y_test_pred_raw, rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79952a9-b3bb-43e4-a58d-cfcff8a0593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup \n",
    "Xset = 'lt-p-s-t' # selected model predictor set\n",
    "Xcols = Xcols_dict[Xset]\n",
    "\n",
    "train, test = train_test_split(mdf, test_size=0.3, random_state=42)\n",
    "pdf = test[meta_cols+Ycols].copy()\n",
    "model_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a95387-ffc8-4c1d-84a0-c50a1d7c89d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ycol in Ycols:\n",
    "    Xtrain, ytrain = train[Xcols], train[ycol]\n",
    "    Xtest, ytest = test[Xcols], test[ycol]\n",
    "\n",
    "    # Execute the EDM method\n",
    "    bc_test, ypred_test, rf = empirical_distribution_matching(\n",
    "        Xtrain, ytrain, Xtest\n",
    "    )\n",
    "    \n",
    "    # Save test results\n",
    "    pdf[\"pred_\"+ycol] = ypred_test\n",
    "    pdf[\"pred_bc_\"+ycol] = bc_test\n",
    "    model_dict[ycol] = rf\n",
    "\n",
    "    # 2. Evaluate Performance\n",
    "\n",
    "    # Evaluate the raw (uncalibrated) predictions\n",
    "    mse_raw = mean_squared_error(ytest, ypred_test)\n",
    "\n",
    "    # Evaluate the corrected (EDM-calibrated) predictions\n",
    "    mse_corrected = mean_squared_error(ytest, bc_test)\n",
    "    \n",
    "    print(\"-\"*20)\n",
    "    print(f\"Metric: {ycol}\\n\")\n",
    "    print(f\"Model OOB Score (R^2): {rf.oob_score_:.4f}\")\n",
    "    print(\"\\nPrediction Statistics:\")\n",
    "    print(f\"  True Test Mean: {np.mean(ytest):.4f}\")\n",
    "    print(f\"  Raw Pred Mean:  {np.mean(ypred_test):.4f} (Note the difference/bias)\")\n",
    "    print(f\"  EDM Pred Mean:  {np.mean(bc_test):.4f} (Should be closer to True Mean)\")\n",
    "\n",
    "    print(\"\\nError Metrics (Mean Squared Error):\")\n",
    "    print(f\"  1. Raw RF Prediction MSE:    {mse_raw:.4f}\")\n",
    "    print(f\"  2. EDM Corrected Prediction MSE: {mse_corrected:.4f}\")\n",
    "\n",
    "    improvement = ((mse_raw - mse_corrected) / mse_raw) * 100\n",
    "    print(f\"\\nResult: EDM improved the MSE by {improvement:.2f}%\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Quick check on the corrected distribution matching\n",
    "    print(\"Quantile Check (Should be similar for ytest and bc_test):\")\n",
    "    quantiles = [0.05, 0.5, 0.95]\n",
    "    print(f\"  {quantiles[0]*100}% Quantile:\")\n",
    "    print(f\"    True Observations: {np.quantile(ytest, quantiles[0]):.4f}\")\n",
    "    print(f\"    Raw Predictions:   {np.quantile(ypred_test, quantiles[0]):.4f}\")\n",
    "    print(f\"    EDM Corrected:     {np.quantile(bc_test, quantiles[0]):.4f}\")\n",
    "    print(f\"  {quantiles[1]*100}% Quantile (Median):\")\n",
    "    print(f\"    True Observations: {np.quantile(ytest, quantiles[1]):.4f}\")\n",
    "    print(f\"    Raw Predictions:   {np.quantile(ypred_test, quantiles[1]):.4f}\")\n",
    "    print(f\"    EDM Corrected:     {np.quantile(bc_test, quantiles[1]):.4f}\")\n",
    "    print(f\"  {quantiles[2]*100}% Quantile:\")\n",
    "    print(f\"    True Observations: {np.quantile(ytest, quantiles[2]):.4f}\")\n",
    "    print(f\"    Raw Predictions:   {np.quantile(ypred_test, quantiles[2]):.4f}\")\n",
    "    print(f\"    EDM Corrected:     {np.quantile(bc_test, quantiles[2]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5081cdee-6c4f-449c-b5ce-f53fff95813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = 'J:\\\\projects\\\\ECOFOR\\\\gedi\\\\models\\\\v08EDM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29899f-e4a3-4f37-a5d9-6a8a23ae2602",
   "metadata": {},
   "outputs": [],
   "source": [
    "ver = \"v08EDM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252d4b02-49f2-4d81-ab49-4f9756eb572e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine and save predictions, importances, and model objects\n",
    "\n",
    "# TODO: Set these data types on load next time\n",
    "pdf['delta_time'] = pdf['delta_time'].astype(str)\n",
    "\n",
    "# oob_path = os.path.join(outdir, outbasename+\"_oob_\"+ver+\".gpkg\")\n",
    "# pdf.to_file(oob_path, driver='GPKG', index=True)\n",
    "\n",
    "bc_path = os.path.join(outdir, outbasename+\"_bc_\"+ver+\".parquet\")\n",
    "pdf.to_parquet(bc_path, index=True)\n",
    "\n",
    "for mname, model in model_dict.items():\n",
    "    model_path = os.path.join(outdir, outbasename + \"_\" + mname + \"_bc_\" + ver + \".joblib\")\n",
    "    joblib.dump(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c6c54-c457-430a-b3f8-726c3e429f01",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a15de6-521e-4b3d-8788-78898166ba9d",
   "metadata": {},
   "source": [
    "## Pyramids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e237d-5c14-4358-b393-27c0c442212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating pyramids and stats for all maps\n",
    "\n",
    "# gdaladdo -ro --config COMPRESS_OVERVIEW ZSTD --config ZSTD_LEVEL_OVERVIEW 1 --config PREDICTOR_OVERVIEW 2 --config INTERLEAVE_OVERVIEW BAND --config GDAL_NUM_THREADS 16 --config GDAL_CACHEMAX 4096 path\n",
    "\n",
    "# helper functions\n",
    "def pyr_stats(path, run=True):\n",
    "    \"\"\"Set nodata (str of number or 'nan'). Calculate stats and pyramids for image at path (str).\"\"\"\n",
    "    cmds = {'stats':[], 'pyr':[]}\n",
    "    \n",
    "    stats_cmd = 'gdalinfo -approx_stats ' + path\n",
    "    if run:\n",
    "        result = subprocess.check_output(stats_cmd)\n",
    "    \n",
    "    if not os.path.exists(path+\".ovr\"):\n",
    "        pyr_cmd = 'gdaladdo -ro --config COMPRESS_OVERVIEW ZSTD --config ZSTD_LEVEL 1 --config PREDICTOR 2 --config INTERLEAVE_OVERVIEW BAND --config GDAL_NUM_THREADS 6 --config GDAL_CACHEMAX 4096 ' + path\n",
    "        if run:\n",
    "            result = subprocess.check_output(pyr_cmd)\n",
    "    else:\n",
    "        pyr_cmd = \"ECHO \" + path + \" completed\"\n",
    "    \n",
    "    return stats_cmd, pyr_cmd\n",
    "\n",
    "stat_cmds, pyr_cmds = [], []\n",
    "paths = glob(r\"D:\\ECOFOR\\gedi\\maps\\v04_ltpa2\\*\\*.tif\")\n",
    "for path in paths:\n",
    "    stat_cmd, pyr_cmd = pyr_stats(path, run=False)\n",
    "    stat_cmds.append(stat_cmd)\n",
    "    pyr_cmds.append(pyr_cmd)\n",
    "\n",
    "cmd_concurrent(stat_cmds, threads=16)\n",
    "cmd_concurrent(pyr_cmds, threads=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ed9d05-2dbc-44a0-ab77-69379807f9ff",
   "metadata": {},
   "source": [
    "## Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cdb4c9-2ae7-45ec-8afc-b3c08b3f4719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate difference between two years for each metric\n",
    "basedir = r\"J:\\projects\\ECOFOR\\gedi\\maps\\v08\\lt-p-s-t\"\n",
    "y1 = \"2017\"\n",
    "y2 = \"2021\"\n",
    "pct_chg = False #True\n",
    "mask_y1_lt = 5 # mask out areas where y1 is less than x\n",
    "\n",
    "outdir = os.path.join(basedir, 'change')\n",
    "metrics = [\"cover_gt5m\"] #\"rh98\", \"cover\"] #, \"pai\", \"fhd\",, \n",
    "\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "for metric in metrics:\n",
    "    path1 = os.path.join(basedir, metric, metric+\"_\"+y1+\".tif\")\n",
    "    path2 = os.path.join(basedir, metric, metric+\"_\"+y2+\".tif\")\n",
    "    pct_str = \"_pct\" if pct_chg else \"\"\n",
    "    mask_str = \"_y1gte\"+str(mask_y1_lt) if mask_y1_lt else \"\"\n",
    "    outpath = os.path.join(outdir, metric+\"_\"+y2+\"minus\"+y1+pct_str+mask_str+\".tif\")\n",
    "    \n",
    "    with rasterio.open(path1) as src:\n",
    "        arr1 = src.read(1)\n",
    "        profile = src.profile\n",
    "    \n",
    "    with rasterio.open(path2) as src:\n",
    "        arr2 = src.read(1)\n",
    "        \n",
    "    if mask_y1_lt:\n",
    "        arr1[arr1<mask_y1_lt] = np.nan\n",
    "\n",
    "    chg = np.subtract(arr2, arr1, dtype=np.float32)\n",
    "    chg[np.isnan(arr1) | np.isnan(arr2)] = np.nan\n",
    "    \n",
    "    \n",
    "    if pct_chg:\n",
    "        arr1[arr1==0] = 0.01\n",
    "        chg = chg / arr1 * 100\n",
    "    \n",
    "    # change if using integer layers\n",
    "    # chg = np.subtract(arr2, arr1, dtype=np.int16)\n",
    "    # chg[(arr1==profile['nodata']) | (arr2==profile['nodata'])] = -32768\n",
    "    # profile['nodata'] = -32768\n",
    "    # profile['dtype'] = np.int16\n",
    "\n",
    "    with rasterio.open(outpath, 'w', **profile) as dst:\n",
    "        dst.write(chg, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f2a2a5-1db6-4971-9a44-89bdcce5858e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stat_cmds, pyr_cmds = [], []\n",
    "paths = glob(r\"J:\\projects\\ECOFOR\\gedi\\maps\\v08\\lt-p-s-t\\change\\*.tif\")\n",
    "for path in paths:\n",
    "    stat_cmd, pyr_cmd = pyr_stats(path, run=False)\n",
    "    stat_cmds.append(stat_cmd)\n",
    "    pyr_cmds.append(pyr_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd89d1e-329a-4ede-92d3-5ef5f52fa08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_concurrent(stat_cmds, threads=10)\n",
    "cmd_concurrent(pyr_cmds, threads=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3f865f-0d24-45fa-9a56-05d970bdbf52",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload to GEE  \n",
    "Rescale each of the datasets to an 8-bit integer, and save the scale and offset. Then upload to GEE.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f53fbd-252f-4bd2-83bf-15e64d0b44fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = r\"D:\\ECOFOR\\gedi\\maps\\v08\\lt-p-s-t\"\n",
    "metrics = next(os.walk(basedir))[1]\n",
    "\n",
    "for metric in metrics:\n",
    "    paths = glob(os.path.join(basedir, metric, \"*.tif\"))\n",
    "    outdir = os.path.join(basedir, metric, 'as_uint8')\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    \n",
    "    # load each array and get the min & max for a universal scaling\n",
    "    # TODO: could load all at once and then process to avoid reading twice\n",
    "    mins, maxs = [], []\n",
    "    for path in paths:\n",
    "        with rasterio.open(path) as src:\n",
    "            arr = src.read(1, masked=True)\n",
    "            mins.append(arr.min())\n",
    "            maxs.append(arr.max())\n",
    "    low = np.array(mins).min()\n",
    "    high = np.array(maxs).max()\n",
    "    \n",
    "    scale = 254.0 / (high - low)  # setting 254 as max and 255 as nodata\n",
    "    offset = -low * scale\n",
    "    \n",
    "    for path in paths:\n",
    "        with rasterio.open(path) as src:\n",
    "            arr = src.read(1, masked=True)\n",
    "            prof = src.profile\n",
    "        \n",
    "        arr = (arr * scale + offset).round().astype(np.uint8)\n",
    "        arr = arr.filled(255)\n",
    "        \n",
    "        outpath = os.path.join(outdir, os.path.basename(path))\n",
    "        prof.update({'dtype':np.uint8, 'nodata':255})\n",
    "        with rasterio.open(outpath, 'w', **prof) as dst:\n",
    "            dst.write(arr, 1)\n",
    "            \n",
    "    text = (\"scale = \" + str(scale) + \"\\n\" + \n",
    "            \"offset = \" + str(offset) + \"\\n\" + \n",
    "            \"orig = (value - offset) / scale\")\n",
    "            \n",
    "    with open(os.path.join(outdir, \"readme.txt\"), 'w') as f:\n",
    "        f.writelines(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b072d1c-cf66-4097-a293-af1265f775db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image collections in legacy storage for each metric\n",
    "for metric in metrics:\n",
    "    indir = os.path.join(r\"D:\\ECOFOR\\gedi\\maps\\v08\\lt-p-s-t\", metric, \"as_uint8\")\n",
    "    asset_id = \"projects/earthengine-legacy/assets/users/stevenf/ecofor/\" + metric + \"_v8_uint8\"\n",
    "    cmd = \"earthengine create collection \" + asset_id\n",
    "    stdout = subprocess.check_output(cmd)\n",
    "    \n",
    "    # Get scale and offset to write in the image and collection properties\n",
    "    # TODO: next time process metric groups so reading the readme doesn't need to be repeated\n",
    "    readme = os.path.join(indir, \"readme.txt\")\n",
    "    with open(readme) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for l in lines:\n",
    "        if l.startswith('scale'):\n",
    "            scale = float(l.split('= ')[1])\n",
    "        if l.startswith('offset'):\n",
    "            offset = float(l.split('= ')[1])\n",
    "        if l.startswith('orig'):\n",
    "            rescaling_str = l\n",
    "    \n",
    "    # set properties on the collection\n",
    "    cmd = ('earthengine asset set'+\n",
    "       ' -p scale='+str(scale) +\n",
    "       ' -p offset='+str(offset) +\n",
    "       ' -p description=\"' + rescaling_str + '\"' +\n",
    "       ' ' + asset_id)\n",
    "    print(cmd)\n",
    "    stdout = subprocess.check_output(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29fca69-f7ef-485b-9318-4a5d44aa76af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to GEE\n",
    "# Note: Must manually upload all images together to the gedi_v8 cloud storage folder first. No subfolders.\n",
    "\n",
    "\n",
    "# Get files in GCS bucket\n",
    "gsutil = r\"C:\\Users\\stevenf\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\bin\\gsutil.cmd\"\n",
    "stdout = subprocess.check_output(gsutil + \" ls gs://earthengine-228722/gedi_v8\")\n",
    "uris = stdout.decode(\"utf-8\").split(\"\\n\")[1:-1]\n",
    "\n",
    "import json\n",
    "\n",
    "# Upload with manifest\n",
    "for uri in uris:\n",
    "    name = os.path.basename(uri)[:-4]\n",
    "    metric = name.split('_')[0]\n",
    "    indir = os.path.join(basedir, metric, \"as_uint8\")\n",
    "    \n",
    "    # Get scale and offset to write in the image and collection properties\n",
    "    readme = os.path.join(indir, \"readme.txt\")\n",
    "    with open(readme) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for l in lines:\n",
    "        if l.startswith('scale'):\n",
    "            scale = float(l.split('= ')[1])\n",
    "        if l.startswith('offset'):\n",
    "            offset = float(l.split('= ')[1])\n",
    "        if l.startswith('orig'):\n",
    "            rescaling_str = l\n",
    "\n",
    "    # manifest\n",
    "    manifest_path = os.path.join(indir, \"manifest.json\")\n",
    "    manifest = {\n",
    "      \"name\": \"projects/earthengine-legacy/assets/users/stevenf/ecofor/\" + metric + \"_v8_uint8/\"+name,\n",
    "      \"tilesets\": [\n",
    "        {\n",
    "          \"sources\": [\n",
    "            {\n",
    "              \"uris\": [\n",
    "                uri\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"missingData\": {\n",
    "         \"values\": [255]\n",
    "      },\n",
    "      \"startTime\": name.split(\"_\")[1]+\"-01-01T00:00:00Z\",\n",
    "      \"endTime\": name.split(\"_\")[1]+\"-12-31T00:00:00Z\",\n",
    "      \"properties\":{\n",
    "          \"scale\":scale,\n",
    "          \"offset\":offset\n",
    "      }\n",
    "    }\n",
    "\n",
    "    # write manifest to JSON\n",
    "    with open(manifest_path, 'w') as f:\n",
    "        json.dump(manifest, f)\n",
    "    \n",
    "    # upload to earth engine\n",
    "    cmd = \"earthengine upload image --manifest \" + manifest_path\n",
    "    stdout = subprocess.check_output(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceb456d-dbf7-4689-8745-bc37316b136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Old\n",
    "# # Cover as integer for manual upload to GEE\n",
    "# paths = glob(r\"C:\\scratch\\ECOFOR\\gedi\\maps\\v03\\cover\\*.tif\")\n",
    "# # paths = paths[:1]\n",
    "\n",
    "# for path in paths:\n",
    "#     outdir, outname = os.path.split(path)\n",
    "#     outpath = os.path.join(outdir, \"as_int\", outname)\n",
    "#     with rasterio.open(path) as src:\n",
    "#         arr = src.read(1)\n",
    "#         profile = src.profile\n",
    "    \n",
    "#     nodata = np.isnan(arr)\n",
    "#     arr = np.uint8(np.round(arr*100))\n",
    "#     arr[nodata] = 255\n",
    "    \n",
    "#     profile['dtype'] = np.uint8\n",
    "#     profile['nodata'] = 255\n",
    "    \n",
    "#     with rasterio.open(outpath, 'w', **profile) as dst:\n",
    "#         dst.write(arr, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4acf59-6dde-4fb2-85ad-4357b1fd1bcb",
   "metadata": {},
   "source": [
    "## Reproject for DAAC\n",
    "The ORNL DAAC wants the data in UTM 36 S intead of N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d31199-4ea9-4d16-b3d0-a5f82b7091e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob(r\"J:\\projects\\ECOFOR\\deliverables\\ORNL_DAAC_202505\\v08\\lt-p-s-t\\*\\*.tif\")\n",
    "outdir = r\"J:\\projects\\ECOFOR\\deliverables\\ORNL_DAAC_202506\" \n",
    "\n",
    "for path in paths:\n",
    "    outpath = os.path.join(outdir, os.path.basename(path))\n",
    "\n",
    "    cmd = \"gdalwarp -t_srs EPSG:32736 -co COMPRESS=LZW \" + path + \" \" + outpath\n",
    "    result = subprocess.check_output(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce2909f-4442-49a2-adc5-710113bcd46c",
   "metadata": {},
   "source": [
    "## Prep SAE tables  \n",
    "Prepare data necessary for doing small area estimation in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b376149-62a5-4b21-a3e0-bd46962e2351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get population of pixels for all years\n",
    "aois_path = r\"J:\\projects\\ECOFOR\\gedi\\sae\\sae_aois.gpkg\"\n",
    "basedir = r\"D:\\ECOFOR\\gedi\\maps\\v08\\lt-p-s-t\"\n",
    "outpath = r\"J:\\projects\\ECOFOR\\gedi\\sae\\sae_gedi_pop.parquet\"\n",
    "\n",
    "rast_paths = [p for p in glob(os.path.join(basedir, \"*/*.tif\")) if \"change\" not  in p]\n",
    "\n",
    "def get_fdf(fdict):\n",
    "    fprop = fdict['properties']\n",
    "    arr = fprop['mini_raster_array'].ravel()\n",
    "    arr = arr[~arr.mask].data\n",
    "    aoi = fprop['name']\n",
    "    return pd.DataFrame({'val':arr, 'aoi':aoi})\n",
    "\n",
    "def get_aoi_vals(aois_path, rast_path):\n",
    "    zstats = zonal_stats(aois_path, rast_path, stats=\"count\", raster_out=True, geojson_out=True)\n",
    "    fname = os.path.basename(rast_path)[:-4]\n",
    "    metric, year = fname.split('_')\n",
    "    fdfs = [get_fdf(f) for f in zstats]\n",
    "    df = pd.concat(fdfs, axis=0)\n",
    "    df['metric'] = metric\n",
    "    df['year'] = int(year)\n",
    "    return df\n",
    "\n",
    "rast_dfs = Parallel(n_jobs=10)(delayed(get_aoi_vals)(aois_path, rast_path) for rast_path in rast_paths)\n",
    "\n",
    "df = pd.concat(rast_dfs, axis=0)\n",
    "\n",
    "# pivoting this way takes 3ish minutes and lots of ram\n",
    "df['ix'] = df.index\n",
    "dfw = df.pivot(index=['aoi', 'year', 'ix'], columns='metric', values='val').reset_index()\n",
    "dfw['domain'] = dfw['aoi']+'_'+dfw['year'].astype(str)\n",
    "\n",
    "dfw.to_parquet(outpath)\n",
    "del dfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5404c9db-fd1a-48d3-8dd9-38e3cc9925f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GEDI pred/obs samples in domains\n",
    "oob_path = r\"J:\\projects\\ECOFOR\\gedi\\models\\v08\\GEDI_2AB_2019to2023_leafon_sampy500m_all_v08.parquet\"\n",
    "aois_path = r\"J:\\projects\\ECOFOR\\gedi\\sae\\sae_aois.gpkg\"\n",
    "outpath = r\"J:\\projects\\ECOFOR\\gedi\\sae\\sae_gedi_samp_v08EDM.gpkg\"\n",
    "\n",
    "df = gpd.read_parquet(oob_path)\n",
    "aois = gpd.read_file(aois_path)\n",
    "\n",
    "sdf = df.sjoin(aois[['name', 'geometry']], how='inner')\n",
    "sdf = sdf.drop(columns=['index_right'])\n",
    "sdf = sdf.rename(columns={'name':'aoi'})\n",
    "sdf['domain'] = sdf['aoi']+'_'+sdf['rain_year'].astype(str)\n",
    "\n",
    "sdf.to_file(outpath, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a9d6ac-89d2-4a1e-adae-3bc0dc6b5fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all GEDI footprints in domains\n",
    "path = r\"J:\\projects\\ECOFOR\\gedi\\gedi_data\\04_gedi_filtered_data_shp\\GEDI_2AB_2019to2023.parquet\"\n",
    "aois_path = r\"J:\\projects\\ECOFOR\\gedi\\sae\\sae_aois.gpkg\"\n",
    "outpath = r\"J:\\projects\\ECOFOR\\gedi\\sae\\sae_gedi_all.gpkg\"\n",
    "\n",
    "df = gpd.read_parquet(path)\n",
    "aois = gpd.read_file(aois_path)\n",
    "\n",
    "# Filter to points that will be used (leaf-on only)\n",
    "df['delta_time'] = pd.to_datetime(df['delta_time'])\n",
    "df = df[df['rh98']<45] # Remove unreasonable points\n",
    "df = df[(df['delta_time'].dt.day_of_year < 121) | (df['delta_time'].dt.day_of_year > 305)] # keep only leaf-on (Nov - Apr) as defined in Li 2023\n",
    "\n",
    "# Rain year is defined as the year beginning with the start of the dry season (121-273) and the following wet season (274-120)\n",
    "# e.g., rain year 2018 is May 1, 2018 - April 30 2019\n",
    "df['year'] = df['delta_time'].dt.year\n",
    "df['rain_year'] = df['year'].copy()\n",
    "df.loc[df['delta_time'].dt.day_of_year < 121, 'rain_year'] += -1 \n",
    "\n",
    "df = df.to_crs(aois.crs)\n",
    "\n",
    "sdf = df.sjoin(aois[['name', 'geometry']], how='inner')\n",
    "sdf = sdf.drop(columns=['index_right'])\n",
    "sdf = sdf.rename(columns={'name':'aoi'})\n",
    "\n",
    "sdf['domain'] = sdf['aoi']+'_'+sdf['rain_year'].astype(str)\n",
    "\n",
    "sdf.to_file(outpath, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b2672f-c785-4077-9d04-88d544ace883",
   "metadata": {},
   "source": [
    "## Compare SAE to zonal stats\n",
    "Run standard zonal stats as a comparison to the SAE results obtained from R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024add73-a33b-4b96-beec-88eecce60850",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"J:\\projects\\ECOFOR\\gedi\\sae\\sae_gedi_pop.parquet\"\n",
    "df = pd.read_parquet(path)\n",
    "means = df.groupby('domain').agg({'aoi':'first', 'year':'first', 'cover':'mean', 'fhd':'mean', 'pai':'mean', 'rh98':'mean'})\n",
    "\n",
    "path = r\"J:\\projects\\ECOFOR\\gedi\\sae\\sae_gedi_estimates_20250312.csv\"\n",
    "mb = pd.read_csv(path)\n",
    "mb = mb.pivot(index='domain', columns='metric', values='mean')\n",
    "\n",
    "path = r\"J:\\projects\\ECOFOR\\gedi\\sae\\direct_gedi_estimates_20250415.csv\"\n",
    "db = pd.read_csv(path)\n",
    "db = db[db['samptype']=='srs']\n",
    "db = db.pivot(index='domain', columns='metric', values='mean')\n",
    "\n",
    "mdf = pd.concat([means, mb, db], keys=['zonal', 'mb','db'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08974eef-1057-4413-bf32-7cade04bbbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf[mdf[('zonal','aoi')]=='plantation_a']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e3b588-c3cb-4135-b386-b0d01116b9da",
   "metadata": {},
   "source": [
    "## Climate sensitivity  \n",
    "Evaluate the sensitivity of the predicted metrics to climate, and in particular the drought in 2015/2016. This drought was during the rainy season of 2015/16 so October 2015 to April 2016. There was a recovery in the wet season of 2016. https://doi.org/10.2989/10220119.2020.1718755\n",
    "\n",
    "Compare 2015 (2015 dry + 2015/16) to 2016 for areas with no apparent vegetation change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab012b-f2af-468e-aa29-a6b375b9c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter VCA sites for intersection with fires between 2009 and 2017\n",
    "fire_paths = [\n",
    "    'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2009fires.shp',\n",
    "    'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2010fires.shp',\n",
    "    'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2011fires.shp',\n",
    "    'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2012fires.shp',\n",
    "    'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2013fires.shp',\n",
    "    'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2014fires.shp',\n",
    "    'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2014firesUTM.shp',\n",
    "    'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2015firesUTM.shp',\n",
    "    'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2016firesUTM.shp',\n",
    "    'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2017firesUTM.shp'\n",
    "]\n",
    "\n",
    "fire_df = pd.concat([gpd.read_file(p).to_crs(epsg=32736) for p in fire_paths])\n",
    "\n",
    "burned = fire_df.unary_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee11a97-487f-46f1-a0d6-04b792dfc405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep VCA points and convert to boxes for photo-interp\n",
    "path = r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\derived\\vca_sites.gpkg\"\n",
    "df = gpd.read_file(path)\n",
    "df = df.to_crs(epsg=fire_df.crs.to_epsg())\n",
    "\n",
    "# Drop points intersecting burns, or with a big difference in coordiantes or their is no geometry\n",
    "df = df[~df.intersects(burned)]\n",
    "df = df[~(df[\"coords_dif\"]>10) & (~df[\"geometry\"].isnull())]\n",
    "\n",
    "# Shuffle to get better spatial distribution when examining first X in list\n",
    "df = df.sample(frac=1, random_state=42) \n",
    "\n",
    "# Snap to nearest pixel\n",
    "rast_path = r\"J:\\projects\\ECOFOR\\gedi\\maps\\v08\\lt-p-s-t\\cover\\cover_2015.tif\"\n",
    "\n",
    "with rasterio.open(rast_path) as src:\n",
    "    src_crs = src.crs\n",
    "df = df.to_crs(epsg=src_crs.to_epsg())\n",
    "\n",
    "def snap_points(gdf_points, raster_filepath):\n",
    "    with rasterio.open(raster_filepath) as src:\n",
    "        transform = src.transform\n",
    "        res = src.res[0]   \n",
    "    coords = np.array([(p.x, p.y) for p in gdf_points.geometry])\n",
    "    rows, cols = rasterio.transform.rowcol(transform, coords[:, 0], coords[:, 1])\n",
    "    x_snapped, y_snapped = rasterio.transform.xy(transform, rows, cols, offset='center')\n",
    "    snapped_geometries = [shapely.geometry.Point(x, y) for x, y in zip(x_snapped, y_snapped)]\n",
    "    return snapped_geometries\n",
    "\n",
    "df[\"geometry\"] = snap_points(df, rast_path)\n",
    "\n",
    "# df.to_file(r\"J:\\projects\\ECOFOR\\climate_sensitivity\\vca_unburned09to17_snapped.gpkg\", driver=\"GPKG\")\n",
    "# df.to_file(r\"J:\\projects\\ECOFOR\\climate_sensitivity\\vca_unburned09to17_snapped.shp\")\n",
    "\n",
    "# # Create boxes for visualization in Google earth\n",
    "# df[\"geometry\"] = df.buffer(15, cap_style=\"square\")\n",
    "# df.to_file(r\"J:\\projects\\ECOFOR\\climate_sensitivity\\vca_unburned09to17_snapped_box.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47fa3ff-b051-4c4f-b9f7-26afc6ddcfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the version with PI completed to a shapefile for upload to GEE\n",
    "path = r\"J:\\projects\\ECOFOR\\climate_sensitivity\\vca_unburned09to17_snapped_pi.gpkg\"\n",
    "df = gpd.read_file(path)\n",
    "df.to_file(os.path.splitext(path)[0]+\".shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e1e6b0-5b3d-468e-bc2b-000e9da1a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract cover and LandTrendr for points\n",
    "path = r\"J:\\projects\\ECOFOR\\climate_sensitivity\\vca_unburned09to17_snapped_pi.gpkg\"\n",
    "outpath = r\"J:\\projects\\ECOFOR\\climate_sensitivity\\vca_unburned09to17_snapped_pi_extract2.gpkg\"\n",
    "\n",
    "df = gpd.read_file(path)\n",
    "\n",
    "indirs = {\n",
    "    \"ltwet\":r\"J:\\projects\\ECOFOR\\lt\\wet\",\n",
    "    \"ltdry\":r\"J:\\projects\\ECOFOR\\lt\\dry\",\n",
    "    \"cover\":r\"J:\\projects\\ECOFOR\\gedi\\maps\\v08\\lt-p-s-t\\cover\"\n",
    "}\n",
    "\n",
    "layers = {}\n",
    "for key, indir in indirs.items():\n",
    "    ext = \"vrt\" if key.startswith(\"lt\") else \"tif\"\n",
    "    paths = glob(os.path.join(indir, \"*.\"+ext))\n",
    "    for path in paths:\n",
    "        year = path[-8:-4]\n",
    "#         if int(year) < 2007:\n",
    "#             continue\n",
    "        layers[key+\"_\"+year] = path\n",
    "        \n",
    "# Only points with a classification for extraction\n",
    "df = df[~df[\"change\"].isnull()]\n",
    "\n",
    "# Extract layers for each point\n",
    "for name, path in layers.items():\n",
    "    with rasterio.open(path) as src:\n",
    "        bands = src.descriptions\n",
    "        \n",
    "    if len(bands)<2:\n",
    "        bands = [name.split(\"_\")[0]]\n",
    "    \n",
    "    def extract_vals(band, band_name):\n",
    "        vals = list(gen_point_query(df['geometry'], path, band=band+1, interpolate='nearest'))\n",
    "        return pd.Series(vals, index=df.index, name=name+'_'+band_name)\n",
    "\n",
    "    val_series = Parallel(n_jobs=8)(delayed(extract_vals)(band, band_name) for band, band_name in enumerate(bands))\n",
    "    valdf = pd.concat(val_series, axis=1)\n",
    "    df = pd.merge(df, valdf, 'left', left_index=True, right_index=True)\n",
    "\n",
    "df.to_file(outpath, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df67a0e-774c-4fc2-a3c5-7c5d414981d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load earth engine functions\n",
    "sys.path.append(r'J:\\users\\stevenf\\code\\utils\\pee')\n",
    "import ee\n",
    "import landsat as lxtools\n",
    "import time_series\n",
    "\n",
    "ee.Initialize()\n",
    "\n",
    "# Extract original landsat wet season NDVI composites for points (before LandTrendr)\n",
    "fc = ee.FeatureCollection(\"projects/earthengine-legacy/assets/users/stevenf/ecofor/vca_unburned09to17_snapped_pi\")\n",
    "fc = fc.filter(ee.Filter.notEquals(\"change\", \"\"))\n",
    "# fc = ee.FeatureCollection([fc.first()])\n",
    "\n",
    "starty = 1984\n",
    "endy = 2022\n",
    "startdoy, enddoy = 274, 120 # Oct 1st, Apr 30 - non-leap year wet season\n",
    "# startdoy, enddoy = 121, 273 # May 1st, Sept 30 - non-leap year dry season\n",
    "orig_bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']\n",
    "ixbands = ['ndvi', 'nbr', 'ndmi','tcb', 'tcg', 'tcw']\n",
    "coll_kwargs = {'bands':orig_bands, 'rescale':True, 'cloud_cover':50, 'tdom':False}\n",
    "\n",
    "comps = time_series.annual_composites(fc, starty, endy, startdoy, enddoy,\n",
    "                                      lxtools.sr_collection, time_series.medoid,\n",
    "                                      coll_kwargs, fill=False)\n",
    "\n",
    "comps = comps.map(lambda i: (i.addBands(lxtools.specixs(i, ixlist=ixbands))))\n",
    "\n",
    "DEFAULT_PROPERTIES = ee.Dictionary(\n",
    "    {band: -32768 for band in orig_bands + ixbands}\n",
    ")\n",
    "\n",
    "\n",
    "def ensure_schema(feature):\n",
    "    feature_props = feature.toDictionary()\n",
    "    merged_props = feature_props.combine(DEFAULT_PROPERTIES, overwrite=False)\n",
    "    return ee.Feature(feature.geometry(), merged_props)\n",
    "\n",
    "\n",
    "def extract_point_data(image):\n",
    "    sampled_fc = image.reduceRegions(\n",
    "        collection=fc,\n",
    "        reducer=ee.Reducer.first(),\n",
    "        scale=30,\n",
    "        tileScale=16\n",
    "    )\n",
    "    \n",
    "    sampled_fc = sampled_fc.map(ensure_schema)\n",
    "    \n",
    "    return sampled_fc.map(lambda feature: feature.set('year', image.get(\"year\")))\n",
    "\n",
    "fc = comps.map(extract_point_data).flatten()\n",
    "\n",
    "# fc.getInfo()\n",
    "\n",
    "task = ee.batch.Export.table.toDrive(\n",
    "    collection=fc,\n",
    "    description='vca_unburned09to17_snapped_pi_landsat_wet2',\n",
    "    folder='gee',\n",
    "    fileFormat='CSV'\n",
    ")\n",
    "task.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b08027-6af7-4d0c-a5de-5b8f98f89d83",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Veld Condition Assessment\n",
    "Analyze VCA data for height and density of woody vegetation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3bbb51-71de-4e53-8982-52bb004285b8",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Common species\n",
    "Get a list of common species and their abbreviations to help in constucting field guide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68147341-1036-43a2-b7d8-fa041fe4f339",
   "metadata": {},
   "source": [
    "### Woody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc0fda0-a1c3-4e8d-9068-882e9a976e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\Woody VCA data - all years.xlsx\"\n",
    "df = pd.read_excel(path, sheet_name='2005 spp density per circle', header=2)#, index_col=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e2590e-db10-4e85-9fd4-11d310035dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo index\n",
    "df['site'] = df['SITEPNTCIR'].str[:4]\n",
    "df['pnt'] = df['SITEPNTCIR'].str[4:7]\n",
    "df['cir'] = df['SITEPNTCIR'].str[-1]\n",
    "df = df.set_index(['site', 'pnt', 'cir'])\n",
    "df = df.drop(['SITEPNTCIR', 'POINT', 'CIRCLE'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8055b77-0276-43f1-9c1c-b9ba9597c159",
   "metadata": {},
   "source": [
    "#### C circle - trees > 3 m  \n",
    "Working with C separately because erroneous values needed to be fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e266d209-eb0a-4984-93d0-78e3e1a4eab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the C circle (trees > 3 m)\n",
    "dfc = df.query('cir==\"C\"').droplevel(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f8912-2829-4880-b5a8-b09e14e5c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix erroneously high values (would be better to recalculate density correctly later)\n",
    "def fix_high_C(x):\n",
    "    \"\"\"Function to move the decimal place so a value is less than 1.\"\"\"\n",
    "    return x / (10*(10**np.floor(np.log10(np.abs(x)))))\n",
    "\n",
    "fixedhighC = dfc[dfc>1].apply(np.vectorize(fix_high_C))\n",
    "dfc[dfc>=1] = fixedhighC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c6a44-947e-4c87-83d2-8deb1b6eaed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to number of individuals per hectare\n",
    "m2_per_ha = 100*100\n",
    "dfc = dfc*m2_per_ha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d98c4-0102-4493-90d0-47dab74fb5f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# average across all points\n",
    "cdense = dfc.mean().sort_values(ascending=False)\n",
    "cdense.name='density'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f06db2-6c0b-4fee-ae18-82e232fe5651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get acronyms\n",
    "path = r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\Woody VCA data - all years.xlsx\"\n",
    "abv = pd.read_excel(path, sheet_name='2005 original VCA data', header=2)\n",
    "abv = abv[['ABBREV', 'NAME']]\n",
    "abv = abv.drop_duplicates()\n",
    "abv = abv.set_index('ABBREV')['NAME']\n",
    "abv = abv.str.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ed69fd-4e2c-4bac-b105-ae8c510a8db0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show average density and name across all points\n",
    "cdense = pd.merge(cdense, abv, how='left', left_index=True, right_index=True)\n",
    "cdense = cdense.sort_values('density',ascending=False)\n",
    "\n",
    "with pd.option_context(\"display.max_rows\", 1000):\n",
    "    display(cdense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70398ab1-6454-4317-844f-9e6647850e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mean across all points on a site for each circle type\n",
    "dfc_site = dfc.groupby(level=['site']).mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d14c1-a530-43bf-ba14-fd0af39e0953",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "site_count = (dfc_site>0).sum(axis=0)\n",
    "site_count.name = 'count'\n",
    "site_count = pd.merge(site_count, abv, how='left', left_index=True, right_index=True)\n",
    "site_count = site_count.sort_values('count', ascending=False)\n",
    "with pd.option_context(\"display.max_rows\", 1000):\n",
    "    display(site_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd07d2e3-7d2b-4891-ba64-34a68af8d05a",
   "metadata": {},
   "source": [
    "#### B circle - trees 1 - 3 m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85c0a16-1fc6-4a2f-8fba-09f2539bb76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the B circle (trees 1 -3 m)\n",
    "dfb = df.query('cir==\"B\"').droplevel(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f661d-1dfb-4a6d-bf80-96f41c38281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to number of individuals per hectare\n",
    "m2_per_ha = 100*100\n",
    "dfb = dfb*m2_per_ha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0205c060-2896-494d-898c-740f5a070c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# average across all points\n",
    "bdense = dfb.mean().sort_values(ascending=False)\n",
    "bdense.name='density'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a2cae-2ba3-44e4-8298-df4e7092ad69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show average density and name across all points\n",
    "bdense = pd.merge(bdense, abv, how='left', left_index=True, right_index=True)\n",
    "bdense = bdense.sort_values('density',ascending=False)\n",
    "\n",
    "with pd.option_context(\"display.max_rows\", 1000):\n",
    "    display(bdense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef0b41-5e8b-4f58-b624-1f50d7cb9850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mean across all points on a site for each circle type\n",
    "dfb_site = dfb.groupby(level=['site']).mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9561968-cb63-4283-817b-81202c8dff97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "site_count = (dfb_site>0).sum(axis=0)\n",
    "site_count.name = 'count'\n",
    "site_count = pd.merge(site_count, abv, how='left', left_index=True, right_index=True)\n",
    "site_count = site_count.sort_values('count', ascending=False)\n",
    "with pd.option_context(\"display.max_rows\", 1000):\n",
    "    display(site_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955256e8-aa56-48ba-90ec-057ba160ab7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bdense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07535474-f365-4b06-99dc-6f8df52a1e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76081b0f-df1a-4d7f-b823-113030a8990d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0abc9646-9076-4e36-9ddc-6ff72e0b117e",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Location data  \n",
    "Summarize number of sites visited by year which have GPS locations. Many sites are missing GPS points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6866062-dab0-488f-9539-36ddbbf80f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\derived\\VCA_SitesByYear_numSpecies_Draft.shp\"\n",
    "# vca = gpd.read_file(path)\n",
    "\n",
    "# # # summarize species data\n",
    "# # col_mask = vca.columns.str.startswith('F')\n",
    "# # ((vca.loc[:,col_mask]!='ND') & (vca.loc[:,col_mask].notna())).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6677bee-8407-459c-b180-a0d973957190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from excel and recreate geometry\n",
    "path = r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\derived\\VCA clean up.xlsx\"\n",
    "cdf = pd.read_excel(path, sheet_name=\"With Location\", usecols=range(5), names = ['site', 'section', 'description', 'x', 'y'], index_col='site')\n",
    "cdf = gpd.GeoDataFrame(cdf, geometry=gpd.points_from_xy(cdf['x'], cdf['y'], crs='EPSG:32736'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f181f033-63d6-401d-920c-57c939af5aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load site details for comparison\n",
    "sdf = pd.read_excel(r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\VCA Site details 1998-2015.xlsx\",\n",
    "                    usecols=[0,1,2], names = [\"site\", \"lat\", \"lon\"], index_col='site')\n",
    "\n",
    "# Clean up the lat lon data\n",
    "sdf = sdf.dropna()\n",
    "sdf = sdf[~sdf['lon'].astype(str).str.startswith('S')]\n",
    "sdf['lat_orig'], sdf['lon_orig'] = sdf['lat'].copy(), sdf['lon'].copy()\n",
    "\n",
    "# Correct coordinates   (NOTE: ASSUMING THE FIRST SPACE IS A PERIOD LEADS TO COORDS OUTSIDE KRUGER)\n",
    "for col in ['lat', 'lon']:\n",
    "    # remove S and E, and remove leading and trailing whitespace\n",
    "    sdf[col] = sdf[col].astype('str').str.replace('S|E|\\?','', regex=True).str.strip()\n",
    "\n",
    "    # assume first space is a period \n",
    "    sdf[col] = sdf[col].apply(lambda x: x.replace(' ', '.', 1).replace(' ', ''))\n",
    "\n",
    "    # remove leading zeros\n",
    "    sdf[col] = sdf[col].apply(lambda x: x[1:] if x.startswith('0') else x)\n",
    "    \n",
    "    sdf[col] = sdf[col].astype(float)\n",
    "sdf['lat'] = sdf['lat'] * -1\n",
    "\n",
    "sdf = gpd.GeoDataFrame(sdf, geometry=gpd.points_from_xy(sdf['lon'], sdf['lat'], crs='EPSG:4326'))\n",
    "sdf = sdf.to_crs(cdf.crs)\n",
    "sdf = sdf[~sdf.index.duplicated()]\n",
    "\n",
    "sdf.to_file(r\"G:\\temp\\site_details.gpkg\", driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf11f9-b49f-4733-bd64-56817eedaf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge and get distance between coordinates\n",
    "gdf = pd.merge(cdf, sdf, 'outer', on='site', suffixes=('_clean', '_site'))\n",
    "gdf['coords_dif'] = gdf['geometry_clean'].distance(gdf['geometry_site'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe7748-f109-4cd5-b47e-e008b5dadcd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sites with coordinates that are separated by more than X\n",
    "gdf['coords_dif_gt100'] = gdf['coords_dif'] > 100\n",
    "\n",
    "with pd.option_context('display.max_rows', 200):\n",
    "    display(gdf[gdf['coords_dif']>100].sort_values('coords_dif', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e185a98-0b12-4c84-b6a0-9933fbc2d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up for export\n",
    "gdf['site_lat'], gdf['site_lon'] = gdf['lat'], gdf['lon']\n",
    "gdf['clean_wgs84'] = gdf['geometry_clean'].to_crs(epsg=4326)\n",
    "\n",
    "gdf['clean_lat'], gdf['clean_lon'] = gdf['clean_wgs84'].y, gdf['clean_wgs84'].x\n",
    "\n",
    "gdf.geometry = gdf['geometry_clean'].copy()\n",
    "gdf = gdf.set_crs(gdf['geometry_clean'].crs)\n",
    "\n",
    "gdf = gdf[['section', 'description', 'clean_lat', 'clean_lon', 'site_lat', 'site_lon', 'coords_dif', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58088dcb-75d9-43b4-b599-479789b3d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # project to 36 N to match predictor layers\n",
    "# gdf = gdf.to_crs(epsg=32636)\n",
    "\n",
    "# gdf.reset_index().to_file(r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\derived\\vca_sites.gpkg\", driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aeaf53-0de7-4351-abf6-45768aeff359",
   "metadata": {},
   "source": [
    "### Point shapfiles\n",
    "Check the point shapefiles for 2008 and 2009 sent by Chenay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a31b0f-588a-4bce-b1ff-af131a3ec5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\revcadataversions\\veld_condition_assessment_sites_2008.shp\"\n",
    "df = gpd.read_file(path)\n",
    "\n",
    "# Coordinates in attribute table match those in the geometry\n",
    "df['X_COORD_dif'] = df['X_COORD'] - df.geometry.x\n",
    "df['Y_COORD_dif'] = df['Y_COORD'] - df.geometry.y\n",
    "df['POINT_X_dif'] = df['POINT_X'] - df.geometry.x\n",
    "df['POINT_Y_dif'] = df['POINT_Y'] - df.geometry.y\n",
    "\n",
    "df8 = df.copy()\n",
    "df.describe().loc[['min', 'max'],] # look at min and max for the _dif columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c180c236-c4a2-4d2a-8816-efdcbe0069db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2009\n",
    "path = r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\revcadataversions\\veld_condition_assessment_sites_2009.shp\"\n",
    "df = gpd.read_file(path)\n",
    "\n",
    "# Coordinates in attribute table match those in the geometry\n",
    "df['X_Coord_dif'] = df['X_Coord'] - df.geometry.x\n",
    "df['Y_Coord_dif'] = df['Y_Coord'] - df.geometry.y\n",
    "\n",
    "df9 = df.copy()\n",
    "df.describe().loc[['min', 'max'],] # look at min and max for the _dif columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560bc1e-49f6-4bba-9388-7ac938747563",
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = df9.rename(columns={'SITENO':'SITE_NO'})\n",
    "df9 = df9.to_crs(df8.crs)\n",
    "df = pd.merge(df8, df9[['SITE_NO', 'geometry']], 'outer', on='SITE_NO', suffixes=['_8', '_9'])\n",
    "df['geo_dif'] = df['geometry_8'].distance(df['geometry_9'])\n",
    "df['site_int'] = pd.to_numeric(df['SITE_NO'])\n",
    "\n",
    "df[['SITE_NO', 'geo_dif', 'geometry_8', 'geometry_9']].sort_values('geo_dif', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9bc983-b187-426d-ab09-ce72cd7bbfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check list of sites in 2008 woody against 08/09 points\n",
    "path = r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\Woody VCA data - all years.xlsx\"\n",
    "year = \"2008\"\n",
    "\n",
    "ydict = {\"2002\":{\"header\":5, \"sitecol\":\"SITE_NO\"},\n",
    "         \"2005\":{\"header\":2, \"sitecol\":\"SITE_NO\"},\n",
    "         \"2008\":{\"header\":2, \"sitecol\":\"SITE NUMBER\"}}\n",
    "yvals = ydict[year]\n",
    "dfw = pd.read_excel(path, sheet_name=year+\" original VCA data\", header=yvals[\"header\"])\n",
    "\n",
    "# dfw = gpd.GeoDataFrame(dfw, geometry=gpd.points_from_xy(dfw['Longitude'], dfw['Latitude'], crs=4326))\n",
    "# dfw = dfw.to_crs(df.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0542cf2-1169-4b4b-8ba6-82ebca392627",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compare lists of unique site numbers to find sites with woody data but no point coordinates\n",
    "site_nums = dfw[yvals[\"sitecol\"]].unique()\n",
    "site_nums.sort()\n",
    "\n",
    "mask = ~np.isin(site_nums, df['site_int'].unique())\n",
    "missing = site_nums[mask]\n",
    "\n",
    "display(dfw[dfw[yvals['sitecol']].isin(missing)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc6aac-8de9-4b5e-8bdf-0a620b5bea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfw[yvals['sitecol']].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308d37dc-a60c-43e1-ab29-862203031fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create point file of plots with woody data in each year\n",
    "# (just doing 2008 for now)\n",
    "gdf = df[['SITE_NO', 'site_int', 'geometry_9']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c6c91-188f-4cf6-bd64-2bdf50829213",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.rename(columns={'geometry_9':'geometry'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4743b4bb-f6e6-47b7-a746-f593cf148e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = pd.merge(gdf, dfw[['.drop_duplicates(yvals['sitecol']), how='left', left_on='site_int', right_on=yvals['sitecol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef61508-c07d-4547-ba05-dd2bc5c268c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdfw = gdf[gdf['site_int'].isin(dfw[yvals['sitecol']].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f11af9-69f4-45cc-a884-b6e68b4aa9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdfw = gpd.GeoDataFrame(gdfw, geometry='geometry', crs=gdfw.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65379c82-3f40-4c59-9e4b-ec6cc81f44c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdfw.to_file(r\"G:\\temp\\vca_woody2008_withgeo.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4588995-738a-4e35-a07a-f6630e027f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdfw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12204867-e53f-4bb7-a7a4-5303102d1aea",
   "metadata": {},
   "source": [
    "### Site polygons\n",
    "Create site polygons based on GPS data and location description & protocol. Maybe orient or match site based on visible trees and woody data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c29e79-ff06-4d5c-89f2-2a24accc3ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0e420bf-b8fa-4cf3-b609-e69857592bce",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Woody basal area\n",
    "Calculate live woody basal area using trees measured in circles B & C along with their basal diameters and total stems. Use the B & C expansion factors to convert this to a per area quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99371a9-0e25-4ea0-af2b-264426ba592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\Woody VCA data - all years.xlsx\"\n",
    "dfw = pd.read_excel(path, sheet_name='2005 original VCA data', header=2)\n",
    "\n",
    "dfw.columns = dfw.columns.str.lower()\n",
    "dfw = dfw.rename(columns={'site_no':'site'})\n",
    "dfw = dfw.sort_values('sitepntcir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d2784-e2a9-4ad8-b8a6-bb62537f535b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Circle A usually doesn't have basal diameter measured, so it's excluded\n",
    "dfa = dfw[dfw['circle']=='A'].copy()\n",
    "dfa['basal_dia'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d946a0-2f6e-42d7-8b5f-b656fff2498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfw = dfw[dfw['circle']!='A'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5452f20-c9b6-480f-926a-b3cbfdebb7d6",
   "metadata": {},
   "source": [
    "### QC\n",
    "Just a quick QC to note some of the logical inconsistencies in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3deee1-49ee-4003-baa3-77a381962312",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Some sites don't have a record for every point\n",
    "point_cnt = dfw.drop_duplicates(['site', 'point_no']).groupby('site')['point_no'].size()\n",
    "point_cnt[point_cnt<8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2140ed-77d8-45af-82d6-fdfc5cae0223",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((dfw['sitepntcir'].str[-1]!=dfw['circle']).sum(), \"record with the circle listed in SITEPNTCIR doesn't match CIRCLE\")\n",
    "print((dfw['sitepntcir'].str[-3:-1]!=dfw['point_no'].astype(str)).sum(), \"records with the point_no listed in SITEPNTCIR not matching POINT_NO\")\n",
    "print((dfw['sitepntcir'].str[:4]!=dfw['site'].astype(str).str.zfill(4)).sum(), \"records with the site listed in SITEPNTCIR not matching site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cbdcad-1c1c-4a95-96ee-60c7c10c65ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate sitenptcir\n",
    "# Note: it appears the sitepntcir is correct in some cases because using circle groups a NIL with other species\n",
    "dfw['sitepntcir2'] = dfw['site'].astype(str).str.zfill(4) + \" \" + dfw['point_no'].astype(str) + dfw['circle']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dccc69c-778e-489e-b526-300270a0a0e5",
   "metadata": {},
   "source": [
    "In cases where the circle in sitepntcir differs from circle, it is usually that the sitepntcir is 'C' but the tree is <3 so listed as 'B' in circle. Meaning B is correct with regard to height, but was the tree actually measured in circle B or C? This is necessary for determining which plot expansion factor to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cb66d9-541d-46a9-a9b6-27ae29e70a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max height when sitepntcir is C and circle is B\n",
    "mask = (dfw['sitepntcir'].str[-1]=='C') & (dfw['circle']=='B')\n",
    "sub = dfw.loc[mask, ['sitepntcir', 'circle', 'name', 'a_ttl_hgt']]\n",
    "print(sub['a_ttl_hgt'].max(), 'm max height when sitepntcir says C and circle says B')\n",
    "\n",
    "# Likewise, where B in sitepntcir and C in circle, the height is above 3\n",
    "mask = (dfw['sitepntcir'].str[-1]=='B') & (dfw['circle']=='C') & (dfw['a_ttl_hgt']!=0)\n",
    "sub = dfw.loc[mask, ['sitepntcir', 'circle', 'name', 'a_ttl_hgt']]\n",
    "print(sub['a_ttl_hgt'].min(), 'm min height when sitepntcir says B and circle says C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e99650-7e0d-4150-8f4b-fcc27949131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to find out which is correct with respect to actual circle measured for the sake of expansion factors - sitepntcir or circle\n",
    "mask = (dfw['sitepntcir'] != dfw['sitepntcir2']) & (dfw['name']=='NIL')\n",
    "dfw[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4944f18d-9f7b-4d86-8a9a-089823022810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some rows without a listed species have a valid diameter\n",
    "# In some cases this looks like the diameter was written in the wrong row\n",
    "mask = (dfw['basal_dia']>0) & (dfw['name']=='NIL')\n",
    "dfw.loc[mask, ['year', 'sitepntcir', 'site', 'point_no', 'circle', 'basal_dia']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c41001b-2fc5-4dc8-8587-7502ec0e51a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More often a named tree doesn't have a diameter\n",
    "mask = (dfw['basal_dia']==0) & (dfw['name']!='NIL')\n",
    "dfw.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0c047c-cb87-4823-8d5a-246527969e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem pattern may be null and total stems may be 0, but there is a diameter. These are likely single stem\n",
    "dfw.loc[(dfw['stem_ptn'].isnull() & dfw['basal_dia'].notna()), ['name', 'stem_ptn', 'ttl_stems', 'basal_dia']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef185e5-d6ab-4334-bc7e-57a23729eac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stem pattern may be Multi, but total stems is 0 or null\n",
    "dfw.loc[(dfw['stem_ptn']=='M') & (dfw['ttl_stems'].isnull() | (dfw['ttl_stems']==0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9938c-22b8-45df-b475-5199f8155a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basal diameter > 0 but no stems listed\n",
    "mask = ((dfw['ttl_stems']==0) | (dfw['ttl_stems'].isnull())) & (~dfw['stem_ptn'].isin(['S', 'L', 'T'])) & (dfw['basal_dia']>0)\n",
    "dfw.loc[mask, 'stem_ptn'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0b520-72bf-4a35-b6e3-eb66b15826d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Total stems greater than 0 but no basal diameter\n",
    "mask = (dfw['ttl_stems']>0) & ((dfw['basal_dia']==0) | (dfw['basal_dia'].isna()))\n",
    "dfw.loc[mask, ['year', 'sitepntcir', 'stem_ptn', 'ttl_stems', 'basal_dia']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f9496-2f56-4ea7-bbc1-b9034aef85cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ridiculous # of stems and/or basal diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2c5bee-f92b-4718-bb77-4148d8e2e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basal diameters are absurdly large (e.g. comapi 3 m wide)\n",
    "dfw.groupby('name')['basal_dia'].max().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bef333b-6d52-41df-8337-9940226d2f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfw.loc[dfw['name'] == 'COMBRETUM APICULATUM', 'basal_dia'].hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf8640-15e8-43f8-9ebb-10271769b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some stem counts are absurdly high (e.g. 81 stems for single colmop)\n",
    "dfw.groupby('name')['ttl_stems'].max().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef3851-676c-4e15-ba07-77ba3f71d888",
   "metadata": {},
   "source": [
    "### Prep & Calc\n",
    "\n",
    "Make some assumptions in correcting data for now since fixing all the errors would take too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16291460-f8f4-4391-a641-98e0b696d542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all incomplete sites for now (some of these are site typos)\n",
    "point_cnt = dfw.drop_duplicates(['site', 'point_no']).groupby('site')['point_no'].size()\n",
    "incomplete_sites = point_cnt[point_cnt<8].index.values\n",
    "dfw = dfw[~dfw['site'].isin(incomplete_sites)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f98c5de-8f7b-474e-a74d-c6606bd17291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix hgt though not used\n",
    "dfw['a_ttl_hgt'] = dfw['a_ttl_hgt'].replace({-99.9:np.nan, -9:np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1593cc66-a2c1-4e18-9921-a6136a6c4540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume NIL is wrong for now(likely basal_dia or name in wrong row)\n",
    "mask = (dfw['name']=='NIL') & (dfw['basal_dia']!=0)\n",
    "dfw.loc[mask, 'name'] = 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a02cf4-0df7-4bd7-87a1-41b058e5d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dead\n",
    "# Age has +,-,=,_ which are not in the manual\n",
    "dfw['dead'] = False\n",
    "mask = (dfw['imp_ele'].isin(['A', 'B', 'C'])) | (dfw['imp_age']=='D') | (dfw['fire_type'].isin(['A', 'B']))\n",
    "dfw.loc[mask, 'dead'] = True                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e05fd-6cbe-44fa-a5c5-c3dc6d784875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume empty total # of stems is actually 1 when basal_dia is filled; regardless of stem_ptn\n",
    "mask = ((dfw['ttl_stems'].isnull()) | (dfw['ttl_stems']==0)) & (dfw['basal_dia']>0)\n",
    "dfw.loc[mask, 'ttl_stems'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e06fde-2b43-476e-b85e-090af66b964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basal area per tree in m2\n",
    "dfw['ba_tree'] = dfw['ttl_stems'] * (np.pi * (dfw['basal_dia']/2)**2) / 100**2  # converting cm2 to m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff079e-b896-43e8-94c0-4d4b58da2b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum basal area for all B and C circles on the site\n",
    "ba_circle = dfw.groupby(['site', 'circle'])['ba_tree'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce4ca3d-2ef7-4a03-93f9-d4b0f658c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot and calculate site basal areas\n",
    "ba = ba_circle.pivot(index='site', columns='circle', values='ba_tree')\n",
    "ba.columns = ['ba_Bm2', 'ba_Cm2']\n",
    "\n",
    "# Get area of site circles in hectares\n",
    "b_circle_m2 = np.pi * 2**2 # 2 m radius\n",
    "b_site_ha = b_circle_m2 * 8 / (100**2)\n",
    "c_circle_m2 = np.pi * 5**2 # 5 m radius\n",
    "c_site_ha = c_circle_m2 * 8 / (100**2)\n",
    "\n",
    "# Apply expansion factor to get m2 / ha\n",
    "ba['ba_B'] = ba['ba_Bm2'] / b_site_ha\n",
    "ba['ba_C'] = ba['ba_Cm2'] / c_site_ha\n",
    "ba['ba'] = ba['ba_B'] + ba['ba_C']\n",
    "ba['ba_pct'] = ba['ba'] / 100**2 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a891f48-fba1-4341-9ac3-866e87c6de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get rid of sites with basal area > measured area\n",
    "# ba = ba[ba['ba_pct']<100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928af474-cb96-46fc-b3b1-a5412258b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hist\n",
    "ba['ba_pct'].hist(bins=20, figsize=(3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b924032e-f530-4a42-9d61-2bee84be4652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Much of the data has a basal area less than 0.4%\n",
    "mask = ba['ba_pct'] <= .5\n",
    "ba.loc[mask, 'ba_pct'].hist(bins=30, figsize=(3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542f440f-c5a9-4d35-9763-e8b7f8d14512",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ba['ba_pct'] >= 0.5\n",
    "ba.loc[mask, 'ba_pct'].hist(bins=20, figsize=(3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c6af9-fcde-4045-aece-22e720763eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basal area is over 80 m2/ha (typical of rainforest) and into the 1000's (absurd)\n",
    "fig, ax = plt.subplots(figsize=(3, 2))\n",
    "ba.loc[ba['ba']<=60, 'ba'].hist(bins=30, ax=ax)#, xlims=(0,60))\n",
    "# ax.set(xlim=(0,60))\n",
    "ax.vlines([5, 20, 60], 0, 70, colors='k', linestyles='--')\n",
    "ax.set(ylabel='count', xlabel='basal area m2/ha')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f02b74-9006-4e36-b9d3-85bf07021062",
   "metadata": {},
   "source": [
    "## Herbaceous  \n",
    "Load herbaceous standing crop. Check data for anomalies. Output clean version.  \n",
    "Most data is only disc height and standing crop per site. Only 2015 has individual measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9319afb1-6f14-4672-9f21-7d98cd15d6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\VCA data 1998-2015.xlsx\"\n",
    "dfh1 = pd.read_excel(path, sheet_name='1989-2006 Standing crop',\n",
    "                  names = ['year', 'site', 'herb_disc', 'herb_agb'])\n",
    "\n",
    "dfh1.loc[dfh1['year']<2000, 'year'] += 1900 # Fix year\n",
    "\n",
    "# add date and assume its end of wet season\n",
    "dfh1['date'] = dfh1['year'].apply(lambda y: pd.to_datetime(str(y)+'-4-30'))\n",
    "\n",
    "path = r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\VCA data 1998-2015.xlsx\"\n",
    "dfh2 = pd.read_excel(path, sheet_name='2007-2012 Stanidng Crop', usecols=list(range(1,6)),\n",
    "                  names = ['date', 'section', 'site', 'herb_disc', 'herb_agb'], \n",
    "                  parse_dates=[0])\n",
    "\n",
    "# Filter out bad site names\n",
    "dfh2 = dfh2[dfh2['site'].apply(lambda x: type(x) is int)]\n",
    "\n",
    "# Drop missing AGB\n",
    "dfh2 = dfh2[dfh2['herb_agb']>=0]\n",
    "\n",
    "# drop missing dates\n",
    "dfh2 = dfh2[dfh2['date'].notna()]\n",
    "\n",
    "# replace 2071 with 2010\n",
    "dfh2.loc[dfh2['date']=='2071-05-28', 'date'] = pd.to_datetime('2010-05-28')\n",
    "\n",
    "# get year\n",
    "dfh2['year'] = dfh2['date'].dt.year\n",
    "\n",
    "# Merge\n",
    "dfh = pd.concat([dfh1, dfh2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a4ec5-8aeb-49e5-a229-57a3f9f46e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that standing crop is calculated based on latest allometrics, and fix errors.\n",
    "sns.scatterplot('herb_disc', 'herb_agb', hue='year', data=dfh1.loc[dfh['year']==2005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f34981-6362-472c-adc9-5da03cb47c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3,2))\n",
    "sns.histplot(x='herb_agb', data=dfh1[dfh1['year']==2005], ax=ax)\n",
    "ax.set(xlabel=\"Herb AGB (kg/ha)\")\n",
    "ax.vlines([2500, 5000], 0, 80, colors='k', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8301ea89-0a40-40e3-9d1e-fe2ebcd821e4",
   "metadata": {},
   "source": [
    "**TODO: Compare herbaceous aerial cover and biomass**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd95587-8952-4c97-a11c-1ff94f034ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "743b2ee4-409e-4a3d-9d2b-8b0e7338d56d",
   "metadata": {},
   "source": [
    "## Burn\n",
    "It looks like 'burn effectiveness' is partially encoded in 'burnt' and 'burn_type' through a mix of codes. Some of these can be decifered with safe assumptions, but others are an educated guess or just missing.  \n",
    "\n",
    "There is no burn table for 2007-2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c88d53-ed45-4d2c-963c-95797975c0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\VCA data 1998-2015.xlsx\"\n",
    "dfb = pd.read_excel(path, sheet_name='1989-2006 Burn data', \n",
    "                    names = ['year', 'site', 'burnt', 'burn_type'])\n",
    "\n",
    "dfb.loc[dfb['year']<2000, 'year'] += 1900  # Fix year\n",
    "dfb = dfb.dropna(subset=['year', 'site'])  # drop nulls\n",
    "dfb['year'] = dfb['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256b282-30ef-4cb2-af9f-9a7edd4140e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new burn effectiveness column by making some\n",
    "# assumptions about the mixed codes\n",
    "burn_dict = {0:'unburnt',\n",
    "             -9:'unburnt',\n",
    "             '-':'unburnt',\n",
    "             '?':'unburnt',  # this may be a bad assumption\n",
    "             'No':'unburnt',\n",
    "             'N':'unburnt',\n",
    "             'Z':'unburnt',\n",
    "             np.nan:'unburnt',\n",
    "             'Y':'burnt',   # 'burnt' means no effectiveness code\n",
    "             1:'burnt',\n",
    "             'A':'very_poor',\n",
    "             'B':'poor',\n",
    "             'P':'poor',\n",
    "             'Poor':'poor',\n",
    "             'C':'moderate',\n",
    "             'Moderate':'moderate',\n",
    "             'M':'moderate', # this should be a safe assumption\n",
    "             'D':'clean',       \n",
    "             'Clean':'clean'\n",
    "            }\n",
    "\n",
    "dfb = dfb.replace({'burnt':burn_dict, 'burn_type':burn_dict})\n",
    "dfb['burn_eff'] = dfb['burn_type'].replace({'none':None})\n",
    "dfb['burn_eff'] = dfb['burn_eff'].fillna(dfb['burnt'])\n",
    "\n",
    "dfb[['burnt', 'burn_type', 'burn_eff']].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268d894a-eb8f-429f-ac4d-7d32754b1899",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb['year'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0345a166-a0f8-4f70-8421-49b91f89c209",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## 2005 VCA structure modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd26429-793e-4b6b-8f89-e6854bfeee10",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Combine 2005 for export\n",
    "Run the location, basal area, herb, and burn sections above and then combine 2005 data from those dataframes to output a 2005 geodataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4931e-7b4b-471a-92ea-812a8543640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb05 = dfb[dfb['year']==2005].set_index('site')[['burn_eff']]\n",
    "dfh05 = dfh[dfh['year']==2005].set_index('site')[['herb_disc', 'herb_agb']]\n",
    "df = pd.concat([gdf, ba, dfh05, dfb05], axis=1)\n",
    "\n",
    "# project to 36 N to match predictor layers\n",
    "df = df.to_crs(epsg=32636)\n",
    "\n",
    "df.reset_index().to_file(r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\derived\\vca_2005_merged.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6742f-2757-4ec6-ba2c-2023027e7857",
   "metadata": {},
   "source": [
    "### Extract predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a02eb4f-3bc9-4b7f-a18f-37c7c0f91851",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\derived\\vca_2005_merged.gpkg\"\n",
    "df = gpd.read_file(path)\n",
    "df = df.dropna(subset='geometry')\n",
    "\n",
    "# predictor layers to extract from as prefix:path\n",
    "layers = {\n",
    "          \"ltdry\":r\"K:\\ECOFOR\\lt\\dry\\lt_dry_2005.vrt\",\n",
    "          \"ltwet\":r\"K:\\ECOFOR\\lt\\wet\\lt_wet_2005.vrt\",\n",
    "          \"palsar\":r\"K:\\ECOFOR\\palsar\\palsar_2007.vrt\"\n",
    "          \"ccdc\":r\"C:\\scratch\\ECOFOR\\ccdc\\20050430\\ccdc_coefs_segpre20050430_greaterkruger.vrt\"\n",
    "         }\n",
    "\n",
    "for name, path in layers.items():\n",
    "    with rasterio.open(path) as src:\n",
    "        bands = src.descriptions\n",
    "        crs = src.crs\n",
    "    \n",
    "    if df.crs!=crs:\n",
    "        print(\"CRS mismatch. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    def extract_vals(band, band_name):\n",
    "        vals = list(gen_point_query(df['geometry'], path, band=band+1, interpolate='nearest'))\n",
    "        return pd.Series(vals, index=df.index, name=name+'_'+band_name)\n",
    "\n",
    "    val_series = Parallel(n_jobs=4)(delayed(extract_vals)(band, band_name) for band, band_name in enumerate(bands))\n",
    "    valdf = pd.concat(val_series, axis=1)\n",
    "    df = pd.merge(df, valdf, 'left', left_index=True, right_index=True)\n",
    "\n",
    "# df.to_file(r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\derived\\vca_2005_merged_rs.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec81d3a8-0041-46fc-9d07-8dc2051570d8",
   "metadata": {},
   "source": [
    "### Load existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eba11e-f3b4-46cc-8e93-82e09147ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\derived\\vca_2005_merged_rs.gpkg\"\n",
    "df = gpd.read_file(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c6c47-6053-40f4-ab91-f71d20a8a1e8",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d93d9d-ab17-464e-b462-2ad8fd102069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out unrealistic basal area. Savanna is 0-20 m2/ha, PNW rainforest reaches like 80 m2/ha max.\n",
    "# This removes like 200 plots\n",
    "df = df[df['ba'] < 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710daced-b7d8-4577-9fdd-f2585bbb329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null herbaceous\n",
    "df = df.dropna(subset='herb_agb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a821afae-8bf7-4083-b0fb-201e01d72334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove plots with a large discrepancy in coordinates between the site details and vca data sheets\n",
    "# This removes like 23 plots\n",
    "df = df[(df['coords_dif'].isna()) | (df['coords_dif'] < 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fd6d06-ca36-4b7c-96e8-6e5ecf8c9c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove plots with B/C discrepancy or containing unrealistic stem counts or tree basal diameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90490e9b-90dc-4149-b768-cfd0e6892579",
   "metadata": {},
   "source": [
    "### Create classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21186c46-c810-45cb-acdd-8c31d25e1b57",
   "metadata": {},
   "source": [
    "Basal area and herbaceous biomass appear to not be related and there is no obvious clustering; even when log transforming either or both variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883c42ce-b7ed-46c9-bf28-3470bf3f8772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine histogram; maybe better to decide based on understanding of ecology there though\n",
    "g = sns.jointplot(x='herb_agb', y='ba', data=df, height=3, kind='hex')\n",
    "g.ax_joint.set(xlabel='Herb AGB (kg/ha)', ylabel='BA (Mg/ha)')\n",
    "g.ax_joint.vlines([2500, 5000], 0, 60, colors='k', linestyle='--')\n",
    "g.ax_joint.hlines([5, 20], 0, 8400, colors='k', linestyles='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a60e1-f89f-4a7f-934b-6acdfb4efb3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create classes\n",
    "df['ba_class'] = pd.cut(df['ba'], [0,5,20,60], labels=['Wlo', 'Wmed', 'Whi'], include_lowest=True).astype(str)\n",
    "df['herb_class'] = pd.cut(df['herb_agb'], [0,2500, 5000, 9000], labels = ['Hlo', 'Hmed', 'Hhi'], include_lowest=True).astype(str)\n",
    "\n",
    "df['mixed_class'] = df['ba_class'] + '_' + df['herb_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e752d09-0491-405b-84e8-30a9da70ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode to ints for mapping\n",
    "code_dict = {\n",
    "    'Wlo_Hlo':1,\n",
    "    'Wlo_Hmed':2,\n",
    "    'Wlo_Hhi':3,\n",
    "    'Wmed_Hlo':4,\n",
    "    'Wmed_Hmed':5,\n",
    "    'Wmed_Hhi':6,\n",
    "    'Whi_Hlo':7,\n",
    "    'Whi_Hmed':8,\n",
    "    'Whi_Hhi':9\n",
    "}\n",
    "df['mixed_code'] = df['mixed_class'].replace(code_dict).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da298c8-a8f4-4665-a7a8-affd2aed068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop LT from columns to align with rasters used for prediction\n",
    "df.columns = df.columns.str.replace(\"^lt\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033df232-b400-4fd7-8860-7f2c2a330315",
   "metadata": {},
   "source": [
    "**TODO: Compare herbaceous aerial cover and biomass**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf2a5fd-ae44-4af3-8823-176fc6a470d0",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a3e1c8-8f03-48b5-80b2-a8e6bea6192c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mixed class model\n",
    "Xcols = df.columns[df.columns.str.startswith('dry') | df.columns.str.startswith('wet')].tolist() #, 'ccdc', , 'palsar'\n",
    "ycol = 'mixed_code'\n",
    "mdf = df[~df[Xcols+[ycol]].isnull().any(axis=1)] # drop nulls (should be none)\n",
    "X, y = mdf[Xcols], mdf[ycol]\n",
    "\n",
    "# Run model and show results\n",
    "rf = RandomForestClassifier(n_estimators=500, max_features='sqrt', oob_score=True, random_state=0, n_jobs=1)\n",
    "rf = rf.fit(X, y)\n",
    "\n",
    "# get OOB predictions and importance values\n",
    "ypred = pd.Series(rf.classes_[rf.oob_decision_function_.argmax(axis=1)], index=y.index)\n",
    "imps = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "\n",
    "# confusion matrix of OOB predictions for all samples\n",
    "fig = pretty_matrix(y, ypred, normalize=False, outline_diag=True);\n",
    "display(fig)\n",
    "print(classification_report(y, ypred))\n",
    "print(imps.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b24b79-d10b-4251-8a0a-c9f440537009",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"C:\\scratch\\ECOFOR\\habitat\\vca2005_mixed_v1.joblib\"\n",
    "joblib.dump(rf, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b183693-d123-41c1-a615-afc0bb031c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Herb AGB regression \n",
    "Xcols = df.columns[df.columns.str.startswith(('lt', 'palsar'))].tolist() #'ccdc'\n",
    "ycol = 'herb_agb'\n",
    "mdf = df[~df[Xcols+[ycol]].isnull().any(axis=1)] # drop nulls (should be none)\n",
    "X, y = mdf[Xcols], mdf[ycol]\n",
    "rf = RandomForestRegressor(n_estimators=200, max_features='sqrt', oob_score=True, random_state=0, n_jobs=4)\n",
    "rf = rf.fit(X, y)\n",
    "\n",
    "ypred = pd.Series(rf.oob_prediction_, index=y.index)\n",
    "imps = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "fig = obs_pred_hexbin(y, ypred, vmax=10);\n",
    "fig.suptitle('Herb AGB (kg/ha)')\n",
    "print(\"N=\", len(y))\n",
    "print(imps.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf27901-7721-4371-9ba6-e7ea1549ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basal area regression \n",
    "Xcols = df.columns[df.columns.str.startswith(('lt', 'palsar', 'ccdc'))].tolist()\n",
    "ycol = 'ba'\n",
    "mdf = df[~df[Xcols+[ycol]].isnull().any(axis=1)] # drop nulls (should be none)\n",
    "X, y = mdf[Xcols], mdf[ycol]\n",
    "rf = RandomForestRegressor(n_estimators=200, max_features='sqrt', oob_score=True, random_state=0, n_jobs=4)\n",
    "rf = rf.fit(X, y)\n",
    "\n",
    "ypred = pd.Series(rf.oob_prediction_, index=y.index)\n",
    "imps = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "fig = obs_pred_hexbin(y, ypred, vmax=10);\n",
    "fig.suptitle('Basal Area (Mg/ha)')\n",
    "print(\"N=\", len(y))\n",
    "print(imps.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59f1fd9-6849-4ba0-9fce-a6b2cf492374",
   "metadata": {},
   "source": [
    "### Post-classification Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46710826-176d-4f84-8aa6-5def9024efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"J:\\projects\\ECOFOR\\veg_type\\mixed_2005.tif\"\n",
    "outpath = path[:-4]+\"_sieve5.tif\" \n",
    "sieve_path = r\"C:\\OSGeo4W\\apps\\Python39\\Scripts\\gdal_sieve.py\"\n",
    "cmd = \"python \" + sieve_path + \" -st 5 -8 \" + path + \" \" + outpath[:-4]+\"_temp.tif\"\n",
    "stdout = subprocess.check_output(cmd)\n",
    "\n",
    "# Can't seem to use creation options in gdal_sieve so compressing the gdal_translate\n",
    "cmd  = \"gdal_translate -co COMPRESS=LZW -co TILED=YES -co PREDICTOR=2 \" + outpath[:-4]+\"_temp.tif\" + \" \" + outpath\n",
    "stdout = subprocess.check_output(cmd)\n",
    "os.remove(outpath[:-4]+\"_temp.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40abc8a4-34c6-4160-b4d4-f01513ec0f42",
   "metadata": {},
   "source": [
    "## Herbaceous biomass modeling\n",
    "First run herbaceous section above to get datasheets assembled.  \n",
    "Then extract predictor values and run models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dd3940-088f-4ad1-b150-c49ffc5a877c",
   "metadata": {},
   "source": [
    "### Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755bf66c-9404-4bd3-b09f-5c4632b125f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = gpd.read_file(r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\derived\\vca_sites.gpkg\")\n",
    "df = pd.merge(dfh, dfg[['site', 'coords_dif', 'geometry']], 'left', on='site')\n",
    "\n",
    "df['has_geo'] = df['geometry'].notna()\n",
    "# df.groupby(['year', 'has_geo'], dropna=False).size()\n",
    "\n",
    "# Site numbers may not have been standardized till 2000\n",
    "df = df[df['year']>=2000]\n",
    "\n",
    "# drop rows without geometry\n",
    "df = df[df['has_geo']]\n",
    "\n",
    "# drop null agb\n",
    "df = df[df['herb_agb']>0]\n",
    "\n",
    "# Get geo back\n",
    "df = gpd.GeoDataFrame(df, geometry='geometry', crs=dfg.crs)\n",
    "df['x'], df['y'] = df.geometry.x, df.geometry.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f480d0f-415c-4836-b45c-bf47ff564ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Dask and process images by row   \n",
    "def get_ls_data(r, year_col, krads=[0,0], ls_dir=\"\", basename=\"\", bands=[]):\n",
    "    \"\"\"\n",
    "    r: row in a dataframe\n",
    "    krads: list of kernel radii (rows, cols) for extraction. [0,0] is point extraction of single pixel.\n",
    "    \"\"\"\n",
    "    if (np.isnan(r[year_col])) or (r[year_col]==-9999):\n",
    "        return np.full(len(bands), np.nan)\n",
    "    \n",
    "    path = os.path.join(ls_dir, basename+'{:4.0f}'.format(r[year_col])+\".vrt\")\n",
    "    if not os.path.exists(path):\n",
    "        return np.full(len(bands), np.nan)\n",
    "        \n",
    "    with rasterio.open(path) as src:\n",
    "        row, col = rasterio.transform.rowcol(src.transform, r['x'], r['y']) #r.geometry.x, r.geometry.y)# \n",
    "        window = rasterio.windows.Window.from_slices((row-krads[0], row+krads[0]+1), (col-krads[1], col+krads[1]+1)) # +1 b/c slice end exclusive\n",
    "        arr = src.read(window=window).astype(np.float32)\n",
    "        arr[arr==src.nodata] = np.nan\n",
    "        vals = np.nanmean(arr, axis=(1,2))\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7dea9f-2057-47ab-914b-d4f4a8f73ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract LandTrendr data at each site (Takes ~16 minutes)\n",
    "## It might be faster to just read the entire image and extract for every point at once (imgs only ~2-3 gb)\n",
    "## Or run point_query on each image\n",
    "ls_dir = r\"K:\\ECOFOR\\lt\\dry\"\n",
    "basename=\"lt_dry_\"\n",
    "ncores = 6\n",
    "krads = [0,0] # use mean of 3x3 window (i.e. kernel radius of 1)\n",
    "\n",
    "with rasterio.open(os.path.join(ls_dir, basename+\"2010.vrt\")) as src:\n",
    "    bands = list(src.descriptions)\n",
    "    if src.crs!=df.crs:\n",
    "        raise Exception(\"Coordinate systems of the raster and dataframe don't match\")\n",
    "        \n",
    "ddf = dd.from_pandas(df, npartitions=ncores)\n",
    "empty = pd.DataFrame(columns=list(range(len(bands))))\n",
    "proc = ddf.map_partitions(lambda df: df.apply(get_ls_data, axis=1, result_type='expand', year_col=\"year\", krads=krads, basename=basename, ls_dir=ls_dir, bands=bands), meta=empty)\n",
    "result = proc.compute(scheduler=\"processes\")\n",
    "cols = [basename+b for b in bands]\n",
    "result.columns = cols\n",
    "df.loc[:,cols] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d145ae0f-4857-4611-8ff8-45d05b0f20f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract LandTrendr data at each site\n",
    "ls_dir = r\"K:\\ECOFOR\\lt\\wet\"\n",
    "basename=\"lt_wet_\"\n",
    "ncores = 6\n",
    "krads = [0,0] # use mean of 3x3 window (i.e. kernel radius of 1)\n",
    "\n",
    "with rasterio.open(os.path.join(ls_dir, basename+\"2010.vrt\")) as src:\n",
    "    bands = list(src.descriptions)\n",
    "    if src.crs!=df.crs:\n",
    "        raise Exception(\"Coordinate systems of the raster and dataframe don't match\")\n",
    "        \n",
    "ddf = dd.from_pandas(df, npartitions=ncores)\n",
    "empty = pd.DataFrame(columns=list(range(len(bands))))\n",
    "proc = ddf.map_partitions(lambda df: df.apply(get_ls_data, axis=1, result_type='expand', year_col=\"year\", krads=krads, basename=basename, ls_dir=ls_dir, bands=bands), meta=empty)\n",
    "result = proc.compute(scheduler=\"processes\")\n",
    "cols = [basename+b for b in bands]\n",
    "result.columns = cols\n",
    "df.loc[:,cols] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3d2fa5-37cf-4a92-b937-6fb1b9153a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_file(r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\derived\\vca_herb_lt.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fb1ce1-ad8f-460e-89ca-59dc0a1c1565",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac45c15-9457-4eee-b23c-5fc29b16e655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing data\n",
    "df = gpd.read_file(r\"J:\\projects\\ECOFOR\\ancillary_data\\VCA\\derived\\vca_herb_lt.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214fea68-a472-4b70-9023-6b6375bee449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering\n",
    "\n",
    "# Remove plots with a large discrepancy in coordinates between the site details and vca data sheets\n",
    "# This removes like 300 plots\n",
    "df = df[(df['coords_dif'].isna()) | (df['coords_dif'] < 100)]\n",
    "\n",
    "# Remove plots with really high agb\n",
    "df = df[df['herb_agb']<10000]\n",
    "\n",
    "# TODO: Remove burns?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb51a88-f09e-4698-9668-c7a3ec258842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_pred_hexbin(y, x, folds=None, vmax=100):\n",
    "    \"\"\"y=true, x=pred, \n",
    "       k = Series of fold index in x and y used in K-fold cross-validation. Calculate mean error across folds if given.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(4.5,3.75))   #(3, 2.5)\n",
    "    hb = ax.hexbin(x, y, gridsize=20, mincnt=1, cmap='magma_r', linewidths=0, edgecolor='none', vmax=vmax)\n",
    "    cb = fig.colorbar(hb, ax=ax)\n",
    "    cb.set_label('counts')\n",
    "    ax.plot((y.min(), y.max()), (y.min(),y.max()), '--k')\n",
    "    \n",
    "    if folds is not None:\n",
    "        fdf = pd.DataFrame({'true':y, 'pred':x, 'fold':folds})\n",
    "        folded = fdf.groupby('fold')\n",
    "        r2 = folded.apply(lambda g: r2_score(g['true'], g['pred'])).mean()\n",
    "        bias = folded.apply(lambda g: (g['pred'] - g['true']).mean()).mean()\n",
    "        rmse = folded.apply(lambda g: mean_squared_error(g['true'], g['pred'])**0.5).mean()\n",
    "    else:\n",
    "        r2 = r2_score(y, x)\n",
    "        bias = (x-y).mean()\n",
    "        rmse = mean_squared_error(y, x)**0.5\n",
    "    \n",
    "    # add text\n",
    "    ax.text(0.99, 0.22, \"R$^2$= \" + str(r2.round(2)), transform=ax.transAxes, ha='right')\n",
    "    ax.text(0.99, 0.13, \"Bias= \"+str(bias.round(2)), transform=ax.transAxes, ha='right')\n",
    "    ax.text(0.99, 0.02,  \"RMSE= \" + str(rmse.round(2)), transform=ax.transAxes, ha='right')\n",
    "    ax.set(xlabel='Predicted', ylabel='Observed')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb629c73-615b-413b-8408-ddb39ac5888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Herb AGB regression \n",
    "Xcols = df.columns[df.columns.str.startswith('lt')].tolist()\n",
    "ycol = 'herb_agb'\n",
    "mdf = df[~df[Xcols+[ycol]].isnull().any(axis=1)] # drop nulls (should be none)\n",
    "X, y = mdf[Xcols], mdf[ycol]\n",
    "rf = RandomForestRegressor(n_estimators=500, max_features='sqrt', oob_score=True, random_state=0, n_jobs=4)\n",
    "rf = rf.fit(X, y)\n",
    "\n",
    "ypred = pd.Series(rf.oob_prediction_, index=y.index)\n",
    "imps = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "fig = obs_pred_hexbin(y, ypred, vmax=100);\n",
    "fig.suptitle('Herb AGB 2000-2012 (kg/ha)')\n",
    "print(\"N=\", len(y))\n",
    "# print(imps.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140bc703-1149-4e39-a95c-57211061fd3b",
   "metadata": {},
   "source": [
    "# Burn perimeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b6f49d-7d11-4829-889f-9f9e527ab1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge fire peremiters\n",
    "indir = r\"J:\\projects\\ECOFOR\\ancillary_data\\knp_fires\"\n",
    "paths = glob(os.path.join(indir, \"*.shp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb55c7-483f-4d9b-979d-1ccdc50e94cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates and keep UTM version\n",
    "drop_paths = ['J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2014fires.shp',\n",
    "             'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2015fires.shp',\n",
    "             'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2016fires.shp',\n",
    "             'J:\\\\projects\\\\ECOFOR\\\\ancillary_data\\\\knp_fires\\\\2017fires.shp']\n",
    "paths = [p for p in paths if p not in drop_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f4c29f-ff62-4bc0-88e1-cbf4ef54c6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "933b4119-456f-46b6-bf23-446152a0b8e3",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# CCDC Veg Exploration\n",
    "Explore CCDC metrics for 2015 segment to see if they correspond to vegetation composition or structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347f0e7e-6bb3-4f92-8ee1-7a3e7fc94941",
   "metadata": {},
   "source": [
    "## Unsupervised classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962f2563-2e9f-4022-bc56-e04ce6da13da",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\scratch\\ecofor\\ccdc\\ccdc_coefs_segpre20150301_kruger.vrt\"\n",
    "aoi_path = r\"J:\\projects\\ECOFOR\\boundaries\\kruger_utm36n.tif\"\n",
    "\n",
    "with rasterio.open(aoi_path) as src:\n",
    "    aoi = src.read(1).astype(bool)\n",
    "\n",
    "with rasterio.open(path) as src:\n",
    "    arr = src.read()\n",
    "    bands = src.descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f342809-92a4-4e6e-98be-807257b47e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arr to dataframe and sample\n",
    "tab = arr.transpose([1,2,0]).reshape(arr.shape[1]*arr.shape[2], arr.shape[0], order='C')\n",
    "df = pd.DataFrame(tab, columns=[s.lower() for s in bands])\n",
    "\n",
    "# Use sample\n",
    "# df = df.sample(10000, weights = aoi.ravel(), random_state=0)\n",
    "\n",
    "# Or drop no data and data outside the park\n",
    "df = df[aoi.ravel()]\n",
    "df = df.dropna()\n",
    "\n",
    "# clear some ram (might be less ram intensive to sample array first)\n",
    "del arr, tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b2fcc4-a3c0-4116-9bd6-63d5c2c1d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcols = df.columns.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340fa714-715e-4c46-bc09-4f651596d797",
   "metadata": {},
   "source": [
    "### Build models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6074c330-9496-4a4d-8940-66a38dc3485b",
   "metadata": {},
   "source": [
    "#### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d1e11-4db6-41fd-9f5d-8c2d1246e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini-batch K-means\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from joblib import cpu_count\n",
    "cores = cpu_count() - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b430c4-2405-48b7-a368-9a6d0c2381b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = MiniBatchKMeans(n_clusters=8,\n",
    "                         batch_size=256*cores,\n",
    "                         init=\"k-means++\",\n",
    "                         max_iter=10,\n",
    "                         n_init=5,\n",
    "                         random_state=0\n",
    "                        )\n",
    "kmeans.fit(df[Xcols])\n",
    "\n",
    "df['kmeans'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e73dd-9734-487f-b618-07d68a319e46",
   "metadata": {},
   "source": [
    "#### Mean shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba82e0-0142-45fc-9530-a0b7addbcce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29bfa48f-2e8c-4b1d-bae8-bd5da3d8009c",
   "metadata": {},
   "source": [
    "#### Ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa791de1-d3ca-429c-999b-8c86029cce87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d842f-2e52-440c-b591-e6c4ba1f9dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d41551d-674c-4b62-a811-7594abbafdd9",
   "metadata": {},
   "source": [
    "#### Gaussian mixture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb20132-a7cb-4d4c-9e59-4f7b8d785010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ad5d89-ba7d-43e4-a53a-6b51a4586093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e16cde5-b07d-415e-8f6b-3b57fde1809c",
   "metadata": {},
   "source": [
    "### Evaluate clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04ae847-390b-4419-9808-7837f6bbdd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hexbin pairplot without clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf37511-7962-4524-ae0d-eb2e2f93a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clusters in pairwise comparison of variables\n",
    "cols = [col for col in Xcols for band in ['swir1', 'red', 'nir'] if band in col]\n",
    "# cols = cols[:10] #+ ['kmeans']\n",
    "\n",
    "g = sns.PairGrid(df.sample(500, random_state=0),vars=cols, hue='kmeans', palette='Set1', despine=False, height=3, diag_sharey=False)\n",
    "g.map_upper(sns.scatterplot, alpha=0.4)\n",
    "g.map_lower(sns.kdeplot, alpha=0.6)\n",
    "g.map_diag(sns.kdeplot)\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    ax.tick_params(axis='both', labelleft=True, labelbottom=True)\n",
    "    ax.set_xlabel(ax.xaxis.get_label_text(), visible=True)\n",
    "    ax.set_ylabel(ax.yaxis.get_label_text(), visible=True)\n",
    "    \n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eae641-6e17-48f1-8f3b-c16f2d3eb014",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.savefig(r'C:\\scratch\\ecofor\\kmeans_all_cl8.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f0cc89-96ff-44cb-ac0a-b10efb2e9af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics to indicate most important variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6105a9-c29c-41a0-ae95-715a244d212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics to show distance between clusters and spread within clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20d7e82-35af-4e8c-8bfc-7463e614ae93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c934bc0-0727-44ab-b9a1-6fd6813e0652",
   "metadata": {},
   "source": [
    "Pairwise plots show almost no clustering for most variables. NIR and RED mag show the most clear clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e3fbdc-603f-4010-be3c-a3f1ac2415c7",
   "metadata": {},
   "source": [
    "### Apply model to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43a93f5-0d02-4948-9d26-e9be1554548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\scratch\\ecofor\\ccdc\\ccdc_coefs_segpre20150301_kruger.vrt\"\n",
    "aoi_path = r\"J:\\projects\\ECOFOR\\boundaries\\kruger_utm36n.tif\"\n",
    "\n",
    "with rasterio.open(aoi_path) as src:\n",
    "    aoi = src.read(1).astype(bool)\n",
    "    prof = src.profile\n",
    "\n",
    "with rasterio.open(path) as src:\n",
    "    arr = src.read()\n",
    "    bands = src.descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a519ddf4-63d3-4d5c-8655-ed9774aa2a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arr to dataframe for prediction\n",
    "tab = arr.transpose([1,2,0]).reshape(arr.shape[1]*arr.shape[2], arr.shape[0], order='C')\n",
    "df = pd.DataFrame(tab, columns=[s.lower() for s in bands])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddef985-ddbc-4614-863c-3f0478727952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply model\n",
    "pred = kmeans.predict(df).astype(np.uint8)\n",
    "pred = pred.reshape(arr.shape[1], arr.shape[2])\n",
    "pred[~aoi] = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f843d32-0134-43af-ae98-8b5517d2db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to image\n",
    "outpath = r\"C:\\scratch\\ecofor\\kmeans_all_cl8.tif\"\n",
    "\n",
    "prof['nodata'] = 255\n",
    "with rasterio.open(outpath, 'w', **prof) as dst:\n",
    "        dst.write(pred, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f970a419-ddf0-415c-8cf7-06493026f944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce709d70-cc2f-4be4-9656-8dfa43f3eb6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compare to veg data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f898bfb4-51cb-4571-b5dc-c3311e6a28d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of each class in the pre-existing veg maps\n",
    "vent_path = r\"J:\\projects\\ECOFOR\\ancillary_data\\Venter\\landtypes_venter1990.shp\"\n",
    "gert_path = r\"J:\\projects\\ECOFOR\\ancillary_data\\Gertenbach\\landscapes_gertenbach1983.shp\"\n",
    "\n",
    "vent = gpd.read_file(vent_path)\n",
    "gert = gpd.read_file(gert_path)\n",
    "\n",
    "# reproject\n",
    "rast_path = r\"C:\\scratch\\ecofor\\kmeans_all_cl8.tif\"\n",
    "with rasterio.open(rast_path) as src:\n",
    "    crs = src.crs\n",
    "vent = vent.to_crs(crs)\n",
    "gert = gert.to_crs(crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016c4c4-b76e-48ad-aeaf-ee383abf9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add zonal stats as area\n",
    "ventz = pd.DataFrame(zonal_stats(vent, rast_path, categorical=True), index=vent.index)\n",
    "ventz = ventz.add_prefix('c')\n",
    "vent = pd.concat([vent, ventz], axis=1)\n",
    "class_cols = ventz.columns.tolist()\n",
    "vent[class_cols] = vent[class_cols] * (30**2 / 1000**2) # convert to km2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea93fb0e-a18e-4727-ac08-3e9915aa2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group and sum areas for land types with multiple geometrys\n",
    "vent = vent.drop(['geometry', 'SOIL'], axis=1)\n",
    "vgrp = vent.groupby(vent.columns[vent.dtypes=='object'].tolist(), as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fc4dc0-5e9e-4d57-a673-abba706ed142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get percent area of classes\n",
    "vgrp['sum_km2'] = vgrp[class_cols].sum(axis=1)\n",
    "pct_cols = ventz.add_suffix('_pct').columns.to_list()\n",
    "vgrp[pct_cols] = vgrp[class_cols].divide(vgrp['sum_km2'], axis=0)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ee4e5-acbd-4afe-a15f-a309743c2505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt for plotting\n",
    "id_vars = vgrp.columns[vgrp.dtypes=='object'].tolist()\n",
    "vmelt = pd.melt(vgrp[pct_cols+id_vars], id_vars=id_vars, var_name='class', value_vars=pct_cols, value_name = 'pct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3125290-1787-4885-a8d9-3f5e0b88f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 25))\n",
    "sns.scatterplot(y='DOMWOODY', x='pct', hue='class', data=vmelt, ax=ax, palette='Accent', y_jitter=True, x_jitter=True, s=100, alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e52ea-f69c-42bb-b83e-afa67c4cde9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5257e260-2fd6-4058-8b8a-a3a565a91baf",
   "metadata": {},
   "source": [
    "## Paper figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7840d04c-c1cc-4e2a-989f-724516a61e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# presentation set up\n",
    "figdir = r\"E:\\My Drive\\Work\\ecofor\\manuscript\\figs\"\n",
    "os.makedirs(figdir, exist_ok=True)\n",
    "\n",
    "mpl.rcParams['font.family'] = 'sans-serif'\n",
    "mpl.rcParams['font.sans-serif'] = ['Arial']\n",
    "mpl.rcParams['font.size'] = 8\n",
    "\n",
    "sns.set_style('ticks',\n",
    "               {'font.family':'sans-serif', 'font.sans-serif':['Arial'], 'font.size':8})\n",
    "\n",
    "# temporary style to use on dark backgrounds\n",
    "# white_axes = {'axes.labelcolor': '.99', 'text.color': '.99', 'xtick.color': '.99', 'ytick.color': '.99', 'axes.edgecolor': '.99', 'figure.facecolor': 'black'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2af426-2a55-4d43-b5cb-3de1227a2f13",
   "metadata": {},
   "source": [
    "### Distribution of GEDI  \n",
    "Show distribution of GEDI values for entire study area and by vegetation type.  \n",
    "\n",
    "Maybe do as KDE lines or ECDF lines with \"All\" and then by veg type or land cover class as a hue.\n",
    "Alternatively, also show the pairwise relationship with pairgrid by having a contour plot or hexgrid for bivariate distributions and a kde for univariate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bfdad8-ad75-4bcc-b39c-0d4a4160f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"J:\\projects\\ECOFOR\\gedi\\models\\v08\\GEDI_2AB_2019to2023_leafon_sampy500m_all_oob_v08.parquet\"\n",
    "df = gpd.read_parquet(path)\n",
    "\n",
    "df['cover'] *= 100 # convert cover to %\n",
    "ycol_dict = {'cover':'Cover (%)', 'rh98':'RH98 (m)', 'fhd_normal':'FHD'} #'pai':'PAI', \n",
    "df = df.rename(columns=ycol_dict)\n",
    "\n",
    "\n",
    "# ldf = pd.melt(df[ycol_dict.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b2590-9190-4e15-b754-ff83625f0956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load land cover or veg type for hue\n",
    "path = r\"J:\\projects\\ECOFOR\\gedi\\extracted\\GEDI_2AB_2019to2023_leafon_sampy500m_sanlc20.csv\"\n",
    "cdf = pd.read_csv(path).set_index('shot_number')\n",
    "\n",
    "# Create a modification of salcc1 to separate out open woodland and group others\n",
    "rat_path = r\"J:\\projects\\ECOFOR\\lcluc\\SANLC\\2020\\SA_NLC_2020_GEO.tif.vat.dbf\"\n",
    "rat = gpd.read_file(rat_path).drop('geometry', axis=1)\n",
    "rat['SALCC_1'] = rat['SALCC_1'].replace('Forested Land', 'Forested land')\n",
    "mod_dict = rat.set_index('Value')['SALCC_1'].to_dict()\n",
    "mod_dict[4] = 'Open Woodland'\n",
    "cdf['sanlc20_salcc1_mod'] = cdf['sanlc20_val'].map(mod_dict)\n",
    "other_mask = cdf['sanlc20_salcc1_mod'].isin([None, 'Built-up', 'Wetlands', 'Barren Land', 'Waterbodies', 'Mines & Quarries', 'Shrubland'])\n",
    "cdf.loc[other_mask, 'sanlc20_salcc1_mod'] = 'Other'\n",
    "\n",
    "# Merge GEDI with land cover classes\n",
    "df = pd.merge(df, cdf, how='left', left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90e95e1-7bb1-4f8e-a3a3-83ef539a6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "hue_order = df['sanlc20_salcc1_mod'].value_counts().index\n",
    "palette =  ['#CDAA66', '#728944', '#FFAA00', '#CD6666', '#E9FFBE'] #'deep'#['#1b9e77','#d95f02','#7570b3','#e7298a','#66a61e']\n",
    "# palette.reverse()\n",
    "\n",
    "g = sns.PairGrid(df, vars=ycol_dict.values(), hue='sanlc20_salcc1_mod',\n",
    "                 corner=True, height=1.2, diag_sharey=False,\n",
    "                 hue_order=hue_order)\n",
    "g.map_diag(sns.kdeplot, palette=palette)#, common_norm=False)\n",
    "# g.map_diag(sns.ecdfplot, stat=\"proportion\", alpha=.6, linewidth=1.5)\n",
    "g.map_lower(sns.histplot, hue=None, bins=50, cmap = 'YlGn', vmin=50, vmax=2000)\n",
    "lines = g.fig.axes[-1].get_lines()\n",
    "lines.reverse()\n",
    "g.fig.legend(lines, hue_order, loc=(0.63,0.75))\n",
    "\n",
    "ax = g.fig.add_axes((0.635, 0.64, 0.3, 0.06))\n",
    "norm = mpl.colors.Normalize(vmin=50, vmax=2000)\n",
    "g.fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=mpl.cm.YlGn),\n",
    "             cax=ax, orientation='horizontal')\n",
    "\n",
    "figname = \"gedi_metrics_pairplot\"\n",
    "for ext in [\".pdf\", \".svg\"]:\n",
    "    figpath = os.path.join(figdir, figname + ext)\n",
    "    g.fig.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385bae53-98d2-43c7-baaf-dd2f3dbe13eb",
   "metadata": {},
   "source": [
    "### Optical data availability  \n",
    "Show count of L30 vs HLS scenes available over time to show how adding S2 fills gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189919c4-ee35-4557-bf2a-dfd8f52890cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paired t-test of RMSE of CCDC fit using HLS vs L30 for the sample\n",
    "from scipy.stats import ttest_rel\n",
    "path = r\"J:\\projects\\ECOFOR\\gedi\\extracted\\GEDI_2AB_2019to2023_leafon_sampy500m_all.parquet\"\n",
    "df = gpd.read_parquet(path)\n",
    "\n",
    "bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2'] #'ca', \n",
    "cols = [col for col in df.columns for src in ['hls', 'l30'] for band in bands if col.startswith(src) and (band in col) and col.endswith('rmse')]\n",
    "\n",
    "df = df[cols].dropna()\n",
    "df.columns = pd.MultiIndex.from_tuples([col.split('_')[:2] for col in df.columns], names=[\"Source\", \"Band\"])\n",
    "\n",
    "for band in bands:\n",
    "    print(band)\n",
    "    print(ttest_rel(df[('hls', band)], df[('l30', band)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f163e-f168-4ba1-b1fa-41c382cb28df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get confidence interval for the differences\n",
    "import scipy.stats as st\n",
    "confidence_level = 0.95\n",
    "\n",
    "sdf = df.stack()\n",
    "sdf['dif'] = sdf['hls'] - sdf['l30']\n",
    "ddf = sdf['dif'].unstack()\n",
    "\n",
    "cis = ddf.apply(lambda x: st.t.interval(0.95, len(x)-1, loc=x.mean(), scale=st.sem(x)))\n",
    "cis.index = ['low', 'high']\n",
    "cis = cis.T\n",
    "cis['mean'] = ddf.mean()\n",
    "cis['half_width'] = (cis['high'] - cis['low']) / 2\n",
    "\n",
    "# cis\n",
    "with pd.option_context('display.precision', 2):\n",
    "    display(cis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd22d2a-96e6-4caf-ae93-9059ce327a55",
   "metadata": {},
   "source": [
    "### Model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b760396-a93a-4ae6-9676-a85cbfc387af",
   "metadata": {},
   "source": [
    "#### All models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f025186-ef4d-4963-b28e-06ef215af720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and setup data\n",
    "path = r\"J:\\projects\\ECOFOR\\gedi\\models\\v08\\GEDI_2AB_2019to2023_leafon_sampy500m_all_oob_v08.parquet\"\n",
    "df = gpd.read_parquet(path)\n",
    "\n",
    "source_dict = {\n",
    "    'lt-p-s-t': 'LandTrendr + PALSAR + Soils + Topo',\n",
    "    'ccdcl30-p-s-t': '$CCDC^{L30}$ + PALSAR + Soils + Topo',\n",
    "    'ccdchls-p-s-t': '$CCDC^{HLS}$ + PALSAR + Soils + Topo',\n",
    "    'lt-p': 'LandTrendr + PALSAR',\n",
    "    'ccdcl30-p': '$CCDC^{L30}$ + PALSAR',\n",
    "    'ccdchls-p':'$CCDC^{HLS}$ + PALSAR',\n",
    "    'p-s-t': 'PALSAR + Soils + Topo',\n",
    "    'lt': 'LandTrendr',\n",
    "    'ccdcl30': '$CCDC^{L30}$',\n",
    "    'ccdchls': '$CCDC^{HLS}$',\n",
    "    'p': 'PALSAR',\n",
    "    's-t': 'Soils + Topography',\n",
    "}\n",
    "\n",
    "\n",
    "ydict = {'cover': 'Cover',\n",
    "#          'pai': 'PAI',\n",
    "         'rh98': 'RH98',\n",
    "         'fhd_normal': 'FHD'}\n",
    "Xsets = list(source_dict.keys())\n",
    "Ycols = list(ydict.keys())\n",
    "\n",
    "# make cover cols as percent\n",
    "df[df.columns[df.columns.str.contains('cover')]] *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941fa40-f99e-4096-b371-2563c75980db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get stats for all models\n",
    "sdf = pd.DataFrame(columns = pd.MultiIndex.from_product([list(ydict.values()), ['R2', 'RMSE', 'Bias', 'N']], names=(\"Metric\", \"Stat\")))\n",
    "sdf.index.name = 'Xset'\n",
    "\n",
    "for Xset in Xsets:\n",
    "    for ycol in Ycols:\n",
    "        x, y = df['pred_'+Xset+'_'+ycol], df[ycol]\n",
    "        \n",
    "        # Save stats\n",
    "        sdf.loc[Xset, (ydict[ycol], 'R2')] = r2_score(y, x)\n",
    "        sdf.loc[Xset, (ydict[ycol], 'RMSE')] = mean_squared_error(y, x)**0.5\n",
    "        sdf.loc[Xset, (ydict[ycol], 'Bias')] = (x-y).mean()\n",
    "        sdf.loc[Xset, (ydict[ycol], 'N')] = len(y)\n",
    "sdf = sdf.apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9ff1e6-aea3-4e5e-a3e3-2c8a8d9df374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and setup TCV data\n",
    "tcv_path = r\"J:\\projects\\ECOFOR\\gedi\\models\\v08\\GEDI_2AB_2019to2023_leafon_sampy500m_all_tcv_v08_stats.csv\"\n",
    "tdf = pd.read_csv(tcv_path)\n",
    "\n",
    "tdf = tdf[tdf['metric']!='pai'] # Drop PAI\n",
    "\n",
    "tdf['Metric'] = tdf['metric'].replace(ydict)\n",
    "tdf = tdf.rename(columns={'n':'N', 'r2':'R2', 'rmse':'RMSE', 'bias':'Bias'})\n",
    "tdf.loc[tdf['metric']=='cover', ['RMSE', 'Bias']] *= 100\n",
    "\n",
    "tcv = tdf.groupby(['Xset', 'Metric']).mean(numeric_only=True)\n",
    "tcv = tcv.drop(columns=['year'])\n",
    "tcv.columns.name = 'Stat'\n",
    "\n",
    "tldf = pd.melt(tcv, ignore_index=False, value_name='TCV').reset_index()\n",
    "# tldf['Source'] = tldf['Xset'].map(source_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bf409e-8fd3-44cc-8819-5a131b0906d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bar chart of accuracy stats\n",
    "\n",
    "# Make long form and add TCV stats\n",
    "ldf = pd.melt(sdf, ignore_index=False).reset_index()\n",
    "ldf = pd.merge(ldf, tldf, how='left', on=['Xset', 'Metric', 'Stat'])\n",
    "ldf['Source'] = ldf['Xset'].map(source_dict)\n",
    "\n",
    "mask = ldf['Stat'].isin(['R2', 'RMSE', 'Bias'])\n",
    "\n",
    "# palette = ['#a6cee3','#b2df8a','#cab2d6', '#1f78b4','#33a02c']\n",
    "p = sns.color_palette(palette='tab20c')\n",
    "palette = p[0:9:4]+p[1:10:4]+[p[12]]+p[2:11:4]+[p[13], p[16]]\n",
    "\n",
    "g = sns.catplot(data=ldf[mask], x=\"value\", y=\"Source\", row=\"Stat\", col=\"Metric\", kind='bar',\n",
    "                sharex=False, sharey=True, height=3, aspect=1.3, margin_titles=True, palette=palette)\n",
    "\n",
    "# Overlay point plot with TCV stats\n",
    "g.map(sns.pointplot, \"TCV\", \"Source\", marker=\"o\", join=False, color=\"k\")\n",
    "\n",
    "# Clean up plot\n",
    "g.set_titles(col_template=\"{col_name}\", row_template=\"{row_name}\")\n",
    "g.set_ylabels(\"\")\n",
    "\n",
    "figname = \"gedi_acc_all\"\n",
    "for ext in [\".pdf\", \".svg\"]:\n",
    "    figpath = os.path.join(figdir, figname + ext)\n",
    "    g.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dca50e-d603-4238-a0cc-f9f48a35b34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent difference of RMSE for each optical only model from the mean RMSE of those models\n",
    "opt_rmse = sdf.loc[['lt', 'ccdcl30', 'ccdchls'], (slice(None), 'RMSE')]\n",
    "100 * opt_rmse.subtract(opt_rmse.mean()).divide(opt_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b67f5f-562e-4452-ab41-e35b3fde18f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent difference of chosen model RMSE from best model\n",
    "opt_rmse = sdf.loc[['lt-p-s-t', 'ccdcl30-p-s-t', 'ccdchls-p-s-t'], (slice(None), 'RMSE')]\n",
    "100 * opt_rmse.subtract(opt_rmse.mean()).divide(opt_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bce68ae-3f77-4081-a9d0-d744fc3b4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of LandTrendr only to PALSAR only\n",
    "lt_minus_p_r2 = sdf.loc['lt', (slice(None), 'R2')] - sdf.loc['p', (slice(None), 'R2')]\n",
    "display(lt_minus_p_r2)\n",
    "print(\"LandTrendr explained\", (lt_minus_p_r2.mean()*100).round(), \"% more variance on average than PALSAR.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff98bef9-6a93-4056-804d-b311b2bfb0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change in RMSE when adding PALSAR and Soils and Topo\n",
    "opt_sar_rmse = sdf.loc[['lt-p', 'ccdcl30-p', 'ccdchls-p'], (slice(None), 'RMSE')]\n",
    "opt_sar_rmse = opt_sar_rmse.set_index(opt_rmse.index)\n",
    "mean_opt_sar_chg = np.nanmean(100* opt_sar_rmse.subtract(opt_rmse).divide(opt_rmse)).round(1)\n",
    "print(\"RMSE changed\", mean_opt_sar_chg, \"% on average when adding PALSAR predictors to optical predictors\")\n",
    "\n",
    "# Change in RMSE when adding PALSAR and Soils and Topo\n",
    "opt_pst_rmse = sdf.loc[['lt-p-s-t', 'ccdcl30-p-s-t', 'ccdchls-p-s-t'], (slice(None), 'RMSE')]\n",
    "opt_pst_rmse = opt_pst_rmse.set_index(opt_rmse.index)\n",
    "mean_opt_pst_chg = np.nanmean(100* opt_pst_rmse.subtract(opt_rmse).divide(opt_rmse)).round(1)\n",
    "print(\"RMSE changed\", mean_opt_pst_chg, \"% on average when adding PALSAR and soil and topography predictors to optical predictors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c438da-b934-4cdd-a381-ff02a8a26164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get difference between TCV stats and OOB stats\n",
    "\n",
    "# Reshape TCV stats to match OOB stats (sdf)\n",
    "tcv_wide = tcv.unstack(level=1)\n",
    "tcv_wide.columns = tcv_wide.columns.swaplevel()\n",
    "tcv_wide.sort_index(axis=1, level=0, inplace=True)\n",
    "\n",
    "sdif = tcv_wide.subtract(sdf)\n",
    "spct = sdif.divide(sdf) * 100\n",
    "\n",
    "r2_dif_mean = np.nanmean(sdif.loc[:,(slice(None), 'R2')]).round(2)\n",
    "print(\"TCV R2 different from OOB R2 by\", r2_dif_mean, \"on average.\")\n",
    "\n",
    "rmse_pct_dif_mean = np.nanmean(spct.loc[:,(slice(None), 'RMSE')]).round(2)\n",
    "print(\"TCV RMSE different from OOB RMSE by\", rmse_pct_dif_mean, \"% on average.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220aaae2-f6d2-4aa5-bd1e-20b9e9deb204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias as % of mean observed value\n",
    "# Mean observed value\n",
    "mean_obs = df[['cover', 'rh98', 'fhd_normal']].mean()\n",
    "mean_obs.index = ['Cover', 'RH98', 'FHD']\n",
    "\n",
    "# OOB absolute bias\n",
    "oob_bias = sdf.loc[:,(slice(None), 'Bias')].droplevel(1, axis=1).abs()\n",
    "oob_bias_pct = oob_bias / mean_obs * 100\n",
    "\n",
    "# TCV absolute bias\n",
    "tcv_bias = tcv_wide.loc[:,(slice(None), 'Bias')].droplevel(1, axis=1).abs()\n",
    "tcv_bias_pct = tcv_bias / mean_obs * 100\n",
    "\n",
    "def get_df_max(df):\n",
    "    col_max = df.max().idxmax()\n",
    "    row_max = df[col_max].idxmax()\n",
    "    max_val = df.loc[row_max, col_max]\n",
    "    return max_val, (row_max, col_max)\n",
    "\n",
    "val, (model, metric) = get_df_max(oob_bias_pct)\n",
    "print(\"OOB absolute bias maximum as a percent of the mean observed value was\", val.round(1), \"% for\", model, metric)\n",
    "\n",
    "val, (model, metric) = get_df_max(tcv_bias_pct)\n",
    "print(\"TCV absolute bias maximum  as a percent of the mean observed value was\", val.round(1), \"% for\", model, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a320e90-9a03-4ae6-87df-489695d3cb99",
   "metadata": {},
   "source": [
    "#### Chosen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004633e-1e3f-4f66-874e-e968af9b3780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Obs vs pred for only the best model\n",
    "# 1x4\n",
    "Xset = 'lt-p-s-t' #'hlsp'\n",
    "ycol_dict = {'cover':'Cover (%)','pai':'PAI', 'rh98':'RH98 (m)', 'fhd_normal':'FHD'}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(2.15*3, 1.5))\n",
    "\n",
    "for i, (ycol, ax) in enumerate(zip(Ycols, axes.flat)):\n",
    "    x, y = df['pred_'+Xset+'_'+ycol], df[ycol]\n",
    "    hb = ax.hexbin(x, y, gridsize=20, mincnt=1, cmap='magma_r', linewidths=0, edgecolor='none', vmax=2000)\n",
    "    ax.plot((y.min(), y.max()), (y.min(),y.max()), '--k')\n",
    "\n",
    "    r2 = r2_score(y, x)\n",
    "    bias = (x-y).mean()\n",
    "    rmse = mean_squared_error(y, x)**0.5\n",
    "\n",
    "    # add text\n",
    "    ax.text(0.99, 0.22, \"R$^2$= \" + str(np.round(r2, 2)), transform=ax.transAxes, ha='right')\n",
    "    ax.text(0.99, 0.13, \"Bias= \"+\"{:.2f}\".format(np.round(bias, 2)), transform=ax.transAxes, ha='right')\n",
    "    ax.text(0.99, 0.02,  \"RMSE= \" + str(np.round(rmse, 2)), transform=ax.transAxes, ha='right')\n",
    "\n",
    "    ax.set(title=ycol_dict[ycol])\n",
    "    if i==0:\n",
    "        ax.set(ylabel='Observed')\n",
    "    if i==1:\n",
    "        ax.set(xlabel='Predicted')\n",
    "\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "cb = fig.colorbar(hb, ax=axes, location='right', orientation='vertical', pad=0.02)#, shrink=True, aspect=16, pad=0.02) #cax=cax, aspect=)#\n",
    "cb.set_label('Count')\n",
    "\n",
    "figname = \"gedi_acc_\" + Xset\n",
    "for ext in [\".pdf\", \".svg\"]:\n",
    "    figpath = os.path.join(figdir, figname + ext)\n",
    "    fig.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf6e50-cecb-42ca-a767-c14038db8fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosen model verseus model with lowest RMSE\n",
    "chosen = \"lt-p-s-t\"\n",
    "best_rmse = pd.concat([sdf.loc[:,(slice(None), \"RMSE\")].min(), \n",
    "                       sdf.loc[:,(slice(None), \"RMSE\")].idxmin()], axis=1, keys=['RMSE', 'Xset'])\n",
    "\n",
    "print(\"Model with lowest RMSE\")\n",
    "display(best_rmse)\n",
    "\n",
    "print(\"Difference of chosen model from best model\")\n",
    "display(sdf.loc[chosen, (slice(None), \"RMSE\")] - best_rmse[\"RMSE\"])\n",
    "\n",
    "print(\"Percent difference of chosen model from best model\")\n",
    "display((sdf.loc[chosen, (slice(None), \"RMSE\")] - best_rmse[\"RMSE\"]) / best_rmse[\"RMSE\"] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b79de2-aa99-4982-af77-612c774213ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of CCDC and LandTrendr variables\n",
    "lt_path = r\"J:\\projects\\ECOFOR\\gedi\\extracted\\GEDI_2AB_2019to2023_leafon_sampy500m_lt.csv\"\n",
    "ccdc_path = r\"J:\\projects\\ECOFOR\\gedi\\extracted\\GEDI_2AB_2019to2023_leafon_sampy500m_l30s2_ccdc.csv\"\n",
    "\n",
    "lt = pd.read_csv(lt_path, nrows=0).columns\n",
    "ccdc = pd.read_csv(ccdc_path, nrows=0).columns\n",
    "\n",
    "# Filter to same columns used in modeling\n",
    "lt = lt.drop('shot_number')\n",
    "bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2'] #'ca', \n",
    "ccdc = [col for col in ccdc for band in bands if col.startswith(band)] # using CCDC\n",
    "\n",
    "print(\"LandTrendr used\", len(lt), \"variables.\")\n",
    "print(\"CCDC used\", len(ccdc), \"variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d482cc-1d0f-4435-b035-cd494681f8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of predicted and observed for chosen model\n",
    "pred_cov_lt_1pct = df.loc[df['cover']<1, 'pred_'+chosen+'_cover'].mean()\n",
    "print(\"Observed cover <1% was predicted as\", pred_cov_lt_1pct, \"% on average.\")\n",
    "\n",
    "mask = df['cover']>50\n",
    "pred_cov_err_mask = (df.loc[mask, 'pred_'+chosen+'_cover'] - df.loc[mask, 'cover']).mean()\n",
    "print(\"Observed cover >50% was overestimated by\", pred_cov_err_mask, \"% on average.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e11ecf-552a-477d-b685-29b40e42b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df['rh98']>15\n",
    "(df.loc[mask, 'pred_'+chosen+'_rh98'] - df.loc[mask, 'rh98']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598458b-59cc-4c87-9a55-6a4b9ce95adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TCV stats for chosen model\n",
    "tldf[tldf['Xset']=='lt-p-s-t'].pivot(index=['Xset', 'Metric'], columns='Stat', values='TCV').round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97f123-2776-4983-a741-37942e0761f5",
   "metadata": {},
   "source": [
    "#### Bias correction\n",
    "Bias correction results for chosen model. May need to update stats used above in paper to use the bias-corrected version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1036185-f41d-49c0-b158-5f081e16f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"J:\\projects\\ECOFOR\\gedi\\models\\v08BC\\GEDI_2AB_2019to2023_leafon_sampy500m_all_bc_v08BC.parquet\"\n",
    "# path = r\"J:\\projects\\ECOFOR\\gedi\\models\\v08EDM\\GEDI_2AB_2019to2023_leafon_sampy500m_all_bc_v08EDM.parquet\"\n",
    "df = pd.read_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a9a49f-3c33-4514-bf95-5f33bfe0ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "ycol_dict = {'cover':'Cover (%)', 'rh98':'RH98 (m)', 'fhd_normal':'FHD'}\n",
    "Ycols=list(ycol_dict.keys())\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(2.15*3, 3.1))\n",
    "\n",
    "for pred_type, axrow in zip(['pred', 'pred_bc'], axes):\n",
    "    for i, (ycol, ax) in enumerate(zip(Ycols, axrow)):\n",
    "        x, y = df[pred_type+'_'+ycol], df[ycol]\n",
    "        hb = ax.hexbin(x, y, gridsize=20, mincnt=1, cmap='magma_r', linewidths=0, edgecolor='none', vmax=1000)\n",
    "        ax.plot((y.min(), y.max()), (y.min(),y.max()), '--k')\n",
    "\n",
    "        r2 = r2_score(y, x)\n",
    "        bias = (x-y).mean()\n",
    "        rmse = mean_squared_error(y, x)**0.5\n",
    "\n",
    "        # add text\n",
    "        ax.text(0.99, 0.22, \"R$^2$= \" + str(np.round(r2, 2)), transform=ax.transAxes, ha='right')\n",
    "        ax.text(0.99, 0.13, \"Bias= \"+\"{:.2f}\".format(np.round(bias, 2)), transform=ax.transAxes, ha='right')\n",
    "        ax.text(0.99, 0.02,  \"RMSE= \" + str(np.round(rmse, 2)), transform=ax.transAxes, ha='right')\n",
    "        ytext = \"Original\" if pred_type==\"pred\" else \"Bias-corrected\"\n",
    "        \n",
    "        if pred_type=='pred':\n",
    "            ax.set(title=ycol_dict[ycol])\n",
    "            ax.set_xticklabels([])\n",
    "        if i==0:\n",
    "            ax.set(ylabel='Observed')\n",
    "            ax.text(-0.5, 0.5, ytext, transform=ax.transAxes, ha='center', va='center', rotation='vertical', fontsize=10, fontweight='bold')\n",
    "        if i==1 and pred_type=='pred_bc':\n",
    "            ax.set(xlabel='Predicted')\n",
    "            \n",
    "\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "cb = fig.colorbar(hb, ax=axes, location='right', orientation='vertical', pad=0.02)#, shrink=True, aspect=16, pad=0.02) #cax=cax, aspect=)#\n",
    "cb.set_label('Count')\n",
    "\n",
    "# figname = \"gedi_acc_biascorrection\"\n",
    "# for ext in [\".pdf\", \".svg\"]:\n",
    "#     figpath = os.path.join(figdir, figname + ext)\n",
    "#     fig.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b380d0f-c443-4b21-b70b-3ccb9b031174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions of the predictions and observations\n",
    "from scipy.stats import ks_2samp, t\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(6.5, 2))\n",
    "\n",
    "for i, (ycol, ax) in enumerate(zip(Ycols, axes)):\n",
    "    pred_col = 'pred_'+ycol\n",
    "    bc_col = 'pred_bc_'+ycol\n",
    "    xydf = pd.melt(df[[pred_col, bc_col, ycol]])\n",
    "    sns.ecdfplot(xydf, x='value', hue='variable', ax=ax, legend=False)\n",
    "\n",
    "    # ks_2samp silently gives wrong values if nan's included, so make sure they're removed\n",
    "    mask = df[[pred_col, ycol]].notna().all(axis=1)\n",
    "    ks, pval = ks_2samp(df.loc[mask, pred_col], df.loc[mask, ycol])\n",
    "    ax.text(0.95, 0.16, f\"Orig & Obs KS= {np.round(ks,2)}\", ha='right', transform=ax.transAxes)\n",
    "    \n",
    "    mask = df[[bc_col, ycol]].notna().all(axis=1)\n",
    "    ks, pval = ks_2samp(df.loc[mask, bc_col], df.loc[mask, ycol])\n",
    "    ax.text(0.95, 0.1, f\"BC & Obs KS= {np.round(ks,2)}\", ha='right', transform=ax.transAxes)\n",
    "    \n",
    "    mask = df[[bc_col, pred_col]].notna().all(axis=1)\n",
    "    ks, pval = ks_2samp(df.loc[mask, bc_col], df.loc[mask, pred_col])\n",
    "    ax.text(0.95, 0.03, f\"BC & Orig KS= {np.round(ks,2)}\", ha='right', transform=ax.transAxes)\n",
    "\n",
    "    ax.set(ylabel=None, title=ycol_dict[ycol])\n",
    "        \n",
    "orange_line = mpl.lines.Line2D([0], [0], color='orange', lw=2)\n",
    "blue_line = mpl.lines.Line2D([0], [0], color='blue', lw=2)\n",
    "green_line = mpl.lines.Line2D([0], [0], color='green', lw=2)\n",
    "fig.legend([orange_line, blue_line, green_line], ['Bias-corrected Prediction', 'Original Prediction', 'Observed'], loc='center', bbox_to_anchor=(0.5,-0.15), ncol=3)\n",
    "\n",
    "# figname = \"gedi_bc_ecdf\"\n",
    "# for ext in [\".pdf\", \".svg\"]:\n",
    "#     figpath = os.path.join(figdir, figname + ext)\n",
    "#     fig.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d347fa-f8ab-4f56-8969-9842957ca65e",
   "metadata": {},
   "source": [
    "### Field evaluation\n",
    "Compare the field measurements to GEDI footprints and predicted maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95514bc5-b576-46d0-9668-43d86e795daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_regplot(x, y, ax, lims=None, reg=True, oneone=True, **kwargs):\n",
    "    from scipy.stats import linregress\n",
    "    \n",
    "    sns.regplot(x=x, y=y, ax=ax, **kwargs)\n",
    "\n",
    "    if reg:\n",
    "        # Regression\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "        rmse = mean_squared_error(y, x*slope+intercept)**0.5\n",
    "        eq = \"y = \" + str(np.round(slope,2)) + \"x + \" + str(np.round(intercept, 2))\n",
    "        ax.text(0.97, 0.26, \"Regression:\", transform=ax.transAxes, ha='right')\n",
    "        ax.text(0.98, 0.17, eq, transform=ax.transAxes, ha='right')\n",
    "        ax.text(0.98, 0.09, \"R$^2$= \" + str(np.round(r_value**2, 2)), transform=ax.transAxes, ha='right')\n",
    "        ax.text(0.98, 0.01, \"RMSE= \"+str(np.round(rmse, 2)), transform=ax.transAxes, ha='right')\n",
    "\n",
    "#         # check that line is the same as from sns.reglot\n",
    "#         samp = np.arange(x.min(), x.max(), 1)\n",
    "#         ax.plot(samp, intercept + slope * samp, 'r')\n",
    "\n",
    "    if oneone:\n",
    "        if lims is None:\n",
    "            lims = (0, np.nanmax(x.append(y))) #np.nanmin(x.append(y))\n",
    "        ax.plot(lims, lims, '--k')\n",
    "        ax.set(ylim=lims, xlim=lims)\n",
    "\n",
    "        # add text for R2 and RMSE\n",
    "        r2 = r2_score(y, x)\n",
    "        rmse = mean_squared_error(y, x)**0.5\n",
    "        bias = (x-y).mean()\n",
    "        ax.text(0.03, 0.93, \"1:1 stats:\", transform=ax.transAxes)\n",
    "        ax.text(0.03, 0.85,\"R$^2$= \"+str(np.round(r2, 2)), transform=ax.transAxes)\n",
    "        ax.text(0.03, 0.77, \"RMSE= \"+str(np.round(rmse, 2)), transform=ax.transAxes)\n",
    "        ax.text(0.03, 0.69, \"Bias= \"+str(np.round(bias, 2)), transform=ax.transAxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b962cd5-e113-4f63-bc86-636f7ca9866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot max tree height compared to GEDI's RH98\n",
    "path = r\"J:\\projects\\ECOFOR\\field\\merged\\gedi_trees_cover_simp.csv\"\n",
    "df = pd.read_csv(path, index_col='plot_ix')\n",
    "\n",
    "# Load plot notes to drop bad plots\n",
    "path = r\"J:\\projects\\ECOFOR\\field\\merged\\plot_notes.csv\"\n",
    "pdf = pd.read_csv(path, index_col='plot_ix')\n",
    "\n",
    "df[['exclude_plot', 'exclude_reason']] = pdf[['exclude_plot', 'exclude_reason']]\n",
    "df = df[~df['exclude_plot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50c75db-7dd6-4a0a-826d-1f76c60f7e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get predicted RH98 for 2022 for the location of the tallest measured tree for comparison\n",
    "trees = gpd.read_file(r\"J:\\projects\\ECOFOR\\field\\merged\\gedi_all_merged.gpkg\", layer=\"trees\")\n",
    "rast_path = r\"J:\\projects\\ECOFOR\\gedi\\maps\\v08\\lt-p-s-t\\rh98\\rh98_2022.tif\"\n",
    "\n",
    "with rasterio.open(rast_path) as src:\n",
    "    trees = trees.to_crs(src.crs)\n",
    "trees = trees.dropna(subset=['hgt', 'geometry'])\n",
    "\n",
    "trees['pred_rh98'] = list(gen_point_query(trees, rast_path, interpolate='nearest'))\n",
    "\n",
    "# Get tallest tree of kept plots\n",
    "trees = trees[trees['plot_ix'].isin(df.index.values)]\n",
    "tallest_ix = trees.groupby('plot_ix')['hgt'].idxmax().dropna()\n",
    "trees = trees.loc[tallest_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a86236-bbef-4f8f-85e3-ac1eb1cff4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(4,2))\n",
    "present_regplot(df['rh98'], df['hgt'], lims=(0,30), ax=ax1, scatter_kws={'s': 15, 'zorder':2, 'alpha':0.5}, line_kws={'color':'k', 'alpha':0.5, 'zorder':1})\n",
    "present_regplot(trees['pred_rh98'], trees['hgt'], lims=(0,30), ax=ax2, scatter_kws={'s': 15, 'zorder':2, 'alpha':0.5}, line_kws={'color':'k', 'alpha':0.5, 'zorder':1})\n",
    "ax1.set(xlabel='RH98 (m)', ylabel='Max tree height (m)')\n",
    "ax2.set(xlabel='Predicted RH98 (m)', ylabel='Max tree height (m)')\n",
    "fig.tight_layout()\n",
    "\n",
    "figname = \"field_height\"\n",
    "for ext in [\".pdf\", \".svg\"]: \n",
    "    figpath = os.path.join(figdir, figname + ext)\n",
    "    fig.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aab4ff-f054-4c45-a179-4698afb6a001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot without the tall tree observation\n",
    "fig, ax = plt.subplots(figsize=(2,2))\n",
    "mask = trees['plot_ix']!=14\n",
    "present_regplot(trees.loc[mask, 'pred_rh98'], trees.loc[mask, 'hgt'], lims=(0,15), ax=ax, scatter_kws={'s': 15, 'zorder':2, 'alpha':0.5}, line_kws={'color':'k', 'alpha':0.5, 'zorder':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1705b142-7589-4042-837d-d5ce05b127e9",
   "metadata": {},
   "source": [
    "### Climate sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc36df3-0863-4da5-938c-001025604e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LandTrendr and predictions and make long form for plotting\n",
    "path = r\"J:\\projects\\ECOFOR\\climate_sensitivity\\vca_unburned09to17_snapped_pi_extract2.gpkg\"\n",
    "\n",
    "df = gpd.read_file(path)\n",
    "\n",
    "df = df[df[\"change\"]==\"none\"] # only analyze no change sites\n",
    "\n",
    "srcs = list(indirs.keys())\n",
    "value_vars = [col for col in df.columns for src in srcs if col.startswith(src)]\n",
    "id_vars = [col for col in df.columns if col not in value_vars]\n",
    "\n",
    "ldf = (\n",
    "    df.melt(\n",
    "        id_vars=id_vars,\n",
    "        value_vars=value_vars,\n",
    "        var_name='original_column_name', # Create a temporary column for old names\n",
    "        value_name='value' # The required 'value' column\n",
    "    )\n",
    "    # Split the temporary column into the required new columns\n",
    "    .assign(\n",
    "        source = lambda x: x['original_column_name'].str.split('_').str[0],\n",
    "        year = lambda x: x['original_column_name'].str.split('_').str[1].astype(int), # Convert to int\n",
    "        band = lambda x: x['original_column_name'].str.split('_').str[2]\n",
    "    )\n",
    "    # Drop the temporary column\n",
    "    .drop(columns=['original_column_name'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3156eeb-5493-49aa-86f3-5215321fe276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot all sites for cover\n",
    "bdf = ldf[(ldf[\"band\"]==\"cover\")]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3,1.5))\n",
    "sns.lineplot(data=bdf, x=\"year\", y=\"value\", hue=\"site\", linewidth=0.5, legend=False, palette=\"tab20\", ax=ax)\n",
    "ax.axvspan(xmin=2010.5, xmax=2014.5, facecolor='gray', alpha=0.3, label='No predictions')\n",
    "ax.axvline(2015, linestyle='--', c='k')\n",
    "ax.text(2014, 0.32, '2015 drought')\n",
    "ax.set(xlabel=\"Cover (%)\", ylabel=\"Year\")\n",
    "\n",
    "figname = \"climate_sensitivity_cover\"\n",
    "for ext in [\".svg\"]:\n",
    "    figpath = os.path.join(figdir, figname + ext)\n",
    "    fig.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3671ba-2316-4cd4-85db-580a83b934e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get important predictors to decide which ones to plot\n",
    "path = r\"J:\\projects\\ECOFOR\\gedi\\models\\v08\\GEDI_2AB_2019to2023_leafon_sampy500m_all_imps_v08.csv\"\n",
    "idf = pd.read_csv(path, header=[0,1])\n",
    "idf = idf['lt-p-s-t_cover']\n",
    "idf.mean().sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a88ef0e-a3cc-4343-bda0-f4161c41d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Line plot all sites for NDVI\n",
    "\n",
    "# # Load base landsat composite values for comparison\n",
    "# path = r\"J:\\projects\\ECOFOR\\climate_sensitivity\\vca_unburned09to17_snapped_pi_landsat_wet.csv\"\n",
    "# odf = pd.read_csv(path)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(3,3))\n",
    "# bdf = ldf[(ldf[\"source\"]==\"ltwet\") & (ldf[\"band\"]==\"ndvi\") & (ldf[\"change\"]==\"none\")]\n",
    "# bdf[\"value\"] = bdf[\"value\"] / 1000 # rescale ndvi to real value\n",
    "# sns.lineplot(data=bdf, x=\"year\", y=\"value\", hue=\"site\", linewidth=0.5, legend=False, palette=\"tab20\", ax=ax)\n",
    "\n",
    "# # sns.scatterplot(data=odf, x=\"year\", y=\"ndvi\", hue=\"site\", linewidth=0.5, legend=False, palette=\"tab20\", ax=ax, alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2461272f-217c-4711-9755-477d93521367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecac962-9077-49bf-a46f-a181e88a9be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select LandTrendr fits for demonstration\n",
    "combos = [\n",
    "    (521, \"wet\", \"ndvi\", 2007), \n",
    "    (1704, \"wet\", \"green\", 2007),\n",
    "    (1704, \"wet\", \"green\", 1984),\n",
    "#     (809, \"wet\", \"green\", 2007),\n",
    "#     (809, \"wet\", \"green\", 1984),\n",
    "]\n",
    "\n",
    "for (site, season, band, starty) in combos:\n",
    "    # Load base landsat composite values for comparison\n",
    "    path = r\"J:\\projects\\ECOFOR\\climate_sensitivity\\vca_unburned09to17_snapped_pi_landsat_\"+season+\"2.csv\"\n",
    "    odf = pd.read_csv(path)\n",
    "    odf = odf[(odf[\"site\"] == site) & (odf[band]!=-32768) & (odf[\"year\"]>starty)]\n",
    "    \n",
    "    mask = (ldf[\"site\"]==site) & (ldf[\"source\"]==\"lt\"+season) & (ldf[\"band\"]==band) & (ldf[\"year\"]>starty)\n",
    "    bdf = ldf[mask].copy()\n",
    "    bdf[\"value\"] = bdf[\"value\"] / 1000 # rescale to real value\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(3,1.5))\n",
    "    sns.lineplot(data=bdf, x=\"year\", y=\"value\", linewidth=1, legend=False, ax=ax)\n",
    "    sns.scatterplot(data=odf, x=\"year\", y=band, size=1, legend=False, ax=ax)\n",
    "    ax.set(xlabel=\"Year\", ylabel=band)\n",
    "    \n",
    "    figname = \"lt_vs_landsat_\"+str(site)+\"_\"+season+\"_\"+band+\"_\"+str(starty)\n",
    "    for ext in [\".svg\"]:\n",
    "        figpath = os.path.join(figdir, figname + ext)\n",
    "        fig.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17a0909-afec-4b71-ac7f-9a7104bed7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot LT against Landsat for a given band for each site\n",
    "band = \"green\"\n",
    "season = \"wet\"\n",
    "\n",
    "# Load base landsat composite values for comparison\n",
    "path = r\"J:\\projects\\ECOFOR\\climate_sensitivity\\vca_unburned09to17_snapped_pi_landsat_\"+season+\"2.csv\"\n",
    "odf = pd.read_csv(path)\n",
    "\n",
    "for site in ldf[\"site\"].unique():\n",
    "    fig, ax = plt.subplots(figsize=(3,2))\n",
    "    mask = (ldf[\"site\"]==site) & (ldf[\"source\"]==\"lt\"+season) & (ldf[\"band\"]==band) & (ldf[\"year\"]>starty)\n",
    "    bdf = ldf[mask].copy()\n",
    "    bdf[\"value\"] = bdf[\"value\"] / 1000 # rescale to real value\n",
    "\n",
    "    sns.lineplot(data=bdf, x=\"year\", y=\"value\", linewidth=1, legend=False, ax=ax)\n",
    "\n",
    "    sodf = odf[(odf[\"site\"] == site) & (odf[band]!=-32768)]\n",
    "    sns.scatterplot(data=sodf, x=\"year\", y=band, legend=False, ax=ax)\n",
    "    \n",
    "    ax.set(xlabel=\"Year\", ylabel=band)\n",
    "\n",
    "    print(site)\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ed0a0e-6eb3-4c8e-951e-ba08faa1f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take difference between all pairs of subsequent years\n",
    "bdf_s = bdf.sort_values(by=['site', 'year']).reset_index(drop=True)\n",
    "bdf_s['valdif'] = bdf_s.groupby('site')['value'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd226a4-60c9-4d5a-a34a-0059fc8e3432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average change across all year pairs\n",
    "bdf_s.groupby('year')['valdif'].mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b283a7c-4f3f-4158-8578-803bee68053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average change for 2010 to 2015\n",
    "bdf_s[bdf_s[\"year\"]==2015].set_index(\"site\")[\"valdif\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a74d85e-ef13-465e-8b18-81f9fe01975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average change for 2015 to 2016\n",
    "bdf_s[bdf_s[\"year\"]==2016].set_index(\"site\")[\"valdif\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a59046f-961d-42b9-812a-b848730d9b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa8b66-a6a6-4c5c-97ea-e994e084c9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51a4fa93-8344-4ea5-badb-6439fb10596d",
   "metadata": {},
   "source": [
    "### Small Area Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb04f7b-28c2-4062-a316-47794980b823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get model-based estimators exported from R\n",
    "# path = r\"J:\\projects\\ECOFOR\\gedi\\sae\\sae_gedi_estimates_20240823_l6b6.csv\" #r\"J:\\projects\\ECOFOR\\gedi\\sae\\sae_gedi_estimates_20240823_l6b6.csv\"\n",
    "path =  r\"J:\\projects\\ECOFOR\\gedi\\sae\\sae_gedi_estimates_20251103.csv\" #r\"J:\\projects\\ECOFOR\\gedi\\sae\\sae_gedi_estimates_20250312.csv\" #\n",
    "package = \"emdi\" #\"sae\" # \n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df = df[df['metric']!='pai'] # drop use of PAI\n",
    "\n",
    "if package==\"sae\":\n",
    "    df = df.rename(columns={\"mean\":\"Mean\", \"domain\":\"Domain\", \"mse\":\"Mean_MSE\"})\n",
    "\n",
    "df['metric_title'] = df['metric'].replace({'cover':'Cover (%)', 'rh98': 'RH98 (m)', 'fhd': 'FHD', 'pai':'PAI'})\n",
    "df['aoi'] = df['Domain'].str[:-5]\n",
    "df['year'] = df['Domain'].str[-4:].astype(int)\n",
    "\n",
    "# Fix cover to be in percent for plotting and get confidence intervals\n",
    "df['mean_rmse'] = df['Mean_MSE']**0.5\n",
    "df.loc[df['metric']=='cover', ['Mean', 'mean_rmse']] *= 100\n",
    "t_val = 1.645 # critical value for 90% CI from t-distribution with inf degrees of freedom\n",
    "df['mean_ci90_half'] = (df['mean_rmse'] * 1.645) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b1c35-54f1-4d20-8b88-064bf066b539",
   "metadata": {},
   "source": [
    "#### Model-based vs. Design-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d24b9b6-fa2c-4c17-a933-021ac69ca7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get direct estimators exported from R\n",
    "path = r\"J:\\projects\\ECOFOR\\gedi\\sae\\direct_gedi_estimates_20250415.csv\"\n",
    "ddf = pd.read_csv(path)\n",
    "\n",
    "ddf = ddf[ddf['domain'].isin(df['Domain'].unique())]\n",
    "\n",
    "ddf['aoi'] = ddf['domain'].str[:-5]\n",
    "ddf['year'] = ddf['domain'].str[-4:].astype(int)\n",
    "\n",
    "ddf = ddf[ddf['metric']!='pai'] # drop use of PAI\n",
    "\n",
    "ddf = ddf.dropna(subset='mean_ci90_half') # drop rows without proper variance (nclusters<2)\n",
    "\n",
    "# convert cover to percent\n",
    "ddf.loc[ddf['metric']=='cover', ['mean', 'mean_ci90_half', 'mean_ci90_lower', 'mean_ci90_upper']] *= 100\n",
    "\n",
    "# merge with model-based df in long form so all can be plotted together\n",
    "df.columns = df.columns.str.lower()\n",
    "df['mean_ci90_upper'] = df['mean'] + df['mean_ci90_half']\n",
    "df['mean_ci90_lower'] = df['mean'] - df['mean_ci90_half']\n",
    "df['samptype']='model-based'\n",
    "\n",
    "mdf = pd.concat([df, ddf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a57e1b2-c30b-487f-a914-390fdefd478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge design and model-based in wide form for direct comparison of CIs\n",
    "cdf = ddf[ddf['samptype']=='cluster']\n",
    "cols = ['domain', 'metric', 'aoi', 'year', 'mean', 'mean_ci90_lower', 'mean_ci90_upper', 'mean_ci90_half']\n",
    "wdf = pd.merge(cdf[cols+['nclusters']], df[cols+['metric_title']], how='left', on=['domain', 'metric'], suffixes=['_d', '_m'])\n",
    "\n",
    "# model-based estimates overlapping the CI of the design-based estimates\n",
    "wdf['m_in_dci'] = wdf['mean_m'].between(wdf['mean_ci90_lower_d'], wdf['mean_ci90_upper_d'])\n",
    "\n",
    "# model-based CI and design_based CI overlap\n",
    "wdf['overlap'] = (wdf['mean_ci90_upper_m'] >= wdf['mean_ci90_lower_d']) & (wdf['mean_ci90_lower_m'] <= wdf['mean_ci90_upper_d'])\n",
    "\n",
    "# difference and percent difference in means\n",
    "wdf['m_diff'] = wdf['mean_m'] - wdf['mean_d']\n",
    "wdf['m_pctdiff'] = wdf['m_diff'] / wdf['mean_d'] * 100\n",
    "\n",
    "# Column of relationship between model-based and design-based\n",
    "wdf['relate'] = 'No overlap'\n",
    "wdf.loc[wdf['overlap'], 'relate'] = 'CIs overlap'\n",
    "wdf.loc[wdf['m_in_dci'], 'relate'] = 'Model-based within\\nDesign-based CI'\n",
    "\n",
    "print(len(wdf['domain'].unique()), 'domains.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1430f0be-c2f4-449d-ba28-467e3157be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: look at difference or percent difference or simply overlap/not overlap as a function of\n",
    "#       aoi size, gedi sample size, gedi # of clusters, and difference of m_mean from the training sample mean.\n",
    "#       Consider modeling the difference as a function of these factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172ec6d4-d2e8-4c62-8b28-bb4f44ca4d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one2one(x, y, data, color=None, label=None):\n",
    "    lims = (data[[x,y]].min().min(), data[[x,y]].max().max())\n",
    "    plt.plot(lims, lims, '--k')\n",
    "    \n",
    "xcol, ycol = \"mean_m\", \"mean_d\"\n",
    "g = sns.FacetGrid(wdf, col=\"metric_title\", hue=\"relate\", sharey=False, sharex=False, aspect=0.9, height=2)\n",
    "g.map_dataframe(one2one, xcol, ycol)\n",
    "g.map_dataframe(plt.errorbar, xcol, ycol, \"mean_ci90_half_d\", \"mean_ci90_half_m\", fmt='.', ecolor='0.8', alpha=0.8)\n",
    "g.add_legend(title='')\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "g.set_axis_labels(x_var = 'Model-based Mean', y_var='Design-based Mean')\n",
    "\n",
    "# Add RMSE, R2 and Bias for each metric\n",
    "for col, ax in zip(g.col_names, g.axes[0]):\n",
    "    mask = wdf['metric_title']==col\n",
    "    x, y = wdf.loc[mask, xcol], wdf.loc[mask, ycol]\n",
    "        \n",
    "    # add text\n",
    "    r2 = r2_score(y, x)\n",
    "    ax.text(0.99, 0.22, \"R$^2$= \" + str(r2.round(2)), transform=ax.transAxes, ha='right')\n",
    "    bias = (x-y).mean()\n",
    "    ax.text(0.99, 0.13, \"Bias= \"+str(bias.round(2)), transform=ax.transAxes, ha='right')\n",
    "    rmse = mean_squared_error(y, x)**0.5\n",
    "    ax.text(0.99, 0.02,  \"RMSE= \" + str(rmse.round(2)), transform=ax.transAxes, ha='right')\n",
    "\n",
    "# figname = \"modelbased_vs_designbased\"\n",
    "# for ext in [\".png\", \".svg\"]: #, \".pdf\"]:\n",
    "#     figpath = os.path.join(figdir, figname + ext)\n",
    "#     g.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f704a-0644-41a1-9295-04e800d19466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the distribution of the training data\n",
    "path = r\"J:\\projects\\ECOFOR\\gedi\\models\\v08\\GEDI_2AB_2019to2023_leafon_sampy500m_all_oob_v08.parquet\"\n",
    "oob = pd.read_parquet(path)\n",
    "\n",
    "oob.columns = oob.columns.str.replace('fhd_normal','fhd')\n",
    "oob[oob.columns[oob.columns.str.endswith('cover')]] *=100\n",
    "\n",
    "modstr = '' #'pred_lt-p-s-t_' #\n",
    "ostats = oob[[modstr+'cover', modstr+'rh98', modstr+'fhd']].describe()\n",
    "ostats.rename(columns={'cover':'Cover (%)', 'rh98': 'RH98 (m)', 'fhd': 'FHD', 'pai':'PAI'}, inplace=True)\n",
    "\n",
    "xcol, ycol = \"mean_d\", \"m_diff\"\n",
    "g = sns.relplot(xcol, ycol, col='metric_title', hue='relate', kind='scatter', size='nclusters', data=wdf,\n",
    "                facet_kws={'sharex':False, 'sharey':False}, aspect=0.75, height=2.1, alpha=0.8, sizes=(8,32))\n",
    "g.map(sns.regplot, xcol, ycol, scatter=False, ci=False, color='k', line_kws={'alpha':0.5})\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "g.set_axis_labels(y_var = 'Estimator difference', x_var='Design-based Mean')\n",
    "g.legend.remove()\n",
    "g.add_legend(label_order=['2', '4', '6', '8', '10', '12'], title='N clusters')\n",
    "for col, ax in zip(g.col_names, g.axes[0]):\n",
    "    ax.axhline(0, linestyle='--', c='k')\n",
    "    ax.axvline(ostats.loc['50%', modstr+col], linestyle=':', c='k', alpha=0.5)\n",
    "    \n",
    "# figname = \"modelbased_err_vs_designbased\"\n",
    "# for ext in [\".png\", \".svg\"]: #, \".pdf\"]:\n",
    "#     figpath = os.path.join(figdir, figname + ext)\n",
    "#     g.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a8831c-cf39-4c42-9831-af9cda6df7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent of overlap between design-based and model-based\n",
    "wdf.groupby('metric')[['m_in_dci', 'overlap']].mean().round(2)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f2abd-5b3d-43d3-9b85-02c6b2c2e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average size of CI half-width as percent of the design-based estimator\n",
    "wdf['ci_half_pct_d'] = wdf['mean_ci90_half_d'] / wdf['mean_d'] * 100\n",
    "wdf.groupby('metric')['ci_half_pct_d'].mean().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71deacc-215c-43a1-8623-3bc2e3e6dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average percent difference of model-based from design-based\n",
    "wdf['m_pctdiff_abs'] = wdf['m_pctdiff'].abs()\n",
    "wdf.groupby('metric')['m_pctdiff_abs'].describe().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd358d6-b56a-4f5a-82e0-68f246708e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot('metric', 'm_pctdiff_abs', data=wdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1dc6d6-9be7-4c5a-8550-0e822d90e7a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot mean and CI of all years and metrics for select aois\n",
    "# aois = [\"thornybush\", \"skukuza_se\", \"plantation_a\", \"bushbuckridge_a\"]#, \"justacia\"] #\"moditlo\"]\n",
    "aois = mdf['aoi'].unique()\n",
    "for aoi in aois:\n",
    "    pdf = mdf[mdf['aoi']==aoi]\n",
    "    order = pdf['year'].unique()\n",
    "    order.sort()\n",
    "        \n",
    "    def errplot(x, y, data, order, hue, yerr, palette='deep', color=None):\n",
    "        xs = np.arange(len(order))\n",
    "        hues = data[hue].unique()\n",
    "        dodge_width = 0.8\n",
    "        dodge_vals = np.linspace(-dodge_width / 2, dodge_width / 2, len(hues)*2+1)[1::2]\n",
    "        colors = sns.color_palette(palette, len(hues))\n",
    "        for hue_val, dodge_val, color in zip(hues, dodge_vals, colors):\n",
    "            ys = []\n",
    "            yerrs = []\n",
    "            for xi in order:\n",
    "                mask = (data[x] == xi) & (data[hue] == hue_val)\n",
    "                if mask.sum()>0:\n",
    "                    ys.append(data[mask][y].to_numpy()[0])\n",
    "                    yerrs.append(data[mask][yerr].to_numpy()[0])\n",
    "                else:\n",
    "                    ys.append(np.nan)\n",
    "                    yerrs.append(np.nan)\n",
    "            plt.bar(x=xs + dodge_val, height=ys, yerr=yerrs, width=dodge_width / len(hues), color=color, label=hue_val)\n",
    "        plt.xticks(xs, order)\n",
    "\n",
    "    g = sns.FacetGrid(pdf, row=\"metric\", sharey=False, sharex=False, aspect=2)\n",
    "    g.map_dataframe(errplot, \"year\", \"mean\", hue=\"samptype\", yerr=\"mean_ci90_half\", order=order)\n",
    "    g.set_xlabels(\"\")\n",
    "    g.set_xticklabels(rotation=45, ha='right', rotation_mode='anchor')\n",
    "    g.set_titles(template=aoi+\" {row_name}\")\n",
    "    g.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf01224f-0181-460e-9be5-b2290b02b6dc",
   "metadata": {},
   "source": [
    "#### Change in AOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7868fd1c-1906-41ab-99f3-41259465c14b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot mean and CI of all years and metrics for select aois\n",
    "aois = [\"thornybush\", \"skukuza_se\", \"plantation_a\", \"bushbuckridge_a\", \"justacia\"] #\"moditlo\"]\n",
    "for aoi in aois:\n",
    "    pdf = df[df['aoi']==aoi]\n",
    "\n",
    "    def errplot(x, y, yerr, **kwargs):\n",
    "        ax = plt.gca()\n",
    "        data = kwargs.pop(\"data\")\n",
    "        data.plot(x=x, y=y, yerr=yerr, kind=\"bar\", ax=ax, capsize=4, **kwargs)\n",
    "\n",
    "    g = sns.FacetGrid(pdf, row=\"metric_title\", sharey=False, sharex=False, aspect=2)\n",
    "    g.map_dataframe(errplot, \"year\", \"mean\", \"mean_ci90_half\")\n",
    "    g.set_xlabels(\"\")\n",
    "    g.set_xticklabels(rotation=45, ha='right', rotation_mode='anchor')\n",
    "    g.set_titles(template=aoi+\" {row_name}\")\n",
    "    g.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f48b6-619f-4124-b639-4e4e4e0d6dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make one figure of all metrics of pre/post for each AOI\n",
    "# aois = [\"thornybush\", \"bushbuckridge_a\", \"moditlo\"]\n",
    "aoi_years = {\n",
    "    'thornybush':{'pre':2017, 'post':2021},\n",
    "    'bushbuckridge_a':{'pre':2007, 'post':2021},\n",
    "    'plantation_a':{'pre':2017, 'post':2021},\n",
    "    'skukuza_se':{'pre':2007, 'post':2022},\n",
    "#              'justacia':{'pre':2016, 'post':2021}\n",
    "            }\n",
    "for aoi, ydict in aoi_years.items():\n",
    "    print(aoi)\n",
    "    pdf = df[df['aoi']==aoi]\n",
    "    pdf = pdf[pdf['year'].isin([ydict['pre'], ydict['post']])]\n",
    "    display(pdf)\n",
    "\n",
    "    def errplot(x, y, yerr, **kwargs):\n",
    "        ax = plt.gca()\n",
    "        data = kwargs.pop(\"data\")\n",
    "        data.plot(x=x, y=y, yerr=yerr, kind=\"bar\", ax=ax, capsize=3, **kwargs)\n",
    "\n",
    "    g = sns.FacetGrid(pdf, col=\"metric_title\", sharey=False, height=2, aspect=0.5)\n",
    "    g.map_dataframe(errplot, \"year\", \"mean\", \"mean_ci90_half\")\n",
    "    g.set_xlabels(\"\")\n",
    "    g.set_xticklabels(rotation=45, ha='right', rotation_mode='anchor')\n",
    "    g.set_titles(template=\"{col_name}\")\n",
    "    g.tight_layout()\n",
    "    \n",
    "    figname = \"gedi_sae_\" + aoi + str(ydict['pre'])+\"_\"+str(ydict['post'])\n",
    "    for ext in [\".pdf\"]: #, \".svg\"]: #, \".pdf\"]:\n",
    "        figpath = os.path.join(figdir, figname + ext)\n",
    "        g.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45531aae-0c35-466c-89f6-647cf3b58fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df['aoi']=='bushbuckridge_a') & (df['metric']=='cover')\n",
    "calc_chg = df.loc[mask & (df['year']==2021), 'mean'].iloc[0] - df.loc[mask & (df['year']==2007), 'mean'].iloc[0]\n",
    "print('Cover in bushbuckridge changed', np.round(calc_chg, 1), '%')\n",
    "\n",
    "mask = (df['aoi']=='thornybush') & (df['metric']=='cover')\n",
    "calc_chg = df.loc[mask & (df['year']==2021), 'mean'].iloc[0] - df.loc[mask & (df['year']==2017), 'mean'].iloc[0]\n",
    "print('Cover in thornybush changed', np.round(calc_chg, 1), '%')\n",
    "\n",
    "mask = (df['aoi']=='thornybush') & (df['metric']=='rh98')\n",
    "calc_chg = df.loc[mask & (df['year']==2021), 'mean'].iloc[0] - df.loc[mask & (df['year']==2017), 'mean'].iloc[0]\n",
    "print('RH98 in thornybush changed', np.round(calc_chg, 1), 'm')\n",
    "\n",
    "mask = (df['aoi']=='skukuza_se') & (df['metric']=='cover')\n",
    "calc_chg = df.loc[mask & (df['year']==2022), 'mean'].iloc[0] - df.loc[mask & (df['year']==2007), 'mean'].iloc[0]\n",
    "print('Cover in skukuza changed', np.round(calc_chg, 1), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23db5f89-6c00-424a-a6fc-eba493f2c6cb",
   "metadata": {},
   "source": [
    "## Presentation figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1066d6-4e4b-49ec-9479-c0590fde2f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# presentation set up\n",
    "figdir = r\"E:\\My Drive\\Work\\ecofor\\ssnm_25\" #r\"C:\\Users\\stevenf\\Google Drive\\Work\\ecofor\\forestsat24\" #r\"G:\\My Drive\\Work\\ecofor\\forestsat24\"\n",
    "os.makedirs(figdir, exist_ok=True)\n",
    "\n",
    "mpl.rcParams['font.family'] = 'sans-serif'\n",
    "mpl.rcParams['font.sans-serif'] = ['Arial']\n",
    "mpl.rcParams['font.size'] = 16\n",
    "\n",
    "sns.set_style('ticks',\n",
    "               {'font.family':'sans-serif', 'font.sans-serif':['Arial'], 'font.size':16})\n",
    "\n",
    "# temporary style to use on dark backgrounds\n",
    "# white_axes = {'axes.labelcolor': '.99', 'text.color': '.99', 'xtick.color': '.99', 'ytick.color': '.99', 'axes.edgecolor': '.99', 'figure.facecolor': 'black'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3529ecf5-d387-401d-b00f-6e09ac429aab",
   "metadata": {},
   "source": [
    "### Model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f016e5f0-bba0-4ea2-8814-1996e811d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and setup data\n",
    "pdf = gpd.read_parquet(r\"D:\\ECOFOR\\gedi\\models\\v05\\GEDI_2AB_2019to2023_leafon_sampy500m_all_oob_v05.parquet\")\n",
    "source_dict = {'l30':'Landsat',\n",
    "               'hls':'HLS',\n",
    "               'palsar':'PALSAR',\n",
    "               'l30p': 'Landsat + PALSAR',\n",
    "               'hlsp': 'HLS + PALSAR'}\n",
    "ydict = {'cover': 'Cover',\n",
    "         'pai': 'PAI',\n",
    "         'rh98': 'RH98',\n",
    "         'fhd_normal': 'FHD'}\n",
    "Xsets = list(source_dict.keys())\n",
    "Ycols = list(ydict.keys())\n",
    "\n",
    "# make cover cols as percent\n",
    "pdf[pdf.columns[pdf.columns.str.contains('cover')]] *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d113e-174b-4b6a-9196-f6e7a4b76180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot obs vs pred for all model results\n",
    "nrows = len(Xsets)\n",
    "fig, axes = plt.subplots(nrows, 4, figsize=(12, 2.3*nrows))\n",
    "sdf = pd.DataFrame(columns = pd.MultiIndex.from_product([list(ydict.values()), ['R2', 'RMSE', 'Bias', 'N']], names=(\"Metric\", \"Stat\")))\n",
    "\n",
    "for i, (Xset, axrow) in enumerate(zip(Xsets, axes)):\n",
    "    for j, (ycol, ax) in enumerate(zip(Ycols, axrow)):\n",
    "        x, y = pdf['pred_'+Xset+'_'+ycol], pdf[ycol]\n",
    "        hb = ax.hexbin(x, y, gridsize=20, mincnt=1, cmap='magma_r', linewidths=0, edgecolor='none', vmax=2000)\n",
    "        ax.plot((y.min(), y.max()), (y.min(),y.max()), '--k')\n",
    "\n",
    "        r2 = r2_score(y, x)\n",
    "        bias = (x-y).mean()\n",
    "        rmse = mean_squared_error(y, x)**0.5\n",
    "        \n",
    "        # Save stats\n",
    "        sdf.loc[Xset, (ydict[ycol], 'R2')] = r2\n",
    "        sdf.loc[Xset, (ydict[ycol], 'RMSE')] = rmse\n",
    "        sdf.loc[Xset, (ydict[ycol], 'Bias')] = bias\n",
    "        sdf.loc[Xset, (ydict[ycol], 'N')] = len(y)\n",
    "\n",
    "        # add text\n",
    "        ax.text(0.99, 0.22, \"R$^2$= \" + str(r2.round(2)), transform=ax.transAxes, ha='right')\n",
    "        ax.text(0.99, 0.13, \"Bias= \"+str(bias.round(2)), transform=ax.transAxes, ha='right')\n",
    "        ax.text(0.99, 0.02,  \"RMSE= \" + str(rmse.round(2)), transform=ax.transAxes, ha='right')\n",
    "\n",
    "        ax.set(title=Xset+'_'+ycol)\n",
    "        if j==0:\n",
    "            ax.set(ylabel='Observed')\n",
    "        fig.supxlabel('Predicted', x=0.47, y=0, ha='center', fontsize=10)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.3, hspace=0.5)\n",
    "cb = fig.colorbar(hb, ax=axes, shrink=True, aspect=16, pad=0.02) #cax=cax, aspect=)#\n",
    "cb.set_label('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb76474-01eb-4552-b6dc-91f5f5631228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Obs vs pred for only the best model\n",
    "# 2x2\n",
    "Xset = 'l30p' #'hlsp'\n",
    "ycol_dict = {'cover':'Cover (%)', 'pai':'PAI', 'rh98':'RH98', 'fhd_normal':'FHD'}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(6.8, 6))\n",
    "\n",
    "for i, (ycol, ax) in enumerate(zip(Ycols, axes.flat)):\n",
    "    x, y = pdf['pred_'+Xset+'_'+ycol], pdf[ycol]\n",
    "    hb = ax.hexbin(x, y, gridsize=20, mincnt=1, cmap='magma_r', linewidths=0, edgecolor='none', vmax=2000)\n",
    "    ax.plot((y.min(), y.max()), (y.min(),y.max()), '--k')\n",
    "\n",
    "    r2 = r2_score(y, x)\n",
    "    bias = (x-y).mean()\n",
    "    rmse = mean_squared_error(y, x)**0.5\n",
    "\n",
    "    # add text\n",
    "    ax.text(0.99, 0.22, \"R$^2$= \" + str(round(r2, 2)), transform=ax.transAxes, ha='right')\n",
    "    ax.text(0.99, 0.13, \"Bias= \"+str(bias.round(2)), transform=ax.transAxes, ha='right')\n",
    "    ax.text(0.99, 0.02,  \"RMSE= \" + str(rmse.round(2)), transform=ax.transAxes, ha='right')\n",
    "\n",
    "    ax.set(title=ycol_dict[ycol])\n",
    "#     if i==0 or i==2:\n",
    "#         ax.set(ylabel='Observed')\n",
    "    fig.supxlabel('Predicted', x=0.47, y=0.02, ha='center', fontsize=16)\n",
    "    fig.supylabel('Observed', x=0.02, y=0.5, ha='center', fontsize=16)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.3, hspace=0.5)\n",
    "cb = fig.colorbar(hb, ax=axes, shrink=True, aspect=16, pad=0.02) #cax=cax, aspect=)#\n",
    "cb.set_label('Count')\n",
    "\n",
    "# figname = \"gedi_acc_\" + Xset\n",
    "# for ext in [\".png\", \".svg\"]: #, \".pdf\"]:\n",
    "#     figpath = os.path.join(figdir, figname + ext)\n",
    "#     fig.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d58a78b-398d-4a0c-aa4c-1ee295568c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366fc5b5-69ff-4e4e-be54-a41bbe97202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Obs vs pred for only the best model\n",
    "# 1x4\n",
    "Xset = 'l30p' #'hlsp'\n",
    "ycol_dict = {'cover':'Cover (%)', 'pai':'PAI', 'rh98':'RH98', 'fhd_normal':'FHD'}\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(2.4, 2.8*4))\n",
    "\n",
    "for i, (ycol, ax) in enumerate(zip(Ycols, axes.flat)):\n",
    "    x, y = pdf['pred_'+Xset+'_'+ycol], pdf[ycol]\n",
    "    hb = ax.hexbin(x, y, gridsize=20, mincnt=1, cmap='magma_r', linewidths=0, edgecolor='none', vmax=2000)\n",
    "    ax.plot((y.min(), y.max()), (y.min(),y.max()), '--k')\n",
    "\n",
    "    r2 = r2_score(y, x)\n",
    "    bias = (x-y).mean()\n",
    "    rmse = mean_squared_error(y, x)**0.5\n",
    "\n",
    "    # add text\n",
    "    ax.text(0.99, 0.22, \"R$^2$= \" + str(round(r2, 2)), transform=ax.transAxes, ha='right')\n",
    "    ax.text(0.99, 0.13, \"Bias= \"+str(bias.round(2)), transform=ax.transAxes, ha='right')\n",
    "    ax.text(0.99, 0.02,  \"RMSE= \" + str(rmse.round(2)), transform=ax.transAxes, ha='right')\n",
    "\n",
    "    ax.set(title=ycol_dict[ycol])\n",
    "#     if i==0 or i==2:\n",
    "#         ax.set(ylabel='Observed')\n",
    "\n",
    "\n",
    "fig.supxlabel('Predicted', x=0.5, y=0, ha='center', fontsize=16)\n",
    "fig.supylabel('Observed', x=0, y=0.5, ha='center', fontsize=16)\n",
    "\n",
    "fig.subplots_adjust(left=0.15, bottom=0.05, hspace=0.4)\n",
    "# cb = fig.colorbar(hb, ax=axes, location='bottom', orientation='horizontal')#, pad=0.05)#, shrink=True, aspect=16, pad=0.02) #cax=cax, aspect=)#\n",
    "# cb.set_label('Count')\n",
    "\n",
    "# figname = \"gedi_acc_\" + Xset + \"_vert\"\n",
    "# for ext in [\".png\", \".svg\"]: #, \".pdf\"]:\n",
    "#     figpath = os.path.join(figdir, figname + ext)\n",
    "#     fig.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14bcf5-86a8-4296-9e8d-8f41a3c6d8ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bar chart of accuracy stats\n",
    "ldf = pd.melt(sdf, ignore_index=False).reset_index()\n",
    "ldf['Source'] = ldf['index'].map(source_dict)\n",
    "\n",
    "mask = ldf['Stat'].isin(['R2', 'RMSE'])\n",
    "palette = ['#a6cee3','#b2df8a','#cab2d6', '#1f78b4','#33a02c']\n",
    "g = sns.catplot(data=ldf[mask], x=\"value\", y=\"Source\", col=\"Stat\", row=\"Metric\", kind='bar', sharex=False, sharey=True, height=3, aspect=1.5, margin_titles=True, palette=palette)\n",
    "g.set_titles(col_template=\"{col_name}\", row_template=\"{row_name}\")\n",
    "g.set_ylabels(\"\")\n",
    "\n",
    "figname = \"gedi_acc_all\"\n",
    "for ext in [\".png\", \".svg\"]: #, \".pdf\"]:\n",
    "    figpath = os.path.join(figdir, figname + ext)\n",
    "    g.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36b1ca1-0c27-43be-97c6-328fe36d07d7",
   "metadata": {},
   "source": [
    "#### Temporal cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a9d51-248a-4b3f-939c-14fea3ee354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and setup TCV predictions\n",
    "pdf = pd.read_csv(r\"J:\\projects\\ECOFOR\\gedi\\models\\v05\\tcv\\GEDI_2AB_2019to2023_leafon_sampy500m_all_l30p_tcv_v5.csv\")\n",
    "source_dict = {'l30':'Landsat',\n",
    "               'hls':'HLS',\n",
    "               'palsar':'PALSAR',\n",
    "               'l30p': 'Landsat + PALSAR',\n",
    "               'hlsp': 'HLS + PALSAR'}\n",
    "ydict = {'cover': 'Cover',\n",
    "         'pai': 'PAI',\n",
    "         'rh98': 'RH98',\n",
    "         'fhd_normal': 'FHD'}\n",
    "Xsets = list(source_dict.keys())\n",
    "Ycols = list(ydict.keys())\n",
    "\n",
    "# make cover cols as percent\n",
    "pdf[pdf.columns[pdf.columns.str.contains('cover')]] *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29182210-bedc-429d-81d2-c208fda3cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TCV stats\n",
    "stats_path = r\"J:\\projects\\ECOFOR\\gedi\\models\\v05\\tcv\\GEDI_2AB_2019to2023_leafon_sampy500m_all_l30p_tcv_v5_stats.csv\"\n",
    "stats = pd.read_csv(stats_path)\n",
    "\n",
    "tcv_stats = stats.groupby(['dset', 'ycol']).mean()\n",
    "\n",
    "# Change stats back to percent from frac\n",
    "tcv_stats.loc[('l30p', 'cover'), ['rmse', 'bias']] *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761b960f-8648-4b0d-952f-259b2b0a4730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Obs vs pred for only the best model\n",
    "# 1x4\n",
    "Xset = 'l30p' #'hlsp'\n",
    "ycol_dict = {'cover':'Cover (%)', 'pai':'PAI', 'rh98':'RH98', 'fhd_normal':'FHD'}\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(2.4, 2.8*4))\n",
    "\n",
    "for i, (ycol, ax) in enumerate(zip(Ycols, axes.flat)):\n",
    "    x, y = pdf[Xset+'_'+ycol], pdf[ycol]\n",
    "    hb = ax.hexbin(x, y, gridsize=20, mincnt=1, cmap='magma_r', linewidths=0, edgecolor='none', vmax=2000)\n",
    "    ax.plot((y.min(), y.max()), (y.min(),y.max()), '--k')\n",
    "    \n",
    "    # # Overall error from TCV predictions\n",
    "    # r2 = r2_score(y, x)\n",
    "    # bias = (x-y).mean()\n",
    "    # rmse = mean_squared_error(y, x)**0.5\n",
    "    \n",
    "    # Error from mean of TCV (correct way)\n",
    "    r2 = tcv_stats.loc[(Xset, ycol), 'r2']\n",
    "    bias = tcv_stats.loc[(Xset, ycol), 'bias']\n",
    "    rmse = tcv_stats.loc[(Xset, ycol), 'rmse']\n",
    "    \n",
    "    # add text\n",
    "    ax.text(0.99, 0.22, \"R$^2$= \" + str(round(r2, 2)), transform=ax.transAxes, ha='right')\n",
    "    ax.text(0.99, 0.13, \"Bias= \"+str(bias.round(2)), transform=ax.transAxes, ha='right')\n",
    "    ax.text(0.99, 0.02,  \"RMSE= \" + str(rmse.round(2)), transform=ax.transAxes, ha='right')\n",
    "\n",
    "    ax.set(title=ycol_dict[ycol])\n",
    "#     if i==0 or i==2:\n",
    "#         ax.set(ylabel='Observed')\n",
    "\n",
    "\n",
    "fig.supxlabel('Predicted', x=0.5, y=0, ha='center', fontsize=16)\n",
    "fig.supylabel('Observed', x=0, y=0.5, ha='center', fontsize=16)\n",
    "\n",
    "fig.subplots_adjust(left=0.15, bottom=0.05, hspace=0.4)\n",
    "# cb = fig.colorbar(hb, ax=axes, location='bottom', orientation='horizontal')#, pad=0.05)#, shrink=True, aspect=16, pad=0.02) #cax=cax, aspect=)#\n",
    "# cb.set_label('Count')\n",
    "\n",
    "figname = \"gedi_tcv_\" + Xset + \"_vert\"\n",
    "\n",
    "for ext in [\".png\", \".svg\"]: #, \".pdf\"]:\n",
    "    figpath = os.path.join(figdir, figname + ext)\n",
    "    fig.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c062e-8751-425f-bf87-cd108a1de1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TCV model accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f89d706-972a-4688-869d-866d50062cf6",
   "metadata": {},
   "source": [
    "### SAE figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0b88b0-fd8f-4269-b242-a0a5fb42b973",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"J:\\projects\\ECOFOR\\gedi\\sae\\sae_gedi_estimates_20240823_l6b6.csv\" #r\"J:\\projects\\ECOFOR\\gedi\\sae\\sae_gedi_estimates_20240823_l6b6.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df['aoi'] = df['Domain'].str[:-5]\n",
    "df['year'] = df['Domain'].str[-4:].astype(int)\n",
    "\n",
    "df['mean_rmse_half'] = df['Mean_MSE']**0.5 / 2\n",
    "df['median_rmse_half'] = df['Median_MSE']**0.5 / 2\n",
    "\n",
    "df['metric_title'] = df['metric'].replace({'cover':'Cover (%)', 'rh98': 'RH98 (m)', 'fhd': 'FHD', 'pai':'PAI'})\n",
    "\n",
    "# Fix cover to be in percent for plotting\n",
    "df.loc[df['metric']=='cover', ['Mean', 'Mean_MSE', 'mean_rmse_half']] *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace83e2f-ff7f-4b77-a4ff-a2f322644f5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a line plot of all years\n",
    "aois = [\"thornybush\", \"skukuza_se\", \"plantation_a\"] #\"bushbuckridge_a\", \"moditlo\"\n",
    "for aoi in aois:\n",
    "    pdf = df[df['aoi']==aoi]\n",
    "\n",
    "    def errplot(x, y, yerr, **kwargs):\n",
    "        ax = plt.gca()\n",
    "        data = kwargs.pop(\"data\")\n",
    "        data.plot(x=x, y=y, yerr=yerr, kind=\"bar\", ax=ax, capsize=4, **kwargs)\n",
    "\n",
    "    g = sns.FacetGrid(pdf, row=\"metric_title\", sharey=False, sharex=False, aspect=2)\n",
    "    g.map_dataframe(errplot, \"year\", \"Mean\", \"mean_rmse_half\")\n",
    "    g.set_xlabels(\"\")\n",
    "    g.set_xticklabels(rotation=45, ha='right', rotation_mode='anchor')\n",
    "    g.set_titles(template=aoi+\" {row_name}\")\n",
    "    g.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe52ea0-03e8-46ff-907b-3403d370e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make one figure of all metrics of pre/post for each AOI\n",
    "# aois = [\"thornybush\", \"bushbuckridge_a\", \"moditlo\"]\n",
    "aoi_years = {'thornybush':{'pre':2016, 'post':2022},\n",
    "             'skukuza_se':{'pre':2009, 'post':2019},\n",
    "             'plantation_a':{'pre':2018, 'post':2021}\n",
    "#              'bushbuckridge_a':{'pre':2007, 'post':2022},\n",
    "             # 'moditlo':{'pre':2016, 'post':2021}\n",
    "            }\n",
    "for aoi, ydict in aoi_years.items():\n",
    "    pdf = df[df['aoi']==aoi]\n",
    "    pdf = pdf[pdf['year'].isin([ydict['pre'], ydict['post']])]\n",
    "\n",
    "    def errplot(x, y, yerr, **kwargs):\n",
    "        ax = plt.gca()\n",
    "        data = kwargs.pop(\"data\")\n",
    "        data.plot(x=x, y=y, yerr=yerr, kind=\"bar\", ax=ax, capsize=4, **kwargs)\n",
    "\n",
    "    g = sns.FacetGrid(pdf, col=\"metric_title\", sharey=False, aspect=0.5)\n",
    "    g.map_dataframe(errplot, \"year\", \"Mean\", \"mean_rmse_half\")\n",
    "    g.set_xlabels(\"\")\n",
    "    g.set_xticklabels(rotation=45, ha='right', rotation_mode='anchor')\n",
    "    g.set_titles(template=\"{col_name}\")\n",
    "    g.tight_layout()\n",
    "    \n",
    "    # figname = \"gedi_sae_\" + aoi + \"18to21\"\n",
    "    # for ext in [\".png\", \".svg\"]: #, \".pdf\"]:\n",
    "    #     figpath = os.path.join(figdir, figname + ext)\n",
    "    #     g.savefig(figpath, dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e92f58-50a1-4b1c-95a5-e04684ea7bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop thornybush rh98 in 2021 which has a weirdly high MSE\n",
    "# row_mask = (pdf['year'].isin([2015, 2021])) & (pdf['metric']=='rh98')\n",
    "# pdf.loc[row_mask, ['Mean', 'Mean_MSE']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77857dc5-ab0b-40e8-a9a8-89f5046a7a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_errorbars(y):\n",
    "#     ax=plt.gca()\n",
    "    \n",
    "# g = sns.catplot(data=pdf, x='year', y='Mean', col='metric', kind='bar', sharey=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648cc5f9-b3a8-42b3-927c-513376ca145a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ecofor]",
   "language": "python",
   "name": "conda-env-ecofor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
